{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classifier Training\n",
    "**Focus & Astigmatism Detection**\n",
    "- **Author:** [Aaron Woods](https://aaronwoods.info)\n",
    "- **Date Created:** October 26, 2023\n",
    "- **Objective:** To conduct the training phase for the Image Focus and Astigmatism Classifier, encompassing data preparation, model development, and preliminary validation.\n",
    "\n",
    "**Training Workflow**\n",
    "1. **Data Loading**: Import and structure the dataset for training.\n",
    "2. **Preprocessing**: Apply necessary data transformations and augmentations.\n",
    "3. **Model Training**: Develop and train the classification model.\n",
    "4. **Initial Validation**: Perform initial tests to assess model accuracy.\n",
    "\n",
    "**Progression**\n",
    "- Upon successful completion of this phase, transition to the [Evaluation Notebook](AI_Classify_Eval_v1.ipynb) for in-depth analysis and model refinement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU support is configured for TensorFlow.\n",
      "Platform: Windows-10-10.0.22621-SP0\n",
      "Python Version: 3.10.9\n",
      "TensorFlow Version: 2.10.1\n",
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "# Setup Instructions\n",
    "# 1. Run the shared setup script to prepare the environment and integrate dependencies.\n",
    "# 2. Execute the setup configurations.\n",
    "\n",
    "# Import and run the setup function from the setup module\n",
    "from setup import setup\n",
    "setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import glob\n",
    "import logging\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Union, Any, Optional\n",
    "\n",
    "# Data Manipulation and Numerical Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning and Data Processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.utils import class_weight, resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Deep Learning with TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, callbacks, optimizers, applications\n",
    "\n",
    "# Image Processing and Visualization\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "import seaborn as sns\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "# Experiment Configuration: Sets up the experiment name, random seed, and problem type.\n",
    "experiment_config = {\n",
    "    'NAME': \"Oct26_100pct_CW_SIM\",        \n",
    "    'RANDOM_SEED': 42,                      # Seed for reproducibility\n",
    "    'PROBLEM_TYPE': 'Multi-Output',         # Problem type: Binary, Multi-Class, Multi-Output, Multi-Label\n",
    "}\n",
    "\n",
    "# Model Configuration: Sets up the model parameters.\n",
    "model_config = {\n",
    "    'IMG_SIZE': 224,                        # Image input size\n",
    "    'BATCH_SIZE': 16,                       # Batch size for training\n",
    "    'TRAIN_SIZE': 0.8,                      # Fraction of data to use for training\n",
    "    'VAL_SIZE': 0.5,                        # Fraction of data to use for validation\n",
    "    'EPOCHS': 100,                          # Number of training epochs\n",
    "    'LEARNING_RATE': 0.001,                 # Learning rate\n",
    "    'EARLY_STOPPING_PATIENCE': 50,          # Early stopping patience\n",
    "    'REDUCE_LR_PATIENCE': 3,                # Reduce learning rate on plateau patience\n",
    "    'MIN_LR': 1e-6,                         # Minimum learning rate\n",
    "}\n",
    "\n",
    "# Label Mapping Configuration: Sets up the label mapping for the dataset. (Optional)\n",
    "label_mappings = {\n",
    "    'Focus_Label': {'SharpFocus': 0, 'SlightlyBlurred': 1, 'HighlyBlurred': 2},\n",
    "    'StigX_Label': {'OptimalStig_X': 0, 'ModerateStig_X': 1, 'SevereStig_X': 2},\n",
    "    'StigY_Label': {'OptimalStig_Y': 0, 'ModerateStig_Y': 1, 'SevereStig_Y': 2},\n",
    "}\n",
    "\n",
    "# Augmentation Configuration: Sets up the augmentation parameters.\n",
    "augmentation_config = {\n",
    "    'rotation_factor': 0.002,           # Rotation range (radians)\n",
    "    'height_factor': (-0.18, 0.18),      # Vertical shift range\n",
    "    'width_factor': (-0.18, 0.18),       # Horizontal shift range\n",
    "    'contrast_factor': 0.5,              # Contrast adjustment range\n",
    "}\n",
    "\n",
    "# Combine Experiment, Model, Labels, and Augmentation Configurations\n",
    "config = {\n",
    "    'Experiment': experiment_config,\n",
    "    'Model': model_config,\n",
    "    'Labels': {'MAPPINGS': label_mappings},\n",
    "    'Augmentation': augmentation_config\n",
    "}\n",
    "\n",
    "# Dataset Creation Configuration\n",
    "csv_config = {\n",
    "    'CSV': {\n",
    "        'COLUMNS_TO_READ': ['ImageFile', 'Focus_Offset (V)', 'Stig_Offset_X (V)', 'Stig_Offset_Y (V)']\n",
    "    },\n",
    "    'Thresholds': {\n",
    "        'FOCUS_LOW': 30,                              # Lower focus threshold\n",
    "        'FOCUS_HIGH': 60,                             # Upper focus threshold\n",
    "        'STIGX_LOW': 1,                               # Lower astigmatism threshold (X)\n",
    "        'STIGX_HIGH': 2,                              # Upper astigmatism threshold (X)\n",
    "        'STIGY_LOW': 1,                               # Lower astigmatism threshold (Y)\n",
    "        'STIGY_HIGH': 2,                              # Upper astigmatism threshold (Y)\n",
    "    },\n",
    "    'Paths': {\n",
    "        'OLD_BASE_PATH': \"D:\\\\DOE\\\\\",\n",
    "        # On Simulation Computer\n",
    "        'DATA_FILE': \"combined_output.csv\", # Simulation Computer\n",
    "        # 'DATA_FILE': \"combined_output_cleaned.csv\", # Laptop\n",
    "        'NEW_BASE_PATH': \"Y:\\\\User\\\\Aaron-HX38\\\\DOE\\\\\",  # Simulation Computer\n",
    "        # 'NEW_BASE_PATH': \"C:\\\\Users\\\\aaron.woods\\\\OneDrive - Thermo Fisher Scientific\\\\Desktop\\\\Dec 24\\\\\", # Laptop\n",
    "    },\n",
    "    'SAMPLE_FRAC': 1.0,                                # Fraction of the data for quicker prototyping (1.0 means use all data)\n",
    "}\n",
    "\n",
    "# Update the main configuration dictionary with the dataset configuration\n",
    "config.update(csv_config)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(config['Experiment']['RANDOM_SEED'])\n",
    "tf.random.set_seed(config['Experiment']['RANDOM_SEED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions for Preparation of CSV\n",
    "\n",
    "def read_csv(config: Dict):\n",
    "    # Functionality to read the data\n",
    "    data_file_path = os.path.join(config['Paths']['NEW_BASE_PATH'], config['Paths']['DATA_FILE'])\n",
    "    if not os.path.exists(data_file_path):\n",
    "        raise FileNotFoundError(f\"Error: File does not exist - {data_file_path}\")\n",
    "    try:\n",
    "        data = pd.read_csv(data_file_path, usecols=config['CSV']['COLUMNS_TO_READ'])\n",
    "        print(\"---> Data read successfully.\")\n",
    "        sample_frac = config.get('SAMPLE_FRAC', 1.0)\n",
    "        if 0 < sample_frac < 1.0:\n",
    "            data = data.sample(frac=sample_frac).reset_index(drop=True)\n",
    "            print(f\"---> Data sampled: Using {sample_frac * 100}% of the available data.\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error: Could not read data - {e}\") from e\n",
    "    return data\n",
    "\n",
    "def update_image_paths(df):\n",
    "    old_base_path = config['Paths']['OLD_BASE_PATH']\n",
    "    new_base_path = config['Paths']['NEW_BASE_PATH']\n",
    "    df['ImageFile'] = df['ImageFile'].str.replace(old_base_path, new_base_path, regex=False)\n",
    "    print(\"---> Image paths updated.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_csv(df: pd.DataFrame, save_cleaned: bool = False) -> pd.DataFrame:\n",
    "    def is_valid_string(image_path) -> bool:\n",
    "        return isinstance(image_path, str)\n",
    "    \n",
    "    def does_file_exist(image_path) -> bool:\n",
    "        return os.path.exists(image_path)\n",
    "    \n",
    "    def can_image_be_read(image_path) -> bool:\n",
    "        img = cv2.imread(image_path)\n",
    "        return img is not None\n",
    "    \n",
    "    removal_reasons = defaultdict(list)\n",
    "    total_rows = len(df)\n",
    "    csv_file_path = os.path.join(config['Paths']['NEW_BASE_PATH'], config['Paths']['DATA_FILE'])\n",
    "    print(\"Cleaning CSV file...\")\n",
    "    for index, row in enumerate(df.itertuples()):\n",
    "        \n",
    "        image_path = row.ImageFile\n",
    "        reason = None\n",
    "        \n",
    "        if not is_valid_string(image_path):\n",
    "            reason = \"Invalid ImageFile value - not a string\"\n",
    "        elif not does_file_exist(image_path):\n",
    "            reason = \"File does not exist\"\n",
    "        elif not can_image_be_read(image_path):\n",
    "            reason = \"Image can't be read\"\n",
    "        \n",
    "        if reason:\n",
    "            removal_reasons[reason].append(index)\n",
    "    \n",
    "    invalid_rows = [index for indices in removal_reasons.values() for index in indices]\n",
    "    df.drop(index=invalid_rows, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(\"\\n\\nSummary of row removals:\")\n",
    "    for reason, indices in removal_reasons.items():\n",
    "        print(f\"{len(indices)} rows removed due to: {reason}\")\n",
    "        print(f\"Row indices: {indices}\")\n",
    "    \n",
    "    if save_cleaned and csv_file_path:\n",
    "        cleaned_csv_file_path = f\"{os.path.splitext(csv_file_path)[0]}_cleaned.csv\"\n",
    "        df.to_csv(cleaned_csv_file_path, index=False)\n",
    "        print(f\"Cleaned CSV saved to: {cleaned_csv_file_path}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating labels\n",
    "\n",
    "def generate_thresholds(label_key):\n",
    "    low_key = f\"{label_key.split('_')[0].upper()}_LOW\"\n",
    "    high_key = f\"{label_key.split('_')[0].upper()}_HIGH\"\n",
    "    return config.get('Thresholds', {}).get(low_key, 0), config.get('Thresholds', {}).get(high_key, 0)\n",
    "\n",
    "def generate_single_label(df_copy, label_key, offset_column, choices_dict):\n",
    "    low_threshold, high_threshold = generate_thresholds(label_key)\n",
    "    conditions = [\n",
    "        (df_copy[offset_column].abs() <= low_threshold),\n",
    "        (df_copy[offset_column].abs() > low_threshold) & (df_copy[offset_column].abs() <= high_threshold),\n",
    "        (df_copy[offset_column].abs() > high_threshold)\n",
    "    ]\n",
    "    choices = list(choices_dict.keys())\n",
    "    df_copy[label_key] = np.select(conditions, choices, default='Unknown')\n",
    "    le = LabelEncoder()\n",
    "    df_copy[label_key] = le.fit_transform(df_copy[label_key])\n",
    "    return le\n",
    "\n",
    "def generate_labels(df: pd.DataFrame):\n",
    "    print(\"---> Generating labels for Focus, StigX, and StigY...\")\n",
    "    labels_config = config.get('Labels', {}).get('MAPPINGS', {})\n",
    "    offset_column_mapping = {'Focus_Label': 'Focus_Offset (V)', 'StigX_Label': 'Stig_Offset_X (V)', 'StigY_Label': 'Stig_Offset_Y (V)'}\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    label_encoders = {}\n",
    "\n",
    "    for label_key, choices_dict in labels_config.items():\n",
    "        offset_column = offset_column_mapping.get(label_key)\n",
    "        if not offset_column:\n",
    "            print(f\"Warning: No offset column mapping found for '{label_key}'. Skipping label generation.\")\n",
    "            continue\n",
    "        if offset_column not in df.columns:\n",
    "            print(f\"Warning: Column '{offset_column}' not found in DataFrame. Skipping label generation for '{label_key}'.\")\n",
    "            continue\n",
    "        label_encoders[label_key] = generate_single_label(df_copy, label_key, offset_column, choices_dict)\n",
    "        print(f\"---> Labels generated for {label_key}\")\n",
    "\n",
    "    if config.get('Experiment', {}).get('PROBLEM_TYPE') == 'Multi-Output':\n",
    "        df_copy['Multi_Output_Labels'] = df_copy.apply(lambda row: [row[key] for key in labels_config.keys()], axis=1)\n",
    "        print(\"---> Multi-Output Labels generated.\")\n",
    "        \n",
    "    return df_copy, label_encoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Shuffling and Splitting the Data\n",
    "\n",
    "def shuffle_and_reset_index(data):\n",
    "    print(\"---> Shuffling and resetting index...\")\n",
    "    shuffled_df = data.sample(frac=1, random_state=config['Experiment']['RANDOM_SEED']).reset_index(drop=True)\n",
    "    print(\"---> Data shuffled and index reset.\")\n",
    "    return shuffled_df\n",
    "\n",
    "def prepare_datasets(df: pd.DataFrame):\n",
    "    \"\"\"Prepare training, validation, and test datasets.\"\"\"\n",
    "    # Check if DataFrame is empty\n",
    "    if df is None or df.empty:\n",
    "        print(\"Warning: DataFrame is empty. Cannot proceed with data preparation.\")\n",
    "        return {'train': None, 'valid': None, 'test': None}\n",
    "    # Split Data\n",
    "    try:\n",
    "        train_df, temp_df = train_test_split(df, test_size=1 - config['Model']['TRAIN_SIZE'], random_state=config['Experiment']['RANDOM_SEED'])\n",
    "        val_df, test_df = train_test_split(temp_df, test_size=1 - config['Model']['VAL_SIZE'], random_state=config['Experiment']['RANDOM_SEED'])\n",
    "    except ValueError:\n",
    "        print(\"Not enough data to split into training, validation, and test sets.\")\n",
    "        return {'train': None, 'valid': None, 'test': None}\n",
    "    print(\"---> Data split into training, validation, and test sets.\")\n",
    "    return {'train': train_df, 'valid': val_df, 'test': test_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Computting Class Weights\n",
    "\n",
    "def compute_class_weights_for_label(split: str, df: pd.DataFrame, label: str, label_encoders: Dict[str, LabelEncoder], all_records: List[Dict], is_multi_label: bool = False) -> None:\n",
    "    y_data = df[label].values if is_multi_label else df[label]\n",
    "    unique_labels = np.unique(y_data)\n",
    "    class_weights = compute_class_weight('balanced', classes=unique_labels, y=y_data)\n",
    "    class_weights_dict = dict(zip(unique_labels, class_weights))\n",
    "    \n",
    "    for cls, weight in class_weights_dict.items():\n",
    "        cnt = Counter(y_data)[cls]\n",
    "        original_class = label_encoders[label].inverse_transform([cls])[0]\n",
    "        all_records.append({\n",
    "            'split': split,\n",
    "            'label': label,\n",
    "            'class': original_class,\n",
    "            'Count': cnt,\n",
    "            'Weight': weight\n",
    "        })\n",
    "\n",
    "def compute_and_store_class_weights(datasets: Dict[str, pd.DataFrame], label_encoders: Dict[str, LabelEncoder]) -> pd.DataFrame:\n",
    "    problem_type = config.get('Experiment', {}).get('PROBLEM_TYPE', 'Binary')\n",
    "    all_records = []\n",
    "    \n",
    "    for split, df in datasets.items():\n",
    "        if df is None:\n",
    "            continue\n",
    "        for label in config['Labels']['MAPPINGS']:\n",
    "            compute_class_weights_for_label(\n",
    "                split, df, label, label_encoders, all_records, \n",
    "                is_multi_label=(problem_type == 'Multi-label')\n",
    "            )\n",
    "                \n",
    "    df_class_weights = pd.DataFrame.from_records(all_records)\n",
    "    df_class_weights.set_index(['split', 'label', 'class'], inplace=True)\n",
    "    return df_class_weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare class weights for multi-output problems (Optional) (TODO: Incorporate into code above)\n",
    "\n",
    "def prepare_class_weights_for_multi_output(info: pd.DataFrame) -> Union[Dict[str, Dict[int, float]], None]:\n",
    "    \"\"\"\n",
    "    Prepare class weights for multi-output problems for Keras and TensorFlow.\n",
    "    \n",
    "    Parameters:\n",
    "    - info: DataFrame containing the class weights information\n",
    "    - config: Configuration dictionary.\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary mapping output layer names to their respective class weight dictionaries or None\n",
    "    \"\"\"\n",
    "    if not config.get('USE_CLASS_WEIGHTS', True):\n",
    "        print(\"Configuration says not to use class weights. Returning None.\")\n",
    "        return None\n",
    "\n",
    "    class_weights = {}\n",
    "    for label in info.index.get_level_values('label').unique():\n",
    "        class_weights[label] = {}\n",
    "        sub_df = info.loc[(slice(None), label), :]\n",
    "        for idx, row in sub_df.iterrows():\n",
    "            class_idx = label_encoders[label].transform([idx[2]])[0]  # Transforming class name to class index\n",
    "            class_weights[label][class_idx] = row['Weight']\n",
    "    return class_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating Tensorflow Datasets\n",
    "\n",
    "def create_tf_datasets_from_dfs(dfs: Dict[str, pd.DataFrame], include_offset: bool = False) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create TensorFlow datasets from DataFrames for training, validation, and testing.\n",
    "    \"\"\"\n",
    "    problem_type = config.get('Experiment', {}).get('PROBLEM_TYPE')\n",
    "    batch_size = config.get('Model', {}).get('BATCH_SIZE', 32)\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    \n",
    "    preprocessing_layers = create_preprocessing_layers()\n",
    "    augmentation_layers = create_augmentation_layers()\n",
    "    \n",
    "    def load_and_preprocess_image(file_path: tf.Tensor, label: tf.Tensor, offset: Optional[tf.Tensor], augment: bool = False) -> Tuple:\n",
    "        file_path_str = file_path.numpy().decode('utf-8')\n",
    "        image = read_and_convert_image(file_path_str)\n",
    "        image = preprocessing_layers(image)\n",
    "        if augment:\n",
    "            image = augmentation_layers(image)\n",
    "            image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "        label = tf.cast(label, tf.int32)\n",
    "        offset = tf.cast(offset, tf.float32) if offset is not None else None\n",
    "        return (image, label, offset) if include_offset else (image, label)\n",
    "\n",
    "    def prepare_dataset(file_paths, labels, offsets, augment):\n",
    "        ds = tf.data.Dataset.from_tensor_slices((file_paths, labels, offsets))\n",
    "        ds = ds.map(lambda file_path, label, offset: tf.py_function(\n",
    "            func=load_and_preprocess_image,\n",
    "            inp=[file_path, label, offset if offset is not None else tf.constant([], dtype=tf.float32), augment],\n",
    "            Tout=[tf.float32, label.dtype, tf.float32] if include_offset else [tf.float32, label.dtype]\n",
    "        ))\n",
    "        return ds.batch(batch_size).prefetch(buffer_size=AUTOTUNE) # If I have a lot of memory, I can use cache() here.\n",
    "\n",
    "    tf_datasets = {'train': {}, 'valid': {}, 'test': {}}\n",
    "    offset_column_mapping = {\n",
    "        'Focus_Label': 'Focus_Offset (V)',\n",
    "        'StigX_Label': 'Stig_Offset_X (V)',\n",
    "        'StigY_Label': 'Stig_Offset_Y (V)'\n",
    "    }\n",
    "\n",
    "    for split, df in dfs.items():\n",
    "        augment_data = (split == 'train')\n",
    "        if problem_type in ['Multi-Class', 'Binary']:\n",
    "            for label in ['Focus_Label', 'StigX_Label', 'StigY_Label']:\n",
    "                offset_column = offset_column_mapping.get(label)\n",
    "                offsets = df[offset_column].values if include_offset else None\n",
    "                tf_datasets[split][label] = prepare_dataset(df['ImageFile'].values, df[label].values, offsets, augment_data)\n",
    "        \n",
    "        elif problem_type == 'Multi-Output':\n",
    "            labels = df[['Focus_Label', 'StigX_Label', 'StigY_Label']].values\n",
    "            offsets = df[['Focus_Offset (V)', 'Stig_Offset_X (V)', 'Stig_Offset_Y (V)']].values if include_offset else None\n",
    "            tf_datasets[split]['Multi_Output'] = prepare_dataset(df['ImageFile'].values, labels, offsets, augment_data)\n",
    "        \n",
    "        else:\n",
    "            print(\"Unknown problem type specified in config. Please check.\")\n",
    "\n",
    "    return tf_datasets\n",
    "\n",
    "### Image Augmentation and Preprocessing\n",
    "\n",
    "def create_preprocessing_layers() -> keras.Sequential:\n",
    "    \"\"\"Create preprocessing layers for resizing and rescaling images.\"\"\"\n",
    "    img_size = config['Model']['IMG_SIZE']\n",
    "    return keras.Sequential([\n",
    "        layers.Resizing(img_size, img_size),\n",
    "        layers.Rescaling(1./255)\n",
    "    ])\n",
    "\n",
    "def create_augmentation_layers() -> keras.Sequential:\n",
    "    \"\"\"Create data augmentation layers.\"\"\"\n",
    "    aug_config = config['Augmentation']\n",
    "    try:\n",
    "        return keras.Sequential([\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomFlip(\"vertical\"),\n",
    "            layers.RandomRotation(aug_config['rotation_factor']),\n",
    "            layers.RandomTranslation(\n",
    "                height_factor=aug_config['height_factor'],\n",
    "                width_factor=aug_config['width_factor'],\n",
    "                fill_mode=\"reflect\"\n",
    "            ),\n",
    "            layers.RandomContrast(aug_config['contrast_factor']),\n",
    "        ])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating augmentation layers: {e}\")\n",
    "        return None\n",
    "\n",
    "def read_and_convert_image(file_path: str) -> tf.Tensor:\n",
    "    \"\"\"Read an image from a file and convert it to a 3-channel tensor.\"\"\"\n",
    "    image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        print(\"Failed to read the image.\")\n",
    "        return None\n",
    "    image = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "    image = tf.expand_dims(image, axis=-1)\n",
    "    return tf.image.grayscale_to_rgb(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Resample Datasets to deal with Imbalances (Optional)\n",
    "\n",
    "def validate_inputs(datasets: Dict[str, pd.DataFrame], resample_label: str, resample_strategy: str) -> None:\n",
    "    if not isinstance(datasets, dict):\n",
    "        raise ValueError(\"Input datasets should be a dictionary.\")\n",
    "    if resample_strategy not in [\"upsample\", \"downsample\", \"combined\"]:\n",
    "        raise ValueError(\"Invalid resample_strategy. Choose from 'upsample', 'downsample', or 'combined'.\")\n",
    "    for key, df in datasets.items():\n",
    "        if resample_label not in df.columns:\n",
    "            raise ValueError(f\"'{resample_label}' is not a valid column in the {key} dataset.\")\n",
    "\n",
    "def target_count_for_strategy(label_counts: pd.Series, strategy: str) -> int:\n",
    "    if strategy == \"downsample\":\n",
    "        return label_counts.min()\n",
    "    elif strategy == \"upsample\":\n",
    "        return label_counts.max()\n",
    "    return int(label_counts.median())\n",
    "\n",
    "def iterative_resampling(df: pd.DataFrame, resample_strategy: str, resample_label: str) -> pd.DataFrame:\n",
    "    label_counts = df[resample_label].apply(tuple).value_counts()\n",
    "    target_count = target_count_for_strategy(label_counts, resample_strategy)\n",
    "    subsets = [\n",
    "        resample(\n",
    "            df[df[resample_label].apply(tuple) == unique_label],\n",
    "            replace=(label_counts[unique_label] < target_count),\n",
    "            n_samples=target_count\n",
    "        )\n",
    "        for unique_label in label_counts.keys()\n",
    "    ]\n",
    "    return pd.concat(subsets).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "def resample_datasets(datasets: Dict[str, pd.DataFrame], resample_label='Multi_Output_Labels', resample_strategy=\"downsample\") -> Dict[str, pd.DataFrame]:\n",
    "    validate_inputs(datasets, resample_label, resample_strategy)\n",
    "    \n",
    "    int32_columns = [col for col, dtype in datasets.get('train', pd.DataFrame()).dtypes.items() if dtype == 'int32']\n",
    "    \n",
    "    def process_dataset(key: str, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if key != 'train':\n",
    "            return df\n",
    "        resampled_data = iterative_resampling(df, resample_strategy, resample_label)\n",
    "        for col in int32_columns:\n",
    "            resampled_data[col] = resampled_data[col].astype('int32')\n",
    "        return resampled_data\n",
    "    \n",
    "    return {key: process_dataset(key, df) for key, df in datasets.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸš€ Enhanced Data Loading and Preprocessing Workflow\n",
    "\n",
    "print(\"\\nðŸ” [START] Preprocessing CSV Data...\")\n",
    "data = read_csv(config)\n",
    "data = update_image_paths(data)\n",
    "# Uncomment the below line if you want to clean the CSV\n",
    "data = clean_csv(data, save_cleaned=False)\n",
    "data, label_encoders = generate_labels(data)\n",
    "data = shuffle_and_reset_index(data)\n",
    "print(\"\\nðŸ“Š [LOAD] Preparing TensorFlow Datasets...\")\n",
    "datasets = prepare_datasets(data)\n",
    "print(\"  ðŸ‹ï¸â€â™‚ï¸ Computing class weights for original datasets...\")\n",
    "df_class_weights = compute_and_store_class_weights(datasets, label_encoders)\n",
    "print(\"\\nðŸ”§ [BUILD] Creating TensorFlow datasets from DataFrames...\")\n",
    "datasets = create_tf_datasets_from_dfs(datasets, include_offset=True)\n",
    "print(\"\\nâœ… [DONE] Preprocessing Complete!\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n  ðŸ” Resampling datasets... (Optional)\")\n",
    "resampled_datasets = resample_datasets(datasets, resample_label='Multi_Output_Labels', resample_strategy=\"upsample\")\n",
    "rdf_class_weights = compute_and_store_class_weights(resampled_datasets, label_encoders)\n",
    "resampled_datasets = create_tf_datasets_from_dfs(resampled_datasets, include_offset=True)\n",
    "print(\"\\nâœ… [DONE] Resampling Complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Class Distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def add_annotations(ax, bars, sub_df):\n",
    "    \"\"\"\n",
    "    Adds annotations to the bars.\n",
    "    \"\"\"\n",
    "    for bar, (_, row) in zip(bars, sub_df.iterrows()):\n",
    "        x = bar.get_x() + bar.get_width() / 2.0\n",
    "        y = bar.get_height() + 0.5  # Shift annotation slightly above the bar for clarity\n",
    "        ax.annotate(f\"C: {int(row['Count'])}\\nW: {row['Weight']:.2f}\", \n",
    "                    (x, y), \n",
    "                    ha='center', \n",
    "                    va='bottom', \n",
    "                    fontsize=8)\n",
    "\n",
    "def plot_single_split(ax, df, split):\n",
    "    \"\"\"\n",
    "    Plots the class distribution for a single split (train/test/valid).\n",
    "    \"\"\"\n",
    "    filtered_df = df.loc[split]\n",
    "    x_ticks = []\n",
    "    x_tick_locs = []\n",
    "    current_x = 0  # Keep track of the current x-location for ticks\n",
    "    \n",
    "    labels = filtered_df.index.get_level_values('label').unique()\n",
    "    for label in labels:\n",
    "        sub_df = filtered_df.loc[label]\n",
    "        bars = ax.bar(sub_df.index, sub_df['Count'], label=f\"{label}\")\n",
    "        add_annotations(ax, bars, sub_df)\n",
    "        \n",
    "        x_ticks.extend([f\"{label}_{cls}\" for cls in sub_df.index])\n",
    "        x_tick_locs.extend([current_x + i for i in range(len(sub_df.index))])\n",
    "        current_x += len(sub_df.index)  # Update the x-location for the next set of bars\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set_xticks(x_tick_locs)  # Set tick locations\n",
    "    ax.set_xticklabels(x_ticks, rotation=90, fontsize=8)  # Set tick labels\n",
    "    ax.set_title(f\"{split.capitalize()} Data\")\n",
    "    ax.set_ylabel(\"Count\")  # Indicate that the bars represent counts\n",
    "\n",
    "\n",
    "def plot_dataset_info(df):\n",
    "    \"\"\"\n",
    "    Plots the class distribution for train, valid, and test splits.\n",
    "    \"\"\"\n",
    "    splits = ['train', 'valid', 'test']\n",
    "    fig, axs = plt.subplots(1, len(splits), figsize=(20, 8))\n",
    "    \n",
    "    for i, split in enumerate(splits):\n",
    "        plot_single_split(axs[i], df, split)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_dataset_info(df_class_weights)\n",
    "plot_dataset_info(rdf_class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot Images from Dataset\n",
    "\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_single_image(ax, image, label_names, offsets):\n",
    "    \"\"\"\n",
    "    Plots a single image with associated labels and offsets.\n",
    "    \"\"\"\n",
    "    ax.imshow(image)\n",
    "    title_text = \", \".join(f\"{name} ({offset})\" for name, offset in zip(label_names, offsets))\n",
    "    ax.set_title(\"\\n\".join(textwrap.wrap(title_text, 30)))  # Wrap text to fit into the subplot\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "def extract_and_transform_labels(labels, label_keys, label_encoders):\n",
    "    \"\"\"\n",
    "    Extracts and transforms labels using label encoders.\n",
    "    \"\"\"\n",
    "    label_names = []\n",
    "    for label_value, label_key in zip(labels, label_keys):\n",
    "        label_encoder = label_encoders.get(label_key, None)\n",
    "        if label_encoder:\n",
    "            label_names.append(label_encoder.inverse_transform([label_value])[0])\n",
    "        else:\n",
    "            label_names.append(str(label_value))\n",
    "    return label_names\n",
    "\n",
    "def plot_images_from_dataset(label_encoders, config, datasets):\n",
    "    \"\"\"\n",
    "    Plots images, labels, and offsets (if available) from the provided datasets.\n",
    "    \n",
    "    Args:\n",
    "        label_encoders (dict): Dictionary of label encoders for each label key.\n",
    "        config (dict): Configuration dictionary containing problem type and other parameters.\n",
    "        datasets (dict): Dictionary of datasets, containing training data for each label key.\n",
    "    \"\"\"\n",
    "    label_keys = ['Focus_Label', 'StigX_Label', 'StigY_Label']\n",
    "    problem_type = config['Experiment']['PROBLEM_TYPE']\n",
    "\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "\n",
    "    # Helper function to get relevant dataset based on problem type\n",
    "    def get_relevant_dataset(problem_type, label_key):\n",
    "        return datasets['train'][label_key if problem_type in ['Multi-Class', 'Binary'] else 'Multi_Output']\n",
    "\n",
    "    # If problem type is not Multi-Output, re-use the same logic for both Binary and Multi-Class\n",
    "    relevant_datasets = [label_keys] if problem_type != \"Multi-Output\" else [\"Multi_Output\"]\n",
    "\n",
    "    for label_key in relevant_datasets:\n",
    "        label_encoder = label_encoders.get(label_key, None)\n",
    "        fig.suptitle(f\"Images for {label_key}\")\n",
    "\n",
    "        for data in get_relevant_dataset(problem_type, label_key).take(1):\n",
    "            images, labels = data[:2]\n",
    "            offsets = data[2] if len(data) > 2 else None\n",
    "\n",
    "            for i in range(min(len(images), 9)):\n",
    "                ax = axes[i // 3, i % 3]\n",
    "                current_labels = labels[i].numpy() if problem_type == \"Multi-Output\" else [labels[i].numpy()]\n",
    "                current_offsets = offsets[i].numpy() if offsets is not None else [\"N/A\"] * len(label_keys)\n",
    "                # Round offsets to 2 decimal places\n",
    "                current_offsets = [f\"{offset:.2f}\" for offset in current_offsets]\n",
    "\n",
    "                label_names = extract_and_transform_labels(current_labels, label_keys, label_encoders)\n",
    "                plot_single_image(ax, images[i].numpy(), label_names, current_offsets)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    if problem_type not in [\"Multi-Class\", \"Binary\", \"Multi-Output\"]:\n",
    "        print(\"Unknown problem type specified in config. Please check.\")\n",
    "\n",
    "# Example usage (assuming label_encoders, config, and datasets are defined elsewhere)\n",
    "plot_images_from_dataset(label_encoders, config, datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing Offset Labels from the Dataset and Splitting (TODO: See if this is necessary)\n",
    "\n",
    "def inspect_dataset_content(dataset, name, num_batches=3):\n",
    "    \"\"\"\n",
    "    Inspect the content of the dataset to determine the nature of the tensors.\n",
    "    \"\"\"\n",
    "    print(f\"Inspecting first few records of {name} dataset...\")\n",
    "    \n",
    "    for i, record in enumerate(dataset.take(num_batches)):\n",
    "        summary = {k: (v.shape, v.dtype) for k, v in record.items()}\n",
    "        print(f\"{name} record {i + 1}: {summary}\")\n",
    "        \n",
    "    print(f\"Inspecting unique values in {name} dataset...\")\n",
    "    \n",
    "    for i, batch in enumerate(dataset.take(num_batches)):\n",
    "        print(f\"Batch {i + 1} content:\")\n",
    "        \n",
    "        for tensor_name, tensor in batch.items():\n",
    "            unique_values = tf.unique(tf.reshape(tensor, [-1])).y.numpy()\n",
    "            print(f\"Unique values in {tensor_name}: {unique_values}\")\n",
    "        \n",
    "        print(\"------\")\n",
    "\n",
    "def select_tensors(*tensors):\n",
    "    return tensors[0], tensors[1]  # Return only the image and label tensors\n",
    "    # return tensors[0], tensors[1], tensors[2]  # Return image, label, and offset tensors\n",
    "\n",
    "def get_dataset(raw_datasets, dataset_type):\n",
    "    \"\"\"Retrieve specific dataset type (train, valid, test) from the raw datasets dictionary.\"\"\"\n",
    "    return raw_datasets.get(dataset_type, {}).get('Multi_Output')\n",
    "\n",
    "def prepare_and_inspect_dataset(dataset, dataset_name):\n",
    "    \"\"\"Apply transformations and inspect a dataset.\"\"\"\n",
    "    if dataset is None:\n",
    "        print(f\"{dataset_name} dataset is None. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    dataset = dataset.map(select_tensors)\n",
    "    # inspect_dataset_content(dataset, dataset_name)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def prepare_datasets_for_training(raw_datasets):\n",
    "    \"\"\"\n",
    "    Prepare and inspect datasets for training.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing prepared TensorFlow datasets for training, validation, and testing.\n",
    "    \"\"\"\n",
    "    if raw_datasets is None:\n",
    "        print(\"Datasets dictionary is None. Exiting.\")\n",
    "        return None, None, None\n",
    "\n",
    "    train_dataset = prepare_and_inspect_dataset(get_dataset(raw_datasets, 'train'), 'Train')\n",
    "    valid_dataset = prepare_and_inspect_dataset(get_dataset(raw_datasets, 'valid'), 'Validation')\n",
    "    test_dataset = prepare_and_inspect_dataset(get_dataset(raw_datasets, 'test'), 'Test')\n",
    "    \n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "# Uncomment this line to run the function with your datasets\n",
    "train_dataset, valid_dataset, test_dataset = prepare_datasets_for_training(datasets)\n",
    "# train_dataset, valid_dataset, test_dataset = prepare_datasets_for_training(resampled_datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Building (Define the Model)\n",
    "\n",
    "def add_multi_output_heads(base_layer, num_classes: int, output_names: List[str]) -> List[keras.layers.Layer]:\n",
    "    \"\"\"Creates multiple output heads for a given base layer.\"\"\"\n",
    "    outputs = []\n",
    "    for i in range(num_classes):\n",
    "        x = layers.Dense(128, activation=\"relu\")(base_layer)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.Dense(3, activation=\"softmax\", name=output_names[i])(x)  # Naming each output layer\n",
    "        outputs.append(x)\n",
    "    return outputs\n",
    "\n",
    "def determine_activation_and_units(num_classes: int) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"Determines the activation functions and units based on the number of classes and config settings.\"\"\"\n",
    "    problem_type = config.get('Experiment').get('PROBLEM_TYPE')\n",
    "    if problem_type in ['Multi-Label', 'Binary', 'Multi-Class', 'Multi-Output']:\n",
    "        return {\n",
    "            'Multi-Label': ([\"sigmoid\"] * num_classes, [1] * num_classes),\n",
    "            'Binary': ([\"sigmoid\"], [1]),\n",
    "            'Multi-Class': ([\"softmax\"], [num_classes]),\n",
    "            'Multi-Output': ([\"softmax\"] * num_classes, [3] * num_classes)  # Assuming each output has 3 classes\n",
    "        }[problem_type]\n",
    "    raise ValueError(f\"Invalid problem_type: {problem_type}\")\n",
    "\n",
    "def create_transfer_model(base_model, input_shape: Tuple[int, int, int], num_classes: int, hidden_units: List[int], dropout_rate: float, regularizer_rate: float, output_names: List[str] = None) -> keras.Model:\n",
    "    \"\"\"Creates a transfer learning model based on the provided base model.\"\"\"\n",
    "    base_model.trainable = False\n",
    "    model = keras.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D()\n",
    "    ])\n",
    "    for units in hidden_units:\n",
    "        model.add(layers.Dense(units, kernel_regularizer=keras.regularizers.l2(regularizer_rate), bias_regularizer=keras.regularizers.l2(regularizer_rate)))\n",
    "        model.add(layers.LeakyReLU())\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    activations, units_list = determine_activation_and_units(num_classes)\n",
    "    if len(activations) == 1:\n",
    "        model.add(layers.Dense(units_list[0], activation=activations[0]))\n",
    "        return model\n",
    "    \n",
    "    # output_names = output_names or list(config['Labels']['MAPPINGS'].keys())\n",
    "    output_names = list(config['Labels']['MAPPINGS'].keys())\n",
    "\n",
    "    outputs = add_multi_output_heads(model.layers[-1].output, num_classes, output_names)\n",
    "    return keras.Model(inputs=model.input, outputs=outputs)\n",
    "\n",
    "def create_specific_transfer_model(base_model_class, input_shape: Tuple[int, int, int], num_classes: int) -> keras.Model:\n",
    "    \"\"\"Helper function to create specific transfer models.\"\"\"\n",
    "    base_model = base_model_class(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    return create_transfer_model(base_model, input_shape, num_classes, [128, 64], 0.5, 0.001, output_names=config['Labels']['MAPPINGS'].keys())\n",
    "\n",
    "def create_mobilenetv2_transfer_model(input_shape: Tuple[int, int, int], num_classes: int) -> keras.Model:\n",
    "    return create_specific_transfer_model(tf.keras.applications.MobileNetV2, input_shape, num_classes)\n",
    "\n",
    "def create_inceptionv3_transfer_model(input_shape: Tuple[int, int, int], num_classes: int) -> keras.Model:\n",
    "    return create_specific_transfer_model(tf.keras.applications.InceptionV3, input_shape, num_classes)\n",
    "\n",
    "def create_resnet50_transfer_model(input_shape: Tuple[int, int, int], num_classes: int) -> keras.Model:\n",
    "    return create_specific_transfer_model(tf.keras.applications.ResNet50, input_shape, num_classes)\n",
    "\n",
    "# Define the function to create a basic CNN model\n",
    "def create_basic_cnn_model(input_shape, num_classes):\n",
    "    conv2d_filter_size = (3, 3)\n",
    "    conv2d_activation = 'relu'\n",
    "    dense_activation = 'relu'\n",
    "    num_conv_blocks = 3\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    for _ in range(num_conv_blocks):\n",
    "        x = layers.Conv2D(32 * (2**_), conv2d_filter_size, activation=conv2d_activation, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation=dense_activation)(x)\n",
    "\n",
    "    activations, units_list = determine_activation_and_units(num_classes)\n",
    "    if len(activations) == 1:\n",
    "        # Single output\n",
    "        x = layers.Dense(units_list[0], activation=activations[0])(x)\n",
    "        return keras.Model(inputs=inputs, outputs=x)\n",
    "    else:\n",
    "        # Multiple outputs\n",
    "        outputs = add_multi_output_heads(x, num_classes, output_names=list(config['Labels']['MAPPINGS'].keys()))\n",
    "        return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Define the function to create a small version of the Xception network\n",
    "def create_small_xception_model(input_shape, num_classes):\n",
    "    # Input layer\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Entry block: Initial Convolution and BatchNormalization\n",
    "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    previous_block_activation = x  # Set aside residual for later use\n",
    "\n",
    "    # Middle flow: Stacking Separable Convolution blocks\n",
    "    for size in [256, 512, 728]:\n",
    "        # ReLU activation\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        # Separable Convolution\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        # ReLU activation\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        # Separable Convolution\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        # Max Pooling\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual from previous block and add it to the current block\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(previous_block_activation)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Exit flow: Final Separable Convolution, BatchNormalization, and Global Average Pooling\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    activations, units_list = determine_activation_and_units(num_classes)\n",
    "    if len(activations) == 1:\n",
    "        # Single output\n",
    "        x = layers.Dense(units_list[0], activation=activations[0])(x)\n",
    "        return keras.Model(inputs=inputs, outputs=x)\n",
    "    else:\n",
    "        # Multiple outputs\n",
    "        outputs = add_multi_output_heads(x, num_classes, output_names=list(config['Labels']['MAPPINGS'].keys()))\n",
    "        return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Model Selection function to select which model to use\n",
    "def select_model(model_name: str, input_shape: Tuple[int, int, int], num_classes: int) -> keras.Model:\n",
    "    \"\"\"Selects a model to use based on the given model name.\"\"\"\n",
    "    model_map = {\n",
    "        \"mobilenetv2\": create_mobilenetv2_transfer_model,\n",
    "        \"inceptionv3\": create_inceptionv3_transfer_model,\n",
    "        \"resnet50\": create_resnet50_transfer_model,\n",
    "        \"small_xception\": create_small_xception_model,\n",
    "        \"basic_cnn\": create_basic_cnn_model\n",
    "    }\n",
    "    if model_name not in model_map:\n",
    "        raise ValueError(\"Invalid model name\")\n",
    "\n",
    "    return model_map[model_name](input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Initialization (Compile the Model)\n",
    "\n",
    "# # Constants & Configurations\n",
    "LOSS_CONFIG = {\n",
    "    'Binary': 'binary_crossentropy',\n",
    "    'Multi-Class': 'categorical_crossentropy',\n",
    "    'Multi-Output': ['categorical_crossentropy'] * len(config['Labels']['MAPPINGS']),\n",
    "    'Multi-Label': 'binary_crossentropy'\n",
    "}\n",
    "\n",
    "RECOMMENDED_METRICS = {\n",
    "    'Binary': ['accuracy', 'binary_crossentropy', 'mean_squared_error'],\n",
    "    'Multi-Class': ['categorical_accuracy', 'categorical_crossentropy', 'mean_squared_error'],\n",
    "    'Multi-Output': ['categorical_accuracy'] * len(config['Labels']['MAPPINGS']) + \n",
    "                    ['categorical_crossentropy'] * len(config['Labels']['MAPPINGS']) + \n",
    "                    ['mean_squared_error'] * len(config['Labels']['MAPPINGS']),\n",
    "    'Multi-Label': ['binary_accuracy', 'binary_crossentropy', 'mean_squared_error']\n",
    "}\n",
    "\n",
    "# Helper Functions\n",
    "def get_accuracy_metric(problem_type: str) -> str:\n",
    "    \"\"\"Determine the accuracy metric based on the problem type.\"\"\"\n",
    "    return {'Binary': \"accuracy\", 'Multi-Label': \"binary_accuracy\"}.get(problem_type, \"categorical_accuracy\")\n",
    "\n",
    "def create_directory(path: str):\n",
    "    \"\"\"Create a directory if it doesn't exist.\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Callback Setup Functions\n",
    "def setup_common_callbacks() -> List[callbacks.Callback]:\n",
    "    \"\"\"Set up common callbacks.\"\"\"\n",
    "    return [\n",
    "        callbacks.EarlyStopping(patience=config['Model']['EARLY_STOPPING_PATIENCE'], restore_best_weights=True),\n",
    "        callbacks.ReduceLROnPlateau(patience=config['Model']['REDUCE_LR_PATIENCE'], min_lr=config['Model']['MIN_LR'])\n",
    "    ]\n",
    "\n",
    "def setup_specific_callbacks(model_name: str, model_dir: str, problem_type: str) -> List[callbacks.Callback]:\n",
    "    \"\"\"Set up model-specific callbacks.\"\"\"\n",
    "    datetime_str = datetime.now().strftime(\"%Y%m%d-%I%M%S%p\")\n",
    "    acc_metric = get_accuracy_metric(problem_type)\n",
    "    checkpoint_path = os.path.join(model_dir, f\"saved_model_{datetime_str}_epoch_{{epoch}}_val_loss_{{val_loss:.2f}}_{acc_metric}_{{{{val_{acc_metric}:.2f}}}}.h5\")\n",
    "    return [\n",
    "        callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True),\n",
    "        callbacks.TensorBoard(log_dir=os.path.join(model_dir, \"logs\", datetime_str))\n",
    "    ]\n",
    "\n",
    "def compile_model(model_name: str, input_shape: tuple, num_classes: int, problem_type: str) -> tf.keras.Model:\n",
    "    \"\"\"Compile and return a model.\"\"\"\n",
    "    model = select_model(model_name, input_shape, num_classes)\n",
    "    metrics_to_use = list(set(RECOMMENDED_METRICS.get(problem_type, ['accuracy'])))\n",
    "    loss_to_use = LOSS_CONFIG.get(problem_type, 'categorical_crossentropy')\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(config['Model']['LEARNING_RATE']), \n",
    "        loss=loss_to_use, \n",
    "        metrics=metrics_to_use\n",
    "    )\n",
    "    # model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compile_and_initialize_models() -> Dict[str, Dict[str, tf.keras.Model]]:\n",
    "    \"\"\"Main function to compile and initialize models.\"\"\"\n",
    "    input_shape = (config['Model']['IMG_SIZE'], config['Model']['IMG_SIZE'], 3)\n",
    "    num_classes = 3\n",
    "    problem_type = config['Experiment']['PROBLEM_TYPE']\n",
    "\n",
    "    experiment_name = config['Experiment']['NAME']\n",
    "    base_dir = f\"./{experiment_name}\"\n",
    "    create_directory(base_dir)\n",
    "\n",
    "    common_callbacks = setup_common_callbacks()\n",
    "    label_names = config['Labels']['MAPPINGS'].keys() if problem_type in ['Multi-Class', 'Multi-Output'] else ['']\n",
    "\n",
    "    compiled_models = {}\n",
    "    for label_name in label_names:\n",
    "        label_dir = os.path.join(base_dir, label_name)\n",
    "        create_directory(label_dir)\n",
    "\n",
    "        for model_name in ['mobilenetv2', 'inceptionv3', 'resnet50', 'small_xception', 'basic_cnn']:\n",
    "            model_dir = os.path.join(label_dir, model_name)\n",
    "            create_directory(model_dir)\n",
    "            \n",
    "            specific_callbacks = setup_specific_callbacks(model_name, model_dir, problem_type)\n",
    "            all_callbacks = common_callbacks + specific_callbacks\n",
    "            \n",
    "            model = compile_model(model_name, input_shape, num_classes, problem_type)\n",
    "            compiled_models[model_name] = {'model': model, 'callbacks': all_callbacks}\n",
    "    \n",
    "    return compiled_models\n",
    "\n",
    "# Execution\n",
    "compiled_models = compile_and_initialize_models()\n",
    "print(\"Models compiled and initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Remove this or incorporate it in the beginnning\n",
    "# Additions to the config\n",
    "config['USE_CLASS_WEIGHTS'] = True  # Decide whether to use class weights or not\n",
    "prepared_class_weights = prepare_class_weights_for_multi_output(df_class_weights)\n",
    "# print(prepared_class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions to Train Models\n",
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "\n",
    "def handle_memory_after_training(model):\n",
    "    K.clear_session()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "def _process_single_model(model_name, model_info, train_dataset, valid_dataset, prepared_class_weights):\n",
    "    model = model_info.get('model')\n",
    "    callbacks = model_info.get('callbacks', [])\n",
    "\n",
    "    if model is None:\n",
    "        print(f\"Model for {model_name} is None. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Training {model_name} for multi-output...\")\n",
    "\n",
    "    model_output_names = [layer.name for layer in model.layers if 'output' in layer.name]\n",
    "    current_class_weights = {name: prepared_class_weights[name] for name in model_output_names}\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=valid_dataset,\n",
    "        epochs=config['Model']['EPOCHS'],\n",
    "        callbacks=callbacks,\n",
    "        class_weight=current_class_weights\n",
    "    )\n",
    "\n",
    "    handle_memory_after_training(model)\n",
    "\n",
    "    print(f\"\\nTraining for {model_name} completed.\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions that could probably be improved\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def latest_checkpoint(model_dir):\n",
    "    \"\"\"Find the latest checkpoint in the given directory.\"\"\"\n",
    "    checkpoints = [file for file in os.listdir(model_dir) if file.endswith('.h5')]\n",
    "    if checkpoints:\n",
    "        return os.path.join(model_dir, sorted(checkpoints, reverse=True)[0])\n",
    "    return None\n",
    "\n",
    "def train_and_save_metrics(train_dataset, valid_dataset, test_dataset, compiled_models, prepared_class_weights):\n",
    "    if not all([train_dataset, valid_dataset, test_dataset]):\n",
    "        print(\"One or more datasets are None. Exiting.\")\n",
    "        return\n",
    "\n",
    "    for model_name, model_info in compiled_models.items():\n",
    "        try:\n",
    "            history = _process_single_model(model_name, model_info, train_dataset, valid_dataset, prepared_class_weights)\n",
    "            if history is not None:\n",
    "                # Construct the path for the CSV file\n",
    "                csv_filename = f\"{model_name}_metrics.csv\"\n",
    "                csv_path = os.path.join(\"./\", csv_filename)\n",
    "                # Convert history to DataFrame and save as CSV\n",
    "                pd.DataFrame(history.history).to_csv(csv_path)\n",
    "                print(f\"Saved metrics for {model_name} to {csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing model {model_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\nFinished training all models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've already defined and initialized all your datasets and configs\n",
    "train_and_save_metrics(train_dataset, valid_dataset, test_dataset, compiled_models, prepared_class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging and Investigation Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Investigating Model Outputs\n",
    "\n",
    "\n",
    "for label_category, encoder in label_encoders.items():\n",
    "    print(f'Mapping for {label_category}:')\n",
    "    for index, class_label in enumerate(encoder.classes_):\n",
    "        print(f'{index}: {class_label}')\n",
    "    print()  # print a blank line between categories\n",
    "\n",
    "\n",
    "\n",
    "def analyze_model_outputs(compiled_models, train_dataset):\n",
    "    model_output_details = []\n",
    "    \n",
    "    # Assuming train_dataset is an iterable that yields batches of data,\n",
    "    # We'll take the first batch of images from the training dataset for analysis.\n",
    "    for images, labels in train_dataset.take(1):\n",
    "        image_batch = images.numpy()  # Convert tensors to numpy arrays if necessary\n",
    "\n",
    "    # Order of label encoders corresponding to the outputs\n",
    "    encoder_order = ['Focus_Label', 'StigX_Label', 'StigY_Label']\n",
    "    \n",
    "    for model_name, model_details in compiled_models.items():\n",
    "        model = model_details['model']\n",
    "        output = model.predict(image_batch)\n",
    "        \n",
    "        if not isinstance(output, list):\n",
    "            output = [output]  # Ensure output is a list for consistency\n",
    "        \n",
    "        for i, output_to_check in enumerate(output):\n",
    "            one_hot_encoded = all((np.sum(row) > 0.99 and np.sum(row) < 1.01) for row in output_to_check)\n",
    "            suggested_loss = 'categorical_crossentropy' if one_hot_encoded else 'sparse_categorical_crossentropy'\n",
    "            max_prob_class = np.argmax(output_to_check, axis=-1)\n",
    "            \n",
    "            # Get the label encoder for the current output\n",
    "            encoder_key = encoder_order[i] if i < len(encoder_order) else None\n",
    "            label_encoder = label_encoders[encoder_key] if encoder_key else None\n",
    "            \n",
    "            # Decode the sample output\n",
    "            sample_output = output_to_check[0] if len(output_to_check) > 0 else \"No Output\"\n",
    "            sample_output_decoded = label_encoder.inverse_transform([np.argmax(sample_output)])[0] if label_encoder else \"No Decoder\"\n",
    "            \n",
    "            model_output_details.append({\n",
    "                \"Model Name\": model_name,\n",
    "                # Include Output Layer Name \n",
    "                \"Output Layer Name\": model.output_names[i],\n",
    "                \"Output\": f'Output {i+1}',\n",
    "                \"Input Shape\": model.input_shape,\n",
    "                \"Output Shape\": output_to_check.shape,\n",
    "                \"Output Type\": type(output).__name__,\n",
    "                \"Is One-Hot Encoded\": one_hot_encoded,\n",
    "                \"Suggested Loss\": suggested_loss,\n",
    "                \"Max Probability Class\": max_prob_class,\n",
    "                \"Sample Output\": sample_output,\n",
    "                \"Sample Output (Decoded)\": sample_output_decoded\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(model_output_details).style.set_table_styles(\n",
    "        [{\"selector\": \"th\", \"props\": [(\"font-size\", \"100%\"), (\"text-align\", \"center\")]},\n",
    "         {\"selector\": \"td\", \"props\": [(\"font-size\", \"100%\"), (\"text-align\", \"center\")]}]\n",
    "    )\n",
    "\n",
    "# Assuming train_dataset is already defined and loaded with your training data\n",
    "# Display the analysis\n",
    "analyze_model_outputs(compiled_models, train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
