{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classifier Evaluation\n",
    "## Focus & Astigmatism Performance Analysis\n",
    "\n",
    "- **Author:** [Aaron Woods](https://aaronwoods.info)\n",
    "- **Date Created:** October 26, 2023\n",
    "- **Objective:** This notebook is designated for the evaluation phase, concentrating on in-depth analysis of the model's performance and application in real-world scenarios.\n",
    "\n",
    "## Evaluation Agenda\n",
    "1. **Model Loading**: Retrieve the trained model for analysis.\n",
    "2. **Performance Metrics Analysis**: Examine key performance indicators and metrics.\n",
    "3. **Real-world Testing**: Validate model efficacy in practical scenarios.\n",
    "\n",
    "## Further Actions\n",
    "- Based on the outcomes and insights obtained here, consider revisiting the [Training Notebook](AI_Classify_Train_v1.ipynb) to refine or retrain the model, if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Instructions\n",
    "# 1. Run the shared setup script to prepare the environment and integrate dependencies.\n",
    "# 2. Execute the setup configurations.\n",
    "\n",
    "# Import and run the setup function from the setup module\n",
    "from setup import setup\n",
    "setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import glob\n",
    "import logging\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Union, Any, Optional\n",
    "\n",
    "# Data Manipulation and Numerical Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning and Data Processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.utils import class_weight, resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Deep Learning with TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, callbacks, optimizers, applications\n",
    "\n",
    "# Image Processing and Visualization\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "import seaborn as sns\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "# Experiment Configuration: Sets up the experiment name, random seed, and problem type.\n",
    "experiment_config = {\n",
    "    'NAME': \"Oct26_100pct_CW_SIM\",        \n",
    "    'RANDOM_SEED': 42,                      # Seed for reproducibility\n",
    "    'PROBLEM_TYPE': 'Multi-Output',         # Problem type: Binary, Multi-Class, Multi-Output, Multi-Label\n",
    "}\n",
    "\n",
    "# Model Configuration: Sets up the model parameters.\n",
    "model_config = {\n",
    "    'IMG_SIZE': 224,                        # Image input size\n",
    "    'BATCH_SIZE': 16,                       # Batch size for training\n",
    "    'TRAIN_SIZE': 0.8,                      # Fraction of data to use for training\n",
    "    'VAL_SIZE': 0.5,                        # Fraction of data to use for validation\n",
    "    'EPOCHS': 100,                          # Number of training epochs\n",
    "    'LEARNING_RATE': 0.001,                 # Learning rate\n",
    "    'EARLY_STOPPING_PATIENCE': 50,          # Early stopping patience\n",
    "    'REDUCE_LR_PATIENCE': 3,                # Reduce learning rate on plateau patience\n",
    "    'MIN_LR': 1e-6,                         # Minimum learning rate\n",
    "}\n",
    "\n",
    "# Label Mapping Configuration: Sets up the label mapping for the dataset. (Optional)\n",
    "label_mappings = {\n",
    "    'Focus_Label': {'SharpFocus': 0, 'SlightlyBlurred': 1, 'HighlyBlurred': 2},\n",
    "    'StigX_Label': {'OptimalStig_X': 0, 'ModerateStig_X': 1, 'SevereStig_X': 2},\n",
    "    'StigY_Label': {'OptimalStig_Y': 0, 'ModerateStig_Y': 1, 'SevereStig_Y': 2},\n",
    "}\n",
    "\n",
    "# Augmentation Configuration: Sets up the augmentation parameters.\n",
    "augmentation_config = {\n",
    "    'rotation_factor': 0.002,           # Rotation range (radians)\n",
    "    'height_factor': (-0.18, 0.18),      # Vertical shift range\n",
    "    'width_factor': (-0.18, 0.18),       # Horizontal shift range\n",
    "    'contrast_factor': 0.5,              # Contrast adjustment range\n",
    "}\n",
    "\n",
    "# Combine Experiment, Model, Labels, and Augmentation Configurations\n",
    "config = {\n",
    "    'Experiment': experiment_config,\n",
    "    'Model': model_config,\n",
    "    'Labels': {'MAPPINGS': label_mappings},\n",
    "    'Augmentation': augmentation_config\n",
    "}\n",
    "\n",
    "# Dataset Creation Configuration\n",
    "csv_config = {\n",
    "    'CSV': {\n",
    "        'COLUMNS_TO_READ': ['ImageFile', 'Focus_Offset (V)', 'Stig_Offset_X (V)', 'Stig_Offset_Y (V)']\n",
    "    },\n",
    "    'Thresholds': {\n",
    "        'FOCUS_LOW': 30,                              # Lower focus threshold\n",
    "        'FOCUS_HIGH': 60,                             # Upper focus threshold\n",
    "        'STIGX_LOW': 1,                               # Lower astigmatism threshold (X)\n",
    "        'STIGX_HIGH': 2,                              # Upper astigmatism threshold (X)\n",
    "        'STIGY_LOW': 1,                               # Lower astigmatism threshold (Y)\n",
    "        'STIGY_HIGH': 2,                              # Upper astigmatism threshold (Y)\n",
    "    },\n",
    "    'Paths': {\n",
    "        'OLD_BASE_PATH': \"D:\\\\DOE\\\\\",\n",
    "        # On Simulation Computer\n",
    "        # 'DATA_FILE': \"combined_output.csv\", # Simulation Computer\n",
    "        'DATA_FILE': \"combined_output_cleaned.csv\", # Laptop\n",
    "        # 'NEW_BASE_PATH': \"Y:\\\\User\\\\Aaron-HX38\\\\DOE\\\\\",  # Simulation Computer\n",
    "        'NEW_BASE_PATH': \"C:\\\\Users\\\\aaron.woods\\\\OneDrive - Thermo Fisher Scientific\\\\Desktop\\\\Dec 24\\\\\", # Laptop\n",
    "    },\n",
    "    'SAMPLE_FRAC': 0.1,                                # Fraction of the data for quicker prototyping (1.0 means use all data)\n",
    "}\n",
    "\n",
    "# Update the main configuration dictionary with the dataset configuration\n",
    "config.update(csv_config)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(config['Experiment']['RANDOM_SEED'])\n",
    "tf.random.set_seed(config['Experiment']['RANDOM_SEED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Evaluating Classification Models During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the necessary functions and classes from the provided code\n",
    "\n",
    "\n",
    "# Now, execute the function and plot the metrics using the uploaded Excel file\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "import numpy as np\n",
    "\n",
    "# Custom handler for vertical legend tuples\n",
    "class HandlerTupleVertical(HandlerTuple):\n",
    "    def create_artists(self, legend, orig_handle, xdescent, ydescent, width, height, fontsize, trans):\n",
    "        numlines = len(orig_handle)\n",
    "        handler_map = legend.get_legend_handler_map()\n",
    "        height_y = (height / numlines)\n",
    "        leglines = []\n",
    "        for i, handle in enumerate(orig_handle):\n",
    "            handler = legend.get_legend_handler(handler_map, handle)\n",
    "            legline = handler.create_artists(legend, handle, xdescent, (2*i + 1)*height_y, width, 2*height, fontsize, trans)\n",
    "            leglines.extend(legline)\n",
    "        return leglines\n",
    "\n",
    "# Global variables\n",
    "MODEL_NAME_MAPPING = {\n",
    "    \"mobilenetv2\": \"MobileNetV2 Transfer\",\n",
    "    \"inceptionv3\": \"InceptionV3 Transfer\",\n",
    "    \"resnet50\": \"ResNet50 Transfer\",\n",
    "    \"small_xception\": \"Small Xception\",\n",
    "    \"basic_cnn\": \"Basic CNN\"\n",
    "}\n",
    "LABELS = ['Focus_Label', 'StigX_Label', 'StigY_Label']\n",
    "METRICS = ['loss', 'categorical_accuracy', 'mean_squared_error', 'categorical_crossentropy']\n",
    "\n",
    "# Data processing utilities\n",
    "def get_metrics_from_dataframe(df):\n",
    "    return [col for col in METRICS if any(f\"{label}_{col}\" in df for label in LABELS)]\n",
    "\n",
    "def get_dataframes_from_excel(xls):\n",
    "    model_names = xls.sheet_names\n",
    "    return {model_name: pd.read_excel(xls, model_name) for model_name in model_names}\n",
    "\n",
    "def get_readable_metric_name(metric):\n",
    "    mapping = {\n",
    "        'loss': 'Loss',\n",
    "        'categorical_accuracy': 'Categorical Accuracy',\n",
    "        'categorical_crossentropy': 'Categorical Crossentropy',\n",
    "        'mean_squared_error': 'Mean Squared Error'\n",
    "    }\n",
    "    return mapping.get(metric, metric.capitalize())\n",
    "\n",
    "def moving_average(data, window_size):\n",
    "    \"\"\"Compute the moving average of a dataset.\"\"\"\n",
    "    return data.rolling(window=window_size).mean()\n",
    "\n",
    "def publication_ready_plot(excel_path, epoch_range=None, smoothing_window=None):\n",
    "    \"\"\"Generate a truly publication-ready grid of plots with all adjustments.\"\"\"\n",
    "    # Load data\n",
    "    xls = pd.ExcelFile(excel_path)\n",
    "    dfs = get_dataframes_from_excel(xls)\n",
    "    metrics = get_metrics_from_dataframe(next(iter(dfs.values())))\n",
    "    \n",
    "    # Set up color dictionary for models\n",
    "    model_names = dfs.keys()\n",
    "    readable_model_names = [MODEL_NAME_MAPPING.get(name, name) for name in model_names]\n",
    "    model_palette = sns.color_palette(\"tab10\", len(model_names))\n",
    "    model_color_dict = {readable_name: model_palette[i] for i, readable_name in enumerate(readable_model_names)}\n",
    "    \n",
    "    # Create the grid of plots with adjusted settings\n",
    "    fig, axes = plt.subplots(len(metrics), len(LABELS), figsize=(18, 18 * len(metrics)/len(LABELS)))\n",
    "    \n",
    "    # Titles for columns and rows with enhanced font settings\n",
    "    for j, label in enumerate(LABELS):\n",
    "        axes[0, j].set_title(label, fontsize=25, fontweight='bold', pad=45)\n",
    "    for i, metric in enumerate(metrics):\n",
    "        axes[i, 0].set_ylabel(get_readable_metric_name(metric), fontsize=25, fontweight='bold', labelpad=25)\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        # Enhanced y-limits calculation using percentiles to avoid extreme outliers\n",
    "        all_values = [value for df in dfs.values() for label in LABELS for value in df[f'{label}_{metric}'].tolist() + df[f'val_{label}_{metric}'].tolist()]\n",
    "        lower_bound = np.percentile(all_values, 5)\n",
    "        upper_bound = np.percentile(all_values, 90)\n",
    "        buffer = (upper_bound - lower_bound) * 0.5  \n",
    "        \n",
    "        for j, label in enumerate(LABELS):\n",
    "            ax = axes[i, j]\n",
    "            ax.set_ylim(lower_bound - buffer, upper_bound + buffer)\n",
    "            ax.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "            \n",
    "            # Optional epoch range\n",
    "            if epoch_range:\n",
    "                ax.set_xlim(epoch_range)\n",
    "            \n",
    "            # Declutter y-axis and enhance font settings\n",
    "            if j != 0:\n",
    "                ax.set_yticklabels([])\n",
    "            else:\n",
    "                ax.tick_params(axis='y', labelsize=18)\n",
    "            # Declutter x-axis and enhance font settings\n",
    "            if i != len(metrics) - 1:\n",
    "                ax.set_xticklabels([])\n",
    "            else:\n",
    "                ax.tick_params(axis='x', labelsize=18)\n",
    "                ax.set_xlabel('Epoch', fontsize=20)\n",
    "            \n",
    "            for model_name, df in dfs.items():\n",
    "                readable_model_name = MODEL_NAME_MAPPING.get(model_name, model_name)\n",
    "                color = model_color_dict[readable_model_name]\n",
    "                \n",
    "                # Apply smoothing if specified\n",
    "                if smoothing_window:\n",
    "                    train_data = moving_average(df[f'{label}_{metric}'], smoothing_window)\n",
    "                    val_data = moving_average(df[f'val_{label}_{metric}'], smoothing_window)\n",
    "                else:\n",
    "                    train_data = df[f'{label}_{metric}']\n",
    "                    val_data = df[f'val_{label}_{metric}']\n",
    "                \n",
    "                # Plot training data with enhanced line width\n",
    "                ax.plot(df['epoch'], train_data, '--', color=color, label=f\"{readable_model_name}\", linewidth=3)\n",
    "                \n",
    "                # Plot validation data with enhanced line width\n",
    "                ax.plot(df['epoch'], val_data, '-', color=color, label=f\"{readable_model_name} (Validation)\", linewidth=3)\n",
    "    \n",
    "    # Handle legend with enhanced font settings\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    new_handles = [(handles[j], handles[j + 1]) for j in range(0, len(handles), 2)]\n",
    "    fig.legend(new_handles, labels[::2], handler_map={tuple: HandlerTupleVertical()}, loc=\"lower center\", \n",
    "               borderaxespad=0.1, ncol=len(dfs), fontsize=15, frameon=True, bbox_to_anchor=(0.5, 0.02))\n",
    "\n",
    "\n",
    "    # Adjust layout and add a title for the entire figure\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.93, bottom=0.10)  # Adjust the layout to make space for the legend\n",
    "    fig.suptitle('Model Metrics Comparison', fontsize=30, fontweight='bold', y=1.03)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "excel_path = 'Oct26_100pct_CW_SIM_copy1.xlsx'\n",
    "# Generate the truly publication-ready plot with optional smoothing and epoch range\n",
    "publication_ready_plot(excel_path, epoch_range=(5, 70), smoothing_window=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Comparing Experiment Results (Refactored GraphVersion)\n",
    "\n",
    "# Constants\n",
    "FILE_PATHS = [\n",
    "    \"SIM_Unbalanced.xlsx\",\n",
    "    \"V9_Laptop - Multi-Output.xlsx\",\n",
    "    \"V9.1_Laptop - Multi-Output.xlsx\",\n",
    "]\n",
    "\n",
    "METRIC_VISUALIZATION_KEYS_WITH_EPOCHS = [\n",
    "    (\n",
    "        \"Max Validation Categorical Accuracy\",\n",
    "        \"Max Validation Categorical Accuracy Across Experiments\",\n",
    "        \"Epoch at Max Validation Categorical Accuracy\",\n",
    "    ),\n",
    "    (\n",
    "        \"Min Validation Loss\",\n",
    "        \"Min Validation Loss Across Experiments\",\n",
    "        \"Epoch at Min Validation Loss\",\n",
    "    ),\n",
    "    (\n",
    "        \"Min Validation Categorical Crossentropy\",\n",
    "        \"Min Validation Categorical Crossentropy Across Experiments\",\n",
    "        \"Epoch at Min Validation Categorical Crossentropy\",\n",
    "    ),\n",
    "    (\n",
    "        \"Min Validation Mean Squared Error\",\n",
    "        \"Min Validation Mean Squared Error Across Experiments\",\n",
    "        \"Epoch at Min Validation Mean Squared Error\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "def extract_key_metrics_updated(excel_path):\n",
    "    \"\"\"Extract key metrics from the provided Excel file without using determine_engine.\"\"\"\n",
    "    # Load the Excel file and get DataFrames\n",
    "    xls = pd.ExcelFile(excel_path, engine=\"openpyxl\")\n",
    "    dfs = get_dataframes_from_excel(xls)\n",
    "\n",
    "    return {\n",
    "        model_name: {\n",
    "            \"Min Validation Loss\": df[\"val_loss\"].min(),\n",
    "            \"Epoch at Min Validation Loss\": df[\"epoch\"].iloc[\n",
    "                df[\"val_loss\"].idxmin()\n",
    "            ],\n",
    "            \"Max Validation Categorical Accuracy\": df[\n",
    "                \"val_categorical_accuracy\"\n",
    "            ].max(),\n",
    "            \"Epoch at Max Validation Categorical Accuracy\": df[\"epoch\"].iloc[\n",
    "                df[\"val_categorical_accuracy\"].idxmax()\n",
    "            ],\n",
    "            \"Min Validation Categorical Crossentropy\": df[\n",
    "                \"val_categorical_crossentropy\"\n",
    "            ].min(),\n",
    "            \"Epoch at Min Validation Categorical Crossentropy\": df[\n",
    "                \"epoch\"\n",
    "            ].iloc[df[\"val_categorical_crossentropy\"].idxmin()],\n",
    "            \"Min Validation Mean Squared Error\": df[\n",
    "                \"val_mean_squared_error\"\n",
    "            ].min(),\n",
    "            \"Epoch at Min Validation Mean Squared Error\": df[\"epoch\"].iloc[\n",
    "                df[\"val_mean_squared_error\"].idxmin()\n",
    "            ],\n",
    "        }\n",
    "        for model_name, df in dfs.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_and_map_metrics(excel_path, name_mapping, metric_key, epoch_key):\n",
    "    \"\"\"Extract and map metrics using the provided name mapping.\"\"\"\n",
    "    experiment_name = os.path.basename(excel_path).replace(\".xlsx\", \"\")\n",
    "    metrics = extract_key_metrics_updated(excel_path)\n",
    "\n",
    "    # Use the name_mapping to rename models\n",
    "    mapped_metric_values = {\n",
    "        name_mapping.get(model, model): data[metric_key] for model, data in metrics.items()\n",
    "    }\n",
    "    mapped_epoch_values = {\n",
    "        name_mapping.get(model, model): data[epoch_key] for model, data in metrics.items()\n",
    "    }\n",
    "\n",
    "    return experiment_name, mapped_metric_values, mapped_epoch_values\n",
    "\n",
    "\n",
    "def visualize_mapped_metrics(metric_data, epoch_data, metric_title, metric_key):\n",
    "    \"\"\"Visualize a specific metric across experiments and annotate bars with epoch numbers.\"\"\"\n",
    "    metric_df = pd.DataFrame(metric_data).transpose()\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    ax = metric_df.plot(kind=\"bar\", ax=plt.gca(), colormap=\"viridis\")\n",
    "    plt.title(metric_title, fontsize=18)\n",
    "    plt.ylabel(metric_key, fontsize=16)\n",
    "    plt.xlabel(\"Experiment\", fontsize=16)\n",
    "    plt.legend(title=\"Models\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12, title_fontsize=14)\n",
    "    plt.xticks(rotation=45, fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    ax.grid(axis='y')\n",
    "    \n",
    "    # Annotate bars with epoch numbers\n",
    "    for idx, rect in enumerate(ax.patches):\n",
    "        experiment_idx = idx // len(metric_df.columns)\n",
    "        model_idx = idx % len(metric_df.columns)\n",
    "        experiment_name = metric_df.index[experiment_idx]\n",
    "        model_name = metric_df.columns[model_idx]\n",
    "        epoch_value = epoch_data[experiment_name][model_name]\n",
    "        ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height(), f'Ep {epoch_value}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main_visualization(file_paths, name_mapping, metric_visualization_keys_with_epochs):\n",
    "    \"\"\"Main function for visualization.\"\"\"\n",
    "    for metric_key, metric_title, epoch_key in metric_visualization_keys_with_epochs:\n",
    "        metric_data = {}\n",
    "        epoch_data = {}\n",
    "\n",
    "        for excel_path in file_paths:\n",
    "            experiment_name, mapped_metric_values, mapped_epoch_values = extract_and_map_metrics(\n",
    "                excel_path, name_mapping, metric_key, epoch_key\n",
    "            )\n",
    "            metric_data[experiment_name] = mapped_metric_values\n",
    "            epoch_data[experiment_name] = mapped_epoch_values\n",
    "\n",
    "        visualize_mapped_metrics(metric_data, epoch_data, metric_title, metric_key)\n",
    "\n",
    "\n",
    "# Execute the main function\n",
    "main_visualization(\n",
    "    FILE_PATHS, model_name_mapping, METRIC_VISUALIZATION_KEYS_WITH_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading and Testing Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading the Best Model from Directories\n",
    "\n",
    "def get_best_model_filename(directory):\n",
    "    \"\"\"Identify the best model filename based on the minimum validation loss from the directory.\"\"\"\n",
    "    model_files = [f for f in os.listdir(directory) if f.endswith('.h5')]\n",
    "    if not model_files:\n",
    "        print(f\"No model files found in {directory}\")\n",
    "        return None\n",
    "    return min(model_files, key=lambda x: float(x.split('val_loss_')[1].split('_')[0]))\n",
    "\n",
    "def load_best_model(directory):\n",
    "    \"\"\"Loads the best model from the specified directory.\"\"\"\n",
    "    best_model_file = get_best_model_filename(directory)\n",
    "    if not best_model_file:\n",
    "        return None\n",
    "    best_model_path = os.path.join(directory, best_model_file)\n",
    "    # return load_model(best_model_path)\n",
    "    return load_model(best_model_path, compile=False)\n",
    "\n",
    "def get_label_directories(experiment_directory):\n",
    "    \"\"\"Determine label directories or just model directories in the experiment directory.\"\"\"\n",
    "    first_level_dirs = [os.path.join(experiment_directory, d) for d in os.listdir(experiment_directory) \n",
    "                        if os.path.isdir(os.path.join(experiment_directory, d))]\n",
    "    if any('mobilenetv2' in dir_name for dir_name in first_level_dirs):\n",
    "        return [experiment_directory]\n",
    "    return first_level_dirs\n",
    "\n",
    "def load_all_best_models(experiment_directory):\n",
    "    \"\"\"Load the best model for each model type within the experiment directory.\"\"\"\n",
    "    best_models = {}\n",
    "    label_dirs = get_label_directories(experiment_directory)\n",
    "    for label_dir in label_dirs:\n",
    "        for model_name in ['mobilenetv2', 'inceptionv3', 'resnet50', 'small_xception', 'basic_cnn']:\n",
    "            model_dir = os.path.join(label_dir, model_name)\n",
    "            best_model = load_best_model(model_dir)\n",
    "            if best_model:\n",
    "                key_name = f\"{os.path.basename(label_dir)}_{model_name}\"\n",
    "                best_models[key_name] = best_model\n",
    "    return best_models\n",
    "\n",
    "# Example Usage\n",
    "experiment_directory = \"SIM_Unbalanced\"\n",
    "all_best_models = load_all_best_models(experiment_directory)\n",
    "print(all_best_models.keys())  # This will display the keys of the loaded models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating and Debugging Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Displaying Model Outputs in a Table (Refactored GraphVersion)\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "def predict_with_model(model, sample_input):\n",
    "    \"\"\"Predict the output using the given model and input.\"\"\"\n",
    "    output = model.predict(sample_input)\n",
    "    \n",
    "    # Convert TensorFlow tensor to numpy array if needed\n",
    "    if hasattr(output, 'numpy'):\n",
    "        output = output.numpy()\n",
    "    \n",
    "    return output\n",
    "\n",
    "def capture_model_output_details(model_name, model, sample_input):\n",
    "    \"\"\"Capture details of model's output for a given input.\"\"\"\n",
    "    output = predict_with_model(model, sample_input)\n",
    "    return {\n",
    "        \"Model Name\": model_name,\n",
    "        \"Input Shape\": model.input_shape,\n",
    "        \"Output Shape\": output.shape,\n",
    "        \"Output Type\": type(output).__name__,\n",
    "        \"Sample Output\": output[0] if len(output) > 0 else \"No Output\"\n",
    "    }\n",
    "\n",
    "def display_model_outputs_in_table(all_best_models, sample_input):\n",
    "    \"\"\"Display details of model's outputs in a tabular format.\"\"\"\n",
    "    model_output_details = [capture_model_output_details(model_name, model, sample_input) \n",
    "                            for model_name, model in all_best_models.items()]\n",
    "    \n",
    "    print(tabulate(model_output_details, headers=\"keys\", tablefmt=\"grid\"))\n",
    "\n",
    "# Sample Usage\n",
    "sample_input = test_images\n",
    "display_model_outputs_in_table(all_best_models, sample_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inspecting the first few samples from the test dataset (Refactored GraphVersion)\n",
    "\n",
    "def inspect_test_samples(test_dataset, label_encoders, num_samples_to_inspect=3):\n",
    "    \"\"\"\n",
    "    Inspect a specified number of samples from a test dataset.\n",
    "    \n",
    "    Args:\n",
    "    - test_dataset (tf.data.Dataset): The test dataset to inspect.\n",
    "    - label_encoders (dict): A dictionary of label encoders.\n",
    "    - num_samples_to_inspect (int, optional): Number of samples to inspect. Defaults to 3.\n",
    "    \n",
    "    Returns:\n",
    "    - df: Styled DataFrame with inspected samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load a batch of test images, labels, and offset values\n",
    "    test_images, test_labels_list, test_offsets = next(iter(test_dataset.take(1)))\n",
    "    # test_images, test_labels_list = next(iter(test_dataset.take(1)))\n",
    "    test_labels_array = np.stack([np.array(label) for label in test_labels_list])\n",
    "\n",
    "    # Prepare data for DataFrame\n",
    "    data = []\n",
    "    for i in range(num_samples_to_inspect):\n",
    "        sample_data = {\n",
    "            \"Sample\": i + 1,\n",
    "            \"Image shape\": str(test_images[i].shape),\n",
    "            \"Image values (first few)\": f\"{str(test_images[i].numpy().flatten()[:10])}...\",\n",
    "            \"Labels shape\": str(test_labels_array[i].shape),\n",
    "            \"Labels values\": str(test_labels_array[i]),\n",
    "        }\n",
    "\n",
    "        for label, value in zip(label_encoders.keys(), test_labels_array[i]):\n",
    "            sample_data[f\"{label} (Decoded)\"] = label_encoders[label].inverse_transform([value])[0]\n",
    "\n",
    "        data.append(sample_data)\n",
    "\n",
    "    # Create and display DataFrame\n",
    "    df = pd.DataFrame(data).set_index(\"Sample\")\n",
    "    return df.style.hide_index()\n",
    "\n",
    "# Sample Usage\n",
    "styled_df = inspect_test_samples(train_dataset, label_encoders)\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Displaying Images from the Test Dataset (Refactored GraphVersion)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def decode_single_label(value, label, label_encoders):\n",
    "    \"\"\"Decode a single label.\"\"\"\n",
    "    decoded_label = label_encoders[label].inverse_transform([value])[0]\n",
    "    \n",
    "    simple_mappings = {\n",
    "        'SharpFocus': 'Optimal',\n",
    "        'SlightlyBlurred': 'Moderate',\n",
    "        'HighlyBlurred': 'Severe',\n",
    "        'OptimalStig_X': 'Optimal',\n",
    "        'ModerateStig_X': 'Moderate',\n",
    "        'SevereStig_X': 'Severe',\n",
    "        'OptimalStig_Y': 'Optimal',\n",
    "        'ModerateStig_Y': 'Moderate',\n",
    "        'SevereStig_Y': 'Severe'\n",
    "    }\n",
    "    \n",
    "    return simple_mappings.get(decoded_label, decoded_label)\n",
    "\n",
    "def decode_labels(encoded_labels, label_encoders):\n",
    "    \"\"\"Decode a set of labels.\"\"\"\n",
    "    return [decode_single_label(value, label, label_encoders) for label, value in zip(label_encoders.keys(), encoded_labels)]\n",
    "\n",
    "def format_label_for_display(label, value, offset):\n",
    "    \"\"\"Format the label for display on the image.\"\"\"\n",
    "    label = label.replace(\"_Label\", \"\").replace(\"StigX\", \"Astigmatism (X direction)\").replace(\"StigY\", \"Astigmatism (Y direction)\")\n",
    "    return f\"{label}: {value} (Offset: {offset:.2f})\"\n",
    "\n",
    "def display_samples_with_labels(images, decoded_labels, offsets):\n",
    "    \"\"\"Display images with their decoded labels.\"\"\"\n",
    "    num_samples_to_display = min(3, len(images))\n",
    "    fig, axes = plt.subplots(1, num_samples_to_display, figsize=(15, 5))\n",
    "    for img, labels, offset, ax in zip(images[:num_samples_to_display], \n",
    "                                       decoded_labels[:num_samples_to_display], \n",
    "                                       offsets[:num_samples_to_display], \n",
    "                                       axes):\n",
    "        ax.imshow(img.numpy())\n",
    "        formatted_title = [format_label_for_display(label, value, off) for label, value, off in zip(label_encoders.keys(), labels, offset.numpy())]\n",
    "        ax.set_title(\"\\n\".join(formatted_title))\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Decode the labels of the test samples\n",
    "test_images, test_labels_list, test_offsets = next(iter(test_dataset.take(1)))\n",
    "# test_images, test_labels_list = next(iter(test_dataset.take(1)))\n",
    "\n",
    "test_labels_array = np.stack([np.array(label) for label in test_labels_list])\n",
    "decoded_labels = [decode_labels(labels, label_encoders) for labels in test_labels_array]\n",
    "\n",
    "# Display the test samples\n",
    "display_samples_with_labels(test_images, decoded_labels, test_offsets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Version Code for Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def make_confusion_matrix(y_true, y_pred, classes=None, encoder=None, figsize=(10, 10), text_size=15, norm=False, savefig=False, output_num=None):\n",
    "    \"\"\"\n",
    "    Makes a labelled confusion matrix comparing predictions and ground truth labels.\n",
    "\n",
    "    Args:\n",
    "    y_true: Array of truth labels.\n",
    "    y_pred: Array of predicted labels.\n",
    "    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n",
    "    encoder: Fitted LabelEncoder to transform integer labels back to original string labels.\n",
    "    ... (other arguments as before)\n",
    "\n",
    "    Returns:\n",
    "    A labelled confusion matrix plot comparing y_true and y_pred.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Error handling for array shapes\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        print(f\"Error: Mismatch in shapes of y_true ({y_true.shape}) and y_pred ({y_pred.shape}). Skipping this output.\")\n",
    "        return\n",
    "    \n",
    "    if encoder:\n",
    "        y_true = encoder.inverse_transform(y_true)\n",
    "        y_pred = encoder.inverse_transform(y_pred)\n",
    "    \n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]  # normalize it\n",
    "    n_classes = cm.shape[0]  # find the number of classes we're dealing with\n",
    "\n",
    "    # Plot the figure and make it pretty\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    cax = ax.matshow(cm, cmap=plt.cm.Blues)  # colors will represent how 'correct' a class is, darker == better\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Are there a list of classes?\n",
    "    labels = classes if classes else np.arange(cm.shape[0])\n",
    "    \n",
    "    # Label the axes\n",
    "    ax.set(title=\"Confusion Matrix\",\n",
    "           xlabel=\"Predicted label\",\n",
    "           ylabel=\"True label\",\n",
    "           xticks=np.arange(n_classes),\n",
    "           yticks=np.arange(n_classes),\n",
    "           xticklabels=labels,\n",
    "           yticklabels=labels)\n",
    "\n",
    "    # Make x-axis labels appear on bottom\n",
    "    ax.xaxis.set_label_position(\"bottom\")\n",
    "    ax.xaxis.tick_bottom()\n",
    "\n",
    "    # Set the threshold for different colors\n",
    "    threshold = (cm.max() + cm.min()) / 2.\n",
    "\n",
    "    # Plot the text on each cell\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        text = f\"{cm[i, j]}\" if not norm else f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\"\n",
    "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, text, horizontalalignment=\"center\", color=color, size=text_size)\n",
    "\n",
    "    # Save the figure to the current working directory\n",
    "    if savefig and output_num is not None:\n",
    "        fig.savefig(f\"confusion_matrix_output_{output_num + 1}.png\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_confusion_matrix_multi_output(y_true, y_preds, classes=None, encoders=None, figsize=(10, 10), text_size=15, norm=False, savefig=False): \n",
    "    \"\"\"\n",
    "    Makes a labelled confusion matrix comparing predictions and ground truth labels for multi-output models.\n",
    "\n",
    "    Args:\n",
    "    y_true: 2D array of truth labels for multiple outputs.\n",
    "    y_preds: 2D array of predicted labels for multiple outputs.\n",
    "    classes: List of arrays of class labels (e.g. string form) for each output. If `None`, integer labels are used.\n",
    "    encoders: List of fitted LabelEncoders to transform integer labels back to original string labels for each output.\n",
    "    ... (other arguments as before)\n",
    "\n",
    "    Returns:\n",
    "    A list of labelled confusion matrix plots comparing y_true and y_preds for each output.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Error handling for array shapes\n",
    "    if y_true.shape != y_preds.shape:\n",
    "        print(f\"Error: Mismatch in shapes of y_true ({y_true.shape}) and y_preds ({y_preds.shape}). Cannot proceed.\")\n",
    "        return\n",
    "    \n",
    "    num_outputs = y_true.shape[1]\n",
    "    \n",
    "    for i in range(num_outputs):\n",
    "        print(f\"Confusion Matrix for Output {i + 1}:\")\n",
    "        encoder = encoders[i] if encoders else None\n",
    "        make_confusion_matrix(y_true[:, i], np.argmax(y_preds[:, i], axis=1), classes[i] if classes else None, encoder, figsize, text_size, norm, savefig, i)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for Confusion Matrix\n",
    "\n",
    "def make_confusion_matrix_multi_output(y_true, y_preds, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False): \n",
    "    \"\"\"\n",
    "    Makes a labelled confusion matrix comparing predictions and ground truth labels for multi-output models.\n",
    "\n",
    "    Args:\n",
    "    y_true: List of arrays of truth labels (must be same shape as y_preds).\n",
    "    y_preds: List of arrays of predicted labels (must be same shape as y_true).\n",
    "    classes: List of arrays of class labels (e.g. string form). If `None`, integer labels are used.\n",
    "    ... (other arguments as before)\n",
    "\n",
    "    Returns:\n",
    "    A list of labelled confusion matrix plots comparing y_true and y_preds for each output.\n",
    "    \"\"\"\n",
    "    # Check if y_true and y_preds are lists, if not, convert them to lists (for single-output compatibility)\n",
    "    if not isinstance(y_true, list):\n",
    "        y_true = [y_true]\n",
    "    if not isinstance(y_preds, list):\n",
    "        y_preds = [y_preds]\n",
    "    \n",
    "    for i, (true, pred) in enumerate(zip(y_true, y_preds)):\n",
    "        print(f\"Output {i + 1}:\")\n",
    "        make_confusion_matrix(true, pred, classes[i] if classes else None, figsize, text_size, norm, savefig)\n",
    "        plt.show()\n",
    "\n",
    "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False):\n",
    "    # (The function content remains the same as you provided)\n",
    "\n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]  # normalize it\n",
    "    n_classes = cm.shape[0]  # find the number of classes we're dealing with\n",
    "\n",
    "    # Plot the figure and make it pretty\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    cax = ax.matshow(cm, cmap=plt.cm.Blues)  # colors will represent how 'correct' a class is, darker == better\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Are there a list of classes?\n",
    "    labels = classes if classes else np.arange(cm.shape[0])\n",
    "    \n",
    "    # Label the axes\n",
    "    ax.set(title=\"Confusion Matrix\",\n",
    "           xlabel=\"Predicted label\",\n",
    "           ylabel=\"True label\",\n",
    "           xticks=np.arange(n_classes),\n",
    "           yticks=np.arange(n_classes),\n",
    "           xticklabels=labels,\n",
    "           yticklabels=labels)\n",
    "\n",
    "    # Make x-axis labels appear on bottom\n",
    "    ax.xaxis.set_label_position(\"bottom\")\n",
    "    ax.xaxis.tick_bottom()\n",
    "\n",
    "    # Set the threshold for different colors\n",
    "    threshold = (cm.max() + cm.min()) / 2.\n",
    "\n",
    "    # Plot the text on each cell\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if norm:\n",
    "            plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > threshold else \"black\",\n",
    "                     size=text_size)\n",
    "        else:\n",
    "            plt.text(j, i, f\"{cm[i, j]}\",\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > threshold else \"black\",\n",
    "                     size=text_size)\n",
    "\n",
    "    # Save the figure to the current working directory\n",
    "    if savefig:\n",
    "        fig.savefig(f\"confusion_matrix_output_{i + 1}.png\")\n",
    "        \n",
    "        \n",
    "        \n",
    "### Attempt at Using the Confusion Matrix Function\n",
    "\n",
    "def extract_labels_from_dataset(dataset, problem_type):\n",
    "    \"\"\"\n",
    "    Extract labels from a TensorFlow dataset based on the problem type.\n",
    "    \n",
    "    Args:\n",
    "    - dataset (tf.data.Dataset): The TensorFlow dataset to extract labels from.\n",
    "    - problem_type (str): The type of problem ('Multi-Output', 'Multi-Class', or 'Binary').\n",
    "    \n",
    "    Returns:\n",
    "    - numpy array or dict: If 'Multi-Output', returns a numpy array with shape (num_samples, num_outputs).\n",
    "                           If 'Multi-Class' or 'Binary', returns a dictionary with label types as keys and \n",
    "                           arrays of labels as values.\n",
    "    \"\"\"\n",
    "    \n",
    "    if problem_type == 'Multi-Output':\n",
    "        labels_list = [labels for _, labels in dataset]\n",
    "        return np.array(labels_list).squeeze()\n",
    "\n",
    "    elif problem_type in ['Multi-Class', 'Binary']:\n",
    "        labels_dict = {}\n",
    "        for label_type in ['Focus_Label', 'StigX_Label', 'StigY_Label']:\n",
    "            label_data = []\n",
    "            for _, labels in dataset[label_type]:\n",
    "                label_data.extend(labels.numpy())\n",
    "            labels_dict[label_type] = np.array(label_data)\n",
    "        return labels_dict\n",
    "\n",
    "    else:\n",
    "        print(f\"Unknown problem type: {problem_type}\")\n",
    "        return None\n",
    "\n",
    "# Usage\n",
    "problem_type = config['Experiment']['PROBLEM_TYPE']\n",
    "test_labels = extract_labels_from_dataset(test_dataset, problem_type)\n",
    "\n",
    "# Choose a specific model (replace 'specific_model_name' with the actual model name you're interested in)\n",
    "model_name = 'specific_model_name'\n",
    "model = all_best_models[model_name]\n",
    "\n",
    "if not model:\n",
    "    print(f\"No model found for {model_name}\")\n",
    "    exit()\n",
    "\n",
    "# 1. Predict on the test data\n",
    "predictions = model.predict(test_dataset)\n",
    "\n",
    "\n",
    "# 2. Get true labels and predictions for each output\n",
    "# Assuming test_labels is a list where each item is an array of true labels for a given output\n",
    "true_labels = [test_labels[i] for i in range(len(predictions))]\n",
    "predicted_labels = [np.argmax(predictions[i], axis=1) for i in range(len(predictions))]\n",
    "\n",
    "# List of class names for each output, assuming they are the same for all outputs in this example\n",
    "classes_list = [list(range(3)) for _ in range(len(predictions))]\n",
    "\n",
    "# 3. Generate confusion matrices\n",
    "make_confusion_matrix_multi_output(true_labels, predicted_labels, classes_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Incorporate into main notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_labels_with_probabilities(decoded_labels, probabilities):\n",
    "    \"\"\"\n",
    "    Construct display strings that combine decoded labels with their corresponding probabilities.\n",
    "    \n",
    "    Parameters:\n",
    "    - decoded_labels: A list of decoded label strings.\n",
    "    - probabilities: A list of probability values corresponding to the decoded labels.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of strings combining labels with their probabilities.\n",
    "    \"\"\"\n",
    "    return [f\"{label} ({prob:.2f})\" for label, prob in zip(decoded_labels, list(probabilities))]\n",
    "\n",
    "# Load a batch of test images and labels\n",
    "test_images, test_labels_list = next(iter(test_dataset.take(1)))\n",
    "test_labels_array = np.stack([np.array(label) for label in test_labels_list])\n",
    "\n",
    "# Make predictions using the selected model\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "# Decode the labels of the first few samples\n",
    "decoded_labels = [decode_labels(labels, label_encoders) for labels in test_labels_array]\n",
    "# Extract probabilities for the predicted labels\n",
    "predicted_probs = [predictions[i][np.argmax(predictions[i])] for i in range(predictions.shape[0])]\n",
    "display_strings = [display_labels_with_probabilities(labels, probs) for labels, probs in zip(decoded_labels, predicted_probs)]\n",
    "\n",
    "# Display the images along with their decoded labels and probabilities\n",
    "num_samples_to_display = 3\n",
    "fig, axes = plt.subplots(1, num_samples_to_display, figsize=(15, 5))\n",
    "for img, labels_with_probs, ax in zip(test_images[:num_samples_to_display], display_strings[:num_samples_to_display], axes):\n",
    "    ax.imshow(img)\n",
    "    title = \"\\n\".join(labels_with_probs)\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_labels, pred_classes))\n",
    "\n",
    "# Note on classification_report():\n",
    "# - Precision: Proportion of true positives. Higher precision -> fewer false positives.\n",
    "# - Recall: Proportion of true positives over actual positives. Higher recall -> fewer false negatives.\n",
    "# - F1 score: Harmonic mean of precision and recall. Range: [0, 1].\n",
    "\n",
    "# Convert the report to a dictionary for further processing\n",
    "classification_report_dict = classification_report(y_labels, pred_classes, output_dict=True)\n",
    "\n",
    "# Focusing on f1-score as it combines precision and recall, extract it into a new dictionary\n",
    "class_f1_scores = {class_names[int(k)]: v[\"f1-score\"] for k, v in classification_report_dict.items() if k != \"accuracy\"}\n",
    "\n",
    "class_f1_scores\n",
    "\n",
    "# Convert dictionary to DataFrame and sort by f1-score in descending order\n",
    "f1_scores_df = pd.DataFrame({\n",
    "    \"class_name\": class_f1_scores.keys(),\n",
    "    \"f1-score\": class_f1_scores.values()\n",
    "}).sort_values(\"f1-score\", ascending=False)\n",
    "\n",
    "# Visualize f1-scores using a horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 25))\n",
    "bars = ax.barh(f1_scores_df[\"class_name\"], f1_scores_df[\"f1-score\"])\n",
    "ax.set_xlabel(\"f1-score\")\n",
    "ax.set_title(\"F1-Scores for Different Classes\")\n",
    "ax.invert_yaxis()  # Reverse order for clarity\n",
    "\n",
    "# Annotate bars with their respective scores\n",
    "for bar in bars:\n",
    "    ax.text(bar.get_width() * 1.03, bar.get_y() + bar.get_height() / 1.5,\n",
    "            f\"{bar.get_width():.2f}\", ha='center', va='bottom')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing Most Wrong Predictions:\n",
    "# Observing predictions with high confidence but incorrect can provide insights into model performance.\n",
    "# Steps:\n",
    "# 1. Get test image filepaths.\n",
    "# 2. Create a DataFrame for image details and predictions.\n",
    "# 3. Identify correct predictions.\n",
    "# 4. Extract top 100 most confidently incorrect predictions.\n",
    "# 5. Visualize some of the most wrong examples.\n",
    "\n",
    "# 1. Get the filenames of all test data\n",
    "filepaths = [filepath.numpy() for filepath in test_data.list_files(\"101_food_classes_10_percent/test/*/*.jpg\", shuffle=False)]\n",
    "\n",
    "# 2. Create a dataframe for prediction analysis\n",
    "import pandas as pd\n",
    "pred_df = pd.DataFrame({\n",
    "    \"img_path\": filepaths,\n",
    "    \"y_true\": y_labels,\n",
    "    \"y_pred\": pred_classes,\n",
    "    \"pred_conf\": pred_probs.max(axis=1),\n",
    "    \"y_true_classname\": [class_names[i] for i in y_labels],\n",
    "    \"y_pred_classname\": [class_names[i] for i in pred_classes]\n",
    "})\n",
    "\n",
    "# 3. Identify correct predictions\n",
    "pred_df[\"pred_correct\"] = pred_df[\"y_true\"] == pred_df[\"y_pred\"]\n",
    "\n",
    "# 4. Extract top 100 confidently incorrect predictions\n",
    "top_100_wrong = pred_df[pred_df[\"pred_correct\"] == False].sort_values(\"pred_conf\", ascending=False).head(100)\n",
    "\n",
    "# 5. Visualize some of the most wrong examples\n",
    "images_to_view = 9\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, row in enumerate(top_100_wrong.head(images_to_view).itertuples()): \n",
    "    plt.subplot(3, 3, i+1)\n",
    "    img = load_and_prep_image(row.img_path, scale=True)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"actual: {row.y_true_classname}, pred: {row.y_pred_classname} \\nprob: {row.pred_conf:.2f}\")\n",
    "    plt.axis(False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
