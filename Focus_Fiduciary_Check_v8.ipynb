{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Focus and Astigmatism Classifier\n",
    "**Author:** [Aaron Woods](https://aaronwoods.info)  \n",
    "**Date Created:** September 12, 2023  \n",
    "**Description:** This script provides an end-to-end machine learning pipeline to classify images as either \"In Focus\" or \"Out of Focus\", and additionally identifies astigmatism-related issues.  \n",
    "**Repository:** [Image Classification on VSCode](https://insiders.vscode.dev/tunnel/midnightsim/c:/Users/User/Desktop/Image-Classification)\n",
    "\n",
    "### Overview\n",
    "The script features a comprehensive pipeline that ingests data from Excel spreadsheets and feeds it into various machine learning models. The design is modular, allowing for easy adaptability to address different image classification problems, including focus quality and astigmatism detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# TensorFlow Installation with GPU Support\n",
    "# ------------------------------\n",
    "# Note: TensorFlow versions above 2.10 are not supported on GPUs on native Windows installations.\n",
    "# For more details, visit: https://www.tensorflow.org/install/pip#windows-wsl2_1\n",
    "# Uncomment the following line to install TensorFlow if needed.\n",
    "# %pip install \"tensorflow<2.11\"\n",
    "\n",
    "# ------------------------------\n",
    "# System and TensorFlow Info Check\n",
    "# ------------------------------\n",
    "# Import necessary libraries and initialize an empty dictionary to store system information.\n",
    "import platform\n",
    "system_info = {\"Platform\": platform.platform(), \"Python Version\": platform.python_version()}\n",
    "\n",
    "# Try importing TensorFlow and collecting relevant system information.\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    system_info.update({\n",
    "        \"TensorFlow Version\": tf.__version__,\n",
    "        \"Num GPUs Available\": len(tf.config.list_physical_devices('GPU'))\n",
    "    })\n",
    "    system_info['Instructions'] = (\n",
    "        \"You're all set to run your model on a GPU.\" \n",
    "        if system_info['Num GPUs Available'] \n",
    "        else (\n",
    "            \"No GPUs found. To use a GPU, follow these steps:\\n\"\n",
    "            \"  1. Install NVIDIA drivers for your GPU.\\n\"\n",
    "            \"  2. Install a compatible CUDA toolkit.\\n\"\n",
    "            \"  3. Install the cuDNN library.\\n\"\n",
    "            \"  4. Make sure to install the GPU version of TensorFlow.\"\n",
    "        )\n",
    "    )\n",
    "except ModuleNotFoundError:\n",
    "    system_info['Instructions'] = (\n",
    "        \"TensorFlow is not installed. \"\n",
    "        \"Install it using pip by running: !pip install tensorflow\"\n",
    "    )\n",
    "\n",
    "# Format and display the gathered system information.\n",
    "formatted_info = \"\\n\".join(f\"{key}: {value}\" for key, value in system_info.items())\n",
    "print(formatted_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Package Installation (Optional)\n",
    "# ------------------------------\n",
    "# Uncomment the following lines to install required packages if running on a new machine.\n",
    "# To suppress the output, we use '> /dev/null 2>&1'.\n",
    "# %pip install numpy pandas matplotlib protobuf seaborn scikit-learn tensorflow > /dev/null 2>&1\n",
    "\n",
    "# ------------------------------\n",
    "# Import Libraries\n",
    "# ------------------------------\n",
    "\n",
    "# Standard Libraries\n",
    "import os, sys, random, math, glob, logging\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Third-Party Libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.callbacks import TensorBoard, Callback\n",
    "from tensorflow.keras.applications import InceptionV3, ResNet50\n",
    "from keras.models import load_model\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Type Annotations\n",
    "from typing import List, Dict, Tuple, Union, Any, Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration dictionary\n",
    "config = {\n",
    "    'Experiment': {\n",
    "        'NAME': \"Multi-Label_Thresholds-30-60-1-2\",  # Experiment name\n",
    "        'RANDOM_SEED': 42,  # Seed for reproducibility\n",
    "        'PROBLEM_TYPE': 'Multi-Class',  # Problem type: Binary, Multi-Class, Multi-Label\n",
    "    },\n",
    "    'Model': {\n",
    "        'IMG_SIZE': 224,  # Image input size\n",
    "        'BATCH_SIZE': 32,  # Batch size for training\n",
    "        'EPOCHS': 100,  # Number of epochs\n",
    "        'LEARNING_RATE': 1e-3,  # Learning rate\n",
    "        'EARLY_STOPPING_PATIENCE': 5,  # Early stopping patience parameter\n",
    "        'REDUCE_LR_PATIENCE': 3,  # Learning rate reduction patience parameter\n",
    "        'MIN_LR': 1e-6,  # Minimum learning rate\n",
    "        'LOSS': \"binary_crossentropy\",  # Loss function: \"categorical_crossentropy\" for multi-class\n",
    "        'TRAIN_SIZE': 0.8,  # Fraction of data to use for training\n",
    "        'VAL_SIZE': 0.5,  # Fraction of data to use for validation\n",
    "    },\n",
    "    'Labels': {\n",
    "        'MAPPINGS': {  # Class label mappings\n",
    "            'Focus_Label': {'SharpFocus': 0, 'SlightlyBlurred': 1, 'HighlyBlurred': 2},\n",
    "            'StigX_Label': {'OptimalStig_X': 0, 'ModerateStig_X': 1, 'SevereStig_X': 2},\n",
    "            'StigY_Label': {'OptimalStig_Y': 0, 'ModerateStig_Y': 1, 'SevereStig_Y': 2},\n",
    "        }\n",
    "    },\n",
    "    'Augmentation': {  # Data augmentation parameters\n",
    "        'rotation_factor': 0.002,\n",
    "        'height_factor': (-0.18, 0.18),\n",
    "        'width_factor': (-0.18, 0.18),\n",
    "        'contrast_factor': 0.5,\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(config['Experiment']['RANDOM_SEED'])\n",
    "tf.random.set_seed(config['Experiment']['RANDOM_SEED'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations for Loss Functions and Other Settings Per Problem Type\n",
    "\n",
    "#### Multi-Label Problems:\n",
    "- **Loss Function**: Typically, \"binary_crossentropy\" is used because each class label is independent and the task is to predict whether it is present or not.\n",
    "- **Label Encoding**: One-hot encoding is commonly used where each label is considered as a separate class.\n",
    "- **Activation Function**: The sigmoid activation function is generally used in the output layer to allow for multiple independent classes.\n",
    "- **Evaluation Metrics**: Precision, Recall, and F1 Score can be effective for evaluating multi-label problems.\n",
    "\n",
    "#### Binary Classification Problems:\n",
    "- **Loss Function**: \"binary_crossentropy\" is the standard loss function because the task is to categorize instances into one of the two classes.\n",
    "- **Label Encoding**: Labels are often encoded as 0 or 1.\n",
    "- **Activation Function**: The sigmoid activation function is usually used in the output layer, producing a probability score that can be thresholded to yield a class label.\n",
    "- **Evaluation Metrics**: Accuracy, Precision, Recall, and AUC-ROC are commonly used metrics.\n",
    "\n",
    "#### Multi-Class Problems:\n",
    "- **Loss Function**: \"categorical_crossentropy\" or \"sparse_categorical_crossentropy\" is commonly used. The former requires one-hot encoded labels, while the latter requires integer labels.\n",
    "- **Label Encoding**: One-hot encoding is often used to convert the categorical labels into a format that can be provided to the neural network.\n",
    "- **Activation Function**: The softmax activation function is used in the output layer to produce a probability distribution over the multiple classes.\n",
    "- **Evaluation Metrics**: Accuracy is the most straightforward metric. However, Precision, Recall, and F1 Score can also be used for imbalanced datasets.\n",
    "\n",
    "Remember to refer to these guidelines when setting up your configuration for different types of problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_activation_and_units(num_classes: int) -> tuple:\n",
    "    \"\"\"Determine the activation function and units based on number of classes and problem type from config.\"\"\"\n",
    "    problem_type = config.get('Experiment').get('PROBLEM_TYPE')\n",
    "    if problem_type == 'Multi-Label':\n",
    "        return \"sigmoid\", num_classes # Sigmoid converts each score of the final node between 0 to 1 independent of what the other scores are\n",
    "    elif problem_type == 'Binary' or num_classes == 2:\n",
    "        return \"sigmoid\", 1 # Sigmoid converts each score of the final node between 0 to 1 independent of what the other scores are\n",
    "    elif problem_type == 'Multi-Class':\n",
    "        return \"softmax\", num_classes # Softmax converts each score of the final node between 0 to 1, but also makes sure all the scores add up to 1\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid problem_type: {problem_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning models\n",
    "def create_transfer_model(base_model, input_shape: tuple, num_classes: int, hidden_units: list, dropout_rate: float, regularizer_rate: float) -> keras.Model:\n",
    "    \"\"\"Creates a transfer learning model based on a given base model.\"\"\"\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D()\n",
    "    ])\n",
    "\n",
    "    for units in hidden_units:\n",
    "        model.add(layers.Dense(units, kernel_regularizer=keras.regularizers.l2(regularizer_rate), bias_regularizer=keras.regularizers.l2(regularizer_rate)))\n",
    "        model.add(layers.LeakyReLU())\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "        \n",
    "    activation, units = determine_activation_and_units(num_classes)\n",
    "    model.add(layers.Dense(units, activation=activation))\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_mobilenetv2_transfer_model(input_shape: tuple, num_classes: int) -> keras.Model:\n",
    "    \"\"\"Creates a MobileNetV2 based transfer learning model.\"\"\"\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    return create_transfer_model(base_model, input_shape, num_classes, [128, 64], 0.5, 0.001)\n",
    "\n",
    "def create_inceptionv3_transfer_model(input_shape: tuple, num_classes: int) -> keras.Model:\n",
    "    \"\"\"Creates an InceptionV3 based transfer learning model.\"\"\"\n",
    "    base_model = tf.keras.applications.InceptionV3(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    return create_transfer_model(base_model, input_shape, num_classes, [128, 64], 0.5, 0.001)\n",
    "\n",
    "def create_resnet50_transfer_model(input_shape: tuple, num_classes: int) -> keras.Model:\n",
    "    \"\"\"Creates a ResNet50 based transfer learning model.\"\"\"\n",
    "    base_model = tf.keras.applications.ResNet50(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    return create_transfer_model(base_model, input_shape, num_classes, [256, 128], 0.5, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create a small version of the Xception network\n",
    "def create_small_xception_model(input_shape, num_classes):\n",
    "    # Input layer\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Entry block: Initial Convolution and BatchNormalization\n",
    "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    previous_block_activation = x  # Set aside residual for later use\n",
    "\n",
    "    # Middle flow: Stacking Separable Convolution blocks\n",
    "    for size in [256, 512, 728]:\n",
    "        # ReLU activation\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        # Separable Convolution\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        # ReLU activation\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        # Separable Convolution\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        # Max Pooling\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual from previous block and add it to the current block\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(previous_block_activation)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Exit flow: Final Separable Convolution, BatchNormalization, and Global Average Pooling\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    activation, units = determine_activation_and_units(num_classes)\n",
    "\n",
    "    # Dropout and Dense output layer\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create a basic CNN model\n",
    "def create_basic_cnn_model(input_shape, num_classes):\n",
    "    conv2d_filter_size = (3, 3)\n",
    "    conv2d_activation = 'relu'\n",
    "    dense_activation = 'relu'\n",
    "    num_conv_blocks = 3\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Explicitly define the input shape\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "\n",
    "    for _ in range(num_conv_blocks):\n",
    "        model.add(tf.keras.layers.Conv2D(32 * (2**_), conv2d_filter_size, activation=conv2d_activation, padding='same'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "    model.add(tf.keras.layers.Dense(128, activation=dense_activation))\n",
    "\n",
    "    activation, units = determine_activation_and_units(num_classes)\n",
    "    model.add(layers.Dense(units, activation=activation))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection function to select which model to use\n",
    "def select_model(model_name: str, input_shape: tuple, num_classes: int) -> keras.Model:\n",
    "    \"\"\"Selects a model to use based on the given model name.\"\"\"\n",
    "    model_map = {\n",
    "        \"mobilenetv2\": create_mobilenetv2_transfer_model,\n",
    "        \"inceptionv3\": create_inceptionv3_transfer_model,\n",
    "        \"resnet50\": create_resnet50_transfer_model,\n",
    "        \"small_xception\": create_small_xception_model,\n",
    "        \"basic_cnn\": create_basic_cnn_model\n",
    "    }\n",
    "    if model_name not in model_map:\n",
    "        raise ValueError(\"Invalid model name\")\n",
    "\n",
    "    return model_map[model_name](input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Preparation of CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "def read_csv(config: Dict):\n",
    "    # Functionality to read the data\n",
    "    data_file_path = os.path.join(config['Paths']['NEW_BASE_PATH'], config['Paths']['DATA_FILE'])\n",
    "    if not os.path.exists(data_file_path):\n",
    "        raise FileNotFoundError(f\"Error: File does not exist - {data_file_path}\")\n",
    "    try:\n",
    "        data = pd.read_csv(data_file_path, usecols=config['CSV']['COLUMNS_TO_READ'])\n",
    "        print(\"---> Data read successfully.\")\n",
    "        sample_frac = config.get('SAMPLE_FRAC', 1.0)\n",
    "        if 0 < sample_frac < 1.0:\n",
    "            data = data.sample(frac=sample_frac).reset_index(drop=True)\n",
    "            print(f\"---> Data sampled: Using {sample_frac * 100}% of the available data.\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error: Could not read data - {e}\") from e\n",
    "    return data\n",
    "\n",
    "def clean_csv(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    invalid_rows = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        image_path = row['ImageFile']\n",
    "        \n",
    "        # Check if image_path is not string\n",
    "        if not isinstance(image_path, str):\n",
    "            print(f\"Removing row: {row} (Reason: Invalid ImageFile value - not a string)\")\n",
    "            invalid_rows.append(index)\n",
    "            continue\n",
    "        \n",
    "        # Check if the image path exists\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Removing row: {row} (Reason: File does not exist)\")\n",
    "            invalid_rows.append(index)\n",
    "            continue\n",
    "        \n",
    "        # Check if image can be read\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Removing row: {row} (Reason: Image can't be read)\")\n",
    "            invalid_rows.append(index)\n",
    "    \n",
    "    # Drop invalid rows\n",
    "    df.drop(index=invalid_rows, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def update_image_paths(df):\n",
    "    old_base_path = config['Paths']['OLD_BASE_PATH']\n",
    "    new_base_path = config['Paths']['NEW_BASE_PATH']\n",
    "    df['ImageFile'] = df['ImageFile'].str.replace(old_base_path, new_base_path, regex=False)\n",
    "    print(\"---> Image paths updated.\")\n",
    "    return df\n",
    "\n",
    "def generate_labels(df: pd.DataFrame):\n",
    "    \"\"\"Generate labels based on the configuration.\"\"\"\n",
    "    print(\"---> Generating labels for Focus, StigX, and StigY...\")\n",
    "    # Extract configurations\n",
    "    labels_config = config.get('Labels', {}).get('MAPPINGS', {})\n",
    "    thresholds_config = config.get('Thresholds', {})\n",
    "    # Offset columns mapping\n",
    "    offset_column_mapping = {\n",
    "        'Focus_Label': 'Focus_Offset (V)',\n",
    "        'StigX_Label': 'Stig_Offset_X (V)',\n",
    "        'StigY_Label': 'Stig_Offset_Y (V)'\n",
    "    }\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    label_encoders = {}  # To store label encoders\n",
    "    mlb_classes = None  # To store classes of MultiLabelBinarizer\n",
    "    \n",
    "    for label_key, choices_dict in labels_config.items():\n",
    "        offset_column = offset_column_mapping.get(label_key)\n",
    "        if not offset_column:\n",
    "            print(f\"Warning: No offset column mapping found for '{label_key}'. Skipping label generation.\")\n",
    "            continue\n",
    "        if offset_column not in df.columns:\n",
    "            print(f\"Warning: Column '{offset_column}' not found in DataFrame. Skipping label generation for '{label_key}'.\")\n",
    "            continue\n",
    "        \n",
    "        low_threshold = thresholds_config.get(f\"{label_key.split('_')[0].upper()}_LOW\", 0)\n",
    "        high_threshold = thresholds_config.get(f\"{label_key.split('_')[0].upper()}_HIGH\", 0)\n",
    "        conditions = [\n",
    "            (df_copy[offset_column].abs() <= low_threshold),\n",
    "            (df_copy[offset_column].abs() > low_threshold) & (df_copy[offset_column].abs() <= high_threshold),\n",
    "            (df_copy[offset_column].abs() > high_threshold)\n",
    "        ]\n",
    "        choices = list(choices_dict.keys())\n",
    "        df_copy[label_key] = np.select(conditions, choices, default='Unknown')\n",
    "        le = LabelEncoder()\n",
    "        df_copy[label_key] = le.fit_transform(df_copy[label_key])\n",
    "        label_encoders[label_key] = le\n",
    "        print(\"---> Labels generated for\", label_key)\n",
    "        \n",
    "    # For multi-label problems\n",
    "    if config.get('Experiment', {}).get('PROBLEM_TYPE') == 'Multi-Label':\n",
    "        label_keys = list(labels_config.keys())\n",
    "        df_copy['Multi_Labels'] = df_copy.apply(lambda row: [row[key] for key in label_keys], axis=1)\n",
    "        print(\"---> Multi-labels generated.\")\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        df_copy['Multi_Labels_Binarized'] = list(mlb.fit_transform(df_copy['Multi_Labels']))\n",
    "        mlb_classes = mlb.classes_  # Store the classes attribute for later use\n",
    "        \n",
    "    return df_copy, label_encoders, mlb_classes\n",
    "\n",
    "def shuffle_and_reset_index(data):\n",
    "    print(\"---> Shuffling and resetting index...\")\n",
    "    shuffled_df = data.sample(frac=1, random_state=config['Experiment']['RANDOM_SEED']).reset_index(drop=True)\n",
    "    print(\"---> Data shuffled and index reset.\")\n",
    "    return shuffled_df\n",
    "\n",
    "def prepare_datasets(df: pd.DataFrame):\n",
    "    \"\"\"Prepare training, validation, and test datasets.\"\"\"\n",
    "    # Check if DataFrame is empty\n",
    "    if df is None or df.empty:\n",
    "        print(\"Warning: DataFrame is empty. Cannot proceed with data preparation.\")\n",
    "        return {'train': None, 'valid': None, 'test': None}\n",
    "    # Split Data\n",
    "    try:\n",
    "        train_df, temp_df = train_test_split(df, test_size=1 - config['Model']['TRAIN_SIZE'], random_state=config['Experiment']['RANDOM_SEED'])\n",
    "        val_df, test_df = train_test_split(temp_df, test_size=1 - config['Model']['VAL_SIZE'], random_state=config['Experiment']['RANDOM_SEED'])\n",
    "    except ValueError:\n",
    "        print(\"Not enough data to split into training, validation, and test sets.\")\n",
    "        return {'train': None, 'valid': None, 'test': None}\n",
    "    print(\"---> Data split into training, validation, and test sets.\")\n",
    "    return {'train': train_df, 'valid': val_df, 'test': test_df}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights\n",
    "\n",
    "def compute_and_store_class_weights(datasets: Dict[str, pd.DataFrame], \n",
    "                                    label_encoders: Dict[str, LabelEncoder], \n",
    "                                    mlb_classes: np.ndarray = None) -> pd.DataFrame:\n",
    "    problem_type = config.get('Experiment', {}).get('PROBLEM_TYPE', 'Binary')\n",
    "    \n",
    "    all_records = []  # To store records before converting them to a DataFrame\n",
    "    \n",
    "    if problem_type == 'Multi-Label':\n",
    "        mlb = MultiLabelBinarizer(classes=mlb_classes)  # Initialize with known classes if available\n",
    "        for split, df in datasets.items():\n",
    "            if df is None:\n",
    "                continue\n",
    "            \n",
    "            label_column = np.array(df['Multi_Labels'].tolist())\n",
    "            binarized_labels = mlb.transform(label_column)  # Use transform instead of fit_transform to ensure consistent classes\n",
    "            \n",
    "            for label_idx, label_name in enumerate(mlb.classes_):\n",
    "                label_data = binarized_labels[:, label_idx]\n",
    "                unique_labels = np.unique(label_data)\n",
    "                \n",
    "                class_weights = compute_class_weight('balanced', classes=unique_labels, y=label_data)\n",
    "                class_weights_dict = dict(zip(unique_labels, class_weights))\n",
    "                \n",
    "                for cls, weight in class_weights_dict.items():\n",
    "                    cnt = Counter(label_data)[cls]\n",
    "                    all_records.append({'split': split, 'label': label_name, 'class': cls, 'Count': cnt, 'Weight': weight})\n",
    "    \n",
    "    else:  # Multi-Class or Binary\n",
    "        for split, df in datasets.items():\n",
    "            if df is None:\n",
    "                continue\n",
    "            for label in config['Labels']['MAPPINGS']:\n",
    "                unique_labels = df[label].unique()\n",
    "                class_weights = compute_class_weight('balanced', classes=unique_labels, y=df[label])\n",
    "                \n",
    "                class_weights_dict = dict(zip(unique_labels, class_weights))\n",
    "                \n",
    "                for cls, weight in class_weights_dict.items():\n",
    "                    cnt = Counter(df[label])[cls]\n",
    "                    \n",
    "                    # Reverse map to original class using label_encoders\n",
    "                    original_class = label_encoders[label].inverse_transform([cls])[0]\n",
    "                    \n",
    "                    all_records.append({'split': split, 'label': label, 'class': original_class, 'Count': cnt, 'Weight': weight})\n",
    "                    \n",
    "    df_class_weights = pd.DataFrame.from_records(all_records)\n",
    "    df_class_weights.set_index(['split', 'label', 'class'], inplace=True)\n",
    "    \n",
    "    return df_class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create tf datasets\n",
    "\n",
    "def create_tf_datasets(datasets: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
    "    # batch_size = config.get('BATCH_SIZE', 32)\n",
    "    tf_datasets = {}  # Initialize a dictionary to hold the output datasets\n",
    "    if config['Experiment'].get('PROBLEM_TYPE') == 'Multi-Label':\n",
    "        print(\"[INFO] Problem type detected as Multi-Label.\")\n",
    "        for split, df in datasets.items():\n",
    "            if df is not None:\n",
    "                # Check and remove rows where 'ImageFile' is nan\n",
    "                if df['ImageFile'].isna().any():\n",
    "                    print(\"[WARNING] Removing rows with nan in 'ImageFile' column for split\", split)\n",
    "                    df.dropna(subset=['ImageFile'], inplace=True)\n",
    "                ds = tf.data.Dataset.from_tensor_slices((df['ImageFile'].values, df['Multi_Labels_Binarized'].tolist()))\n",
    "                ds = preprocess_single_dataset(ds, is_training=(split == 'train'))\n",
    "                # ds = ds.batch(batch_size)\n",
    "                tf_datasets[split] = ds\n",
    "    else:\n",
    "        print(\"[INFO] Problem type detected as Multi-Class/Binary.\")\n",
    "        for label in ['Focus_Label', 'StigX_Label', 'StigY_Label']:\n",
    "            label_datasets = {}\n",
    "            for split, df in datasets.items():\n",
    "                if df is not None:\n",
    "                    # Check and remove rows where 'ImageFile' is nan\n",
    "                    if df['ImageFile'].isna().any():\n",
    "                        print(\"[WARNING] Removing rows with nan in 'ImageFile' column for label\", label, \"and split\", split)\n",
    "                        df.dropna(subset=['ImageFile'], inplace=True)\n",
    "                    ds = tf.data.Dataset.from_tensor_slices((df['ImageFile'].values, df[label].values))\n",
    "                    ds = preprocess_single_dataset(ds, is_training=(split == 'train'))\n",
    "                    # ds = ds.batch(batch_size)\n",
    "                    label_datasets[split] = ds\n",
    "            tf_datasets[label] = label_datasets\n",
    "    return apply_preprocessing(tf_datasets)\n",
    "\n",
    "def determine_label_shape() -> int:\n",
    "    problem_type = config['Experiment'].get('PROBLEM_TYPE', None)\n",
    "    mappings = config['Labels'].get('MAPPINGS', None)\n",
    "    if problem_type == 'Multi-Label':\n",
    "        return sum(len(v) for v in mappings.values())\n",
    "    elif problem_type in ['Multi-Class', 'Binary']:\n",
    "        return len(mappings.get(next(iter(mappings))))\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid PROBLEM_TYPE: {problem_type}\")\n",
    "\n",
    "def preprocess_wrapper(file_path, label, augment: bool) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    image, label = tf.py_function(\n",
    "        func=lambda file_path, label, augment: preprocess_image(file_path, label, augment),\n",
    "        inp=[file_path, label, augment], \n",
    "        Tout=[tf.float32, tf.int32]\n",
    "    )\n",
    "    # Set shapes\n",
    "    image.set_shape([config['Model']['IMG_SIZE'], config['Model']['IMG_SIZE'], 3])\n",
    "    label_shape = determine_label_shape()\n",
    "    label.set_shape([label_shape])\n",
    "    return image, label\n",
    "\n",
    "def preprocess_single_dataset(ds, is_training: bool = False):\n",
    "    ds = ds.map(lambda file_path, label: preprocess_wrapper(file_path, label, is_training))\n",
    "    return ds\n",
    "\n",
    "def apply_preprocessing(datasets: Dict) -> Dict:\n",
    "    if config['Experiment'].get('PROBLEM_TYPE') in ['Multi-Class', 'Binary']:\n",
    "        for label, splits in datasets.items():\n",
    "            for split in ['train', 'valid', 'test']:\n",
    "                if split in splits:\n",
    "                    is_training = split == 'train'\n",
    "                    splits[split] = preprocess_single_dataset(splits[split], is_training)\n",
    "\n",
    "                    # Configure for performance\n",
    "                    AUTOTUNE = tf.data.AUTOTUNE\n",
    "                    splits[split] = splits[split].cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    else:\n",
    "        for split in ['train', 'valid', 'test']:\n",
    "            if split in datasets:\n",
    "                is_training = split == 'train'\n",
    "                datasets[split] = preprocess_single_dataset(datasets[split], is_training)\n",
    "\n",
    "                # Configure for performance\n",
    "                AUTOTUNE = tf.data.AUTOTUNE\n",
    "                datasets[split] = datasets[split].cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    print(\"---> TF Datasets created.\")\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Image Preprocessing functions\n",
    "\n",
    "def create_preprocessing_layers(img_width: int, img_height: int, rescale_factor: float) -> keras.Sequential:\n",
    "    \"\"\"Create preprocessing layers for resizing and rescaling images.\"\"\"\n",
    "    return keras.Sequential([\n",
    "        layers.Resizing(img_width, img_height),\n",
    "        layers.Rescaling(rescale_factor)\n",
    "    ])\n",
    "\n",
    "\n",
    "def create_augmentation_layers(augmentation_config: dict) -> keras.Sequential:\n",
    "    \"\"\"Create data augmentation layers.\"\"\"\n",
    "    try:\n",
    "        augmentation_layers = tf.keras.Sequential([\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomFlip(\"vertical\"),\n",
    "            layers.RandomRotation(augmentation_config['rotation_factor']),\n",
    "            layers.RandomTranslation(\n",
    "                height_factor=augmentation_config['height_factor'],\n",
    "                width_factor=augmentation_config['width_factor'],\n",
    "                fill_mode=\"reflect\"\n",
    "            ),\n",
    "            layers.RandomContrast(augmentation_config['contrast_factor']),\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating augmentation layers: {e}\")\n",
    "    return augmentation_layers\n",
    "\n",
    "\n",
    "def read_and_convert_image(file_path: str) -> tf.Tensor:\n",
    "    \"\"\"Read an image from a file and convert it to a 3-channel tensor.\"\"\"\n",
    "    image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        print(\"Failed to read the image.\")\n",
    "        return None\n",
    "    image = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "    image = tf.expand_dims(image, axis=-1)\n",
    "    return tf.image.grayscale_to_rgb(image)\n",
    "\n",
    "\n",
    "def preprocess_image(file_path_or_tensor, label, augment) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    debug_info = {}\n",
    "    try:\n",
    "        debug_info['Initial type'] = str(type(file_path_or_tensor))\n",
    "        debug_info['Initial content'] = str(file_path_or_tensor)\n",
    "        if isinstance(file_path_or_tensor, tf.Tensor):\n",
    "            if len(file_path_or_tensor.shape) == 4:  # Assuming shape is (batch, height, width, channels)\n",
    "                image = file_path_or_tensor\n",
    "            else:\n",
    "                file_path_or_tensor = file_path_or_tensor.numpy()\n",
    "                if isinstance(file_path_or_tensor, (bytes, np.bytes_)):\n",
    "                    file_path_or_tensor = file_path_or_tensor.decode('utf-8')\n",
    "                elif isinstance(file_path_or_tensor, np.ndarray):\n",
    "                    debug_info['Unexpected NumPy array'] = f\"Shape: {file_path_or_tensor.shape}, Content: {file_path_or_tensor}\"\n",
    "                    raise ValueError(\"Unexpected NumPy array for file_path\")\n",
    "                image = read_and_convert_image(file_path_or_tensor)\n",
    "        else:\n",
    "            image = read_and_convert_image(file_path_or_tensor)\n",
    "        debug_info['Final type'] = str(type(file_path_or_tensor))\n",
    "        debug_info['Final content'] = str(file_path_or_tensor)\n",
    "        preprocess_seq = create_preprocessing_layers(\n",
    "            img_width=config['Model']['IMG_SIZE'],\n",
    "            img_height=config['Model']['IMG_SIZE'],\n",
    "            rescale_factor=1./255\n",
    "        )\n",
    "        augment_seq = create_augmentation_layers(config['Augmentation'])\n",
    "        image = preprocess_seq(image)\n",
    "        if augment:\n",
    "            image = augment_seq(image)\n",
    "            image = tf.clip_by_value(image, 0, 1)\n",
    "        \n",
    "        return image, label\n",
    "    except Exception as e:\n",
    "        debug_info['Error'] = str(e)\n",
    "        print(\"An error occurred during preprocessing:\")\n",
    "        for key, value in debug_info.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to integrate all steps\n",
    "\n",
    "\n",
    "# Configure for dataset creation\n",
    "csv_config = {\n",
    "    'CSV': {\n",
    "        'COLUMNS_TO_READ': ['ImageFile', 'Focus_Offset (V)', 'Stig_Offset_X (V)', 'Stig_Offset_Y (V)']\n",
    "    },\n",
    "    'Thresholds': {\n",
    "        'FOCUS_LOW': 30,  # Lower focus threshold\n",
    "        'FOCUS_HIGH': 60,  # Upper focus threshold\n",
    "        'STIGX_LOW': 1,  # Lower astigmatism threshold\n",
    "        'STIGX_HIGH': 2,  # Upper astigmatism threshold\n",
    "        'STIGY_LOW': 1,  # Lower astigmatism threshold\n",
    "        'STIGY_HIGH': 2,  # Upper astigmatism threshold\n",
    "    },\n",
    "    'Paths': {  # Data and model paths\n",
    "        'DATA_FILE': \"combined_output.csv\",\n",
    "        'OLD_BASE_PATH': \"D:\\\\DOE\\\\\",\n",
    "        # 'NEW_BASE_PATH': \"Y:\\\\User\\\\Aaron-HX38\\\\DOE\\\\\",\n",
    "        # 'NEW_BASE_PATH': \"C:\\\\Users\\\\aaron.woods\\\\OneDrive - Thermo Fisher Scientific\\\\Documents\\\\GitHub\\\\Image-Classification\\\\\",\n",
    "        'NEW_BASE_PATH': \"C:\\\\Users\\\\aaron.woods\\\\OneDrive - Thermo Fisher Scientific\\\\Desktop\\\\Dec 24\\\\\",\n",
    "    },\n",
    "    'SAMPLE_FRAC': 1.0,  # Fraction of the data to use for quicker prototyping. 1.0 means use all data.\n",
    "}\n",
    "config.update(csv_config)\n",
    "config['Experiment']['PROBLEM_TYPE'] = 'Multi-Class'\n",
    "# config['Experiment']['PROBLEM_TYPE'] = 'Multi-Label'\n",
    "\n",
    "\n",
    "\n",
    "# Main function to integrate all steps\n",
    "def main_pipeline(config: Dict):\n",
    "    print(\"===== Preprocessing CSV Data =====\")\n",
    "    # data = read_csv(config, clean=True)\n",
    "    data = read_csv(config)\n",
    "    data = update_image_paths(data)\n",
    "    data = clean_csv(data)\n",
    "    data, label_encoders, mlb_classess  = generate_labels(data)\n",
    "    data = shuffle_and_reset_index(data)\n",
    "    print(\"===== Preparing TensorFlow Datasets =====\")\n",
    "    datasets = prepare_datasets(data)\n",
    "    info = compute_and_store_class_weights(datasets, label_encoders)\n",
    "    datasets = create_tf_datasets(datasets)\n",
    "    print(\"===== Preprocessing Complete =====\")\n",
    "    return datasets, info\n",
    "\n",
    "datasets, info = main_pipeline(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the class weights or datasets\n",
    "\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(datasets)\n",
    "# pp.pprint(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Class Distribution\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_single_split(ax, df, split, problem_type):\n",
    "    filtered_df = df.loc[split]\n",
    "    \n",
    "    if problem_type == 'Multi-Class':\n",
    "        labels = filtered_df.index.get_level_values('label').unique()\n",
    "        all_classes = []\n",
    "        for label in labels:\n",
    "            sub_df = filtered_df.loc[label]\n",
    "            bars = ax.bar(sub_df.index, sub_df['Count'], label=f\"{label}\")\n",
    "\n",
    "            for bar, (_, row) in zip(bars, sub_df.iterrows()):\n",
    "                x = bar.get_x() + bar.get_width() / 2.0\n",
    "                y = bar.get_height()\n",
    "                ax.annotate(f\"C: {int(row['Count'])}\\nW: {row['Weight']:.2f}\",\n",
    "                            (x, y), \n",
    "                            ha='center', \n",
    "                            va='bottom', \n",
    "                            fontsize=8)\n",
    "                \n",
    "            all_classes.extend(sub_df.index)\n",
    "        \n",
    "        ax.legend()\n",
    "        ax.set_xticklabels(all_classes, rotation=90, fontsize=8)\n",
    "        ax.set_title(f\"{split.capitalize()} Data\")\n",
    "        \n",
    "    elif problem_type == 'Multi-Label':\n",
    "        x_ticks = [str(cls) for cls in filtered_df.index]\n",
    "        bars = ax.bar(x_ticks, filtered_df['Count'])\n",
    "        \n",
    "        for bar, (_, row) in zip(bars, filtered_df.iterrows()):\n",
    "            x = bar.get_x() + bar.get_width() / 2.0\n",
    "            y = bar.get_height()\n",
    "            ax.annotate(f\"C: {int(row['Count'])}\\nW: {row['Weight']:.2f}\", \n",
    "                        (x, y), \n",
    "                        ha='center', \n",
    "                        va='bottom', \n",
    "                        fontsize=8)\n",
    "        \n",
    "        ax.set_xticklabels(x_ticks, rotation=90, fontsize=8)\n",
    "        ax.set_title(f\"{split.capitalize()} Data\")\n",
    "\n",
    "\n",
    "def plot_dataset_info(df):\n",
    "    global config\n",
    "    \n",
    "    problem_type = config['Experiment']['PROBLEM_TYPE']\n",
    "    splits = ['train', 'valid', 'test']\n",
    "    \n",
    "    fig, axs = plt.subplots(1, len(splits), figsize=(20, 8))\n",
    "    \n",
    "    for i, split in enumerate(splits):\n",
    "        plot_single_split(axs[i], df, split, problem_type)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Assume 'info' is your DataFrame with 'split', 'label', 'class', 'Count', and 'Weight'\n",
    "# Call the function using your DataFrame\n",
    "plot_dataset_info(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Batches of Images\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "def show_batch(dataset, num_images=9):\n",
    "    \"\"\"Display a batch of images and their corresponding labels.\"\"\"\n",
    "    \n",
    "    # Extract a batch of `num_images` samples from the training dataset\n",
    "    for images, labels in dataset.take(1):  # Only take a single batch\n",
    "        images = images.numpy()\n",
    "        labels = labels.numpy()\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(1, num_images, figsize=(15, 15),\n",
    "                                 subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                                 gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "        \n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            ax.imshow(images[i].astype(\"uint8\"))\n",
    "            ax.set_title(f\"Focus: {labels[i][0]}, StigX: {labels[i][1]}, StigY: {labels[i][2]}\")\n",
    "\n",
    "# Assuming 'train' is your training dataset for one of the labels\n",
    "train_dataset = datasets['Focus_Label']['train']\n",
    "\n",
    "# Show a batch of 9 training images\n",
    "show_batch(train_dataset, num_images=9)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
