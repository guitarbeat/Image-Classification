{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Focus and Astigmatism Classifier\n",
    "**Author:** [Aaron Woods](https://aaronwoods.info)  \n",
    "**Date Created:** September 12, 2023  \n",
    "**Description:** This script provides an end-to-end machine learning pipeline to classify images as either \"In Focus\" or \"Out of Focus\", and additionally identifies astigmatism-related issues.  \n",
    "**Repository:** [Image Classification on VSCode](https://insiders.vscode.dev/tunnel/midnightsim/c:/Users/User/Desktop/Image-Classification)\n",
    "\n",
    "### Overview\n",
    "The script features a comprehensive pipeline that ingests data from Excel spreadsheets and feeds it into various machine learning models. The design is modular, allowing for easy adaptability to address different image classification problems, including focus quality and astigmatism detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# TensorFlow Installation with GPU Support\n",
    "# ------------------------------\n",
    "# Note: TensorFlow versions above 2.10 are not supported on GPUs on native Windows installations.\n",
    "# For more details, visit: https://www.tensorflow.org/install/pip#windows-wsl2_1\n",
    "# Uncomment the following line to install TensorFlow if needed.\n",
    "# %pip install \"tensorflow<2.11\"\n",
    "\n",
    "# ------------------------------\n",
    "# System and TensorFlow Info Check\n",
    "# ------------------------------\n",
    "# Import necessary libraries and initialize an empty dictionary to store system information.\n",
    "import platform\n",
    "system_info = {\"Platform\": platform.platform(), \"Python Version\": platform.python_version()}\n",
    "\n",
    "# Try importing TensorFlow and collecting relevant system information.\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    system_info.update({\n",
    "        \"TensorFlow Version\": tf.__version__,\n",
    "        \"Num GPUs Available\": len(tf.config.list_physical_devices('GPU'))\n",
    "    })\n",
    "    system_info['Instructions'] = (\n",
    "        \"You're all set to run your model on a GPU.\" \n",
    "        if system_info['Num GPUs Available'] \n",
    "        else (\n",
    "            \"No GPUs found. To use a GPU, follow these steps:\\n\"\n",
    "            \"  1. Install NVIDIA drivers for your GPU.\\n\"\n",
    "            \"  2. Install a compatible CUDA toolkit.\\n\"\n",
    "            \"  3. Install the cuDNN library.\\n\"\n",
    "            \"  4. Make sure to install the GPU version of TensorFlow.\"\n",
    "        )\n",
    "    )\n",
    "except ModuleNotFoundError:\n",
    "    system_info['Instructions'] = (\n",
    "        \"TensorFlow is not installed. \"\n",
    "        \"Install it using pip by running: !pip install tensorflow\"\n",
    "    )\n",
    "\n",
    "# Format and display the gathered system information.\n",
    "formatted_info = \"\\n\".join(f\"{key}: {value}\" for key, value in system_info.items())\n",
    "print(formatted_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Package Installation (Optional)\n",
    "# ------------------------------\n",
    "# Uncomment the following lines to install required packages if running on a new machine.\n",
    "# To suppress the output, we use '> /dev/null 2>&1'.\n",
    "# %pip install numpy pandas matplotlib protobuf seaborn scikit-learn tensorflow > /dev/null 2>&1\n",
    "\n",
    "# ------------------------------\n",
    "# Import Libraries\n",
    "# ------------------------------\n",
    "\n",
    "# Standard Libraries\n",
    "import os, sys, random, math, glob, logging\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Third-Party Libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from IPython.display import clear_output\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from collections import Counter\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.callbacks import TensorBoard, Callback\n",
    "from tensorflow.keras.applications import InceptionV3, ResNet50\n",
    "from keras.models import load_model\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Type Annotations\n",
    "from typing import List, Dict, Tuple, Union, Any, Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations for Loss Functions and Other Settings Per Problem Type\n",
    "\n",
    "#### Multi-Label Problems:\n",
    "- **Loss Function**: Typically, \"binary_crossentropy\" is used because each class label is independent and the task is to predict whether it is present or not.\n",
    "- **Label Encoding**: One-hot encoding is commonly used where each label is considered as a separate class.\n",
    "- **Activation Function**: The sigmoid activation function is generally used in the output layer to allow for multiple independent classes.\n",
    "- **Evaluation Metrics**: Precision, Recall, and F1 Score can be effective for evaluating multi-label problems.\n",
    "\n",
    "#### Binary Classification Problems:\n",
    "- **Loss Function**: \"binary_crossentropy\" is the standard loss function because the task is to categorize instances into one of the two classes.\n",
    "- **Label Encoding**: Labels are often encoded as 0 or 1.\n",
    "- **Activation Function**: The sigmoid activation function is usually used in the output layer, producing a probability score that can be thresholded to yield a class label.\n",
    "- **Evaluation Metrics**: Accuracy, Precision, Recall, and AUC-ROC are commonly used metrics.\n",
    "\n",
    "#### Multi-Class Problems:\n",
    "- **Loss Function**: \"categorical_crossentropy\" or \"sparse_categorical_crossentropy\" is commonly used. The former requires one-hot encoded labels, while the latter requires integer labels.\n",
    "- **Label Encoding**: One-hot encoding is often used to convert the categorical labels into a format that can be provided to the neural network.\n",
    "- **Activation Function**: The softmax activation function is used in the output layer to produce a probability distribution over the multiple classes.\n",
    "- **Evaluation Metrics**: Accuracy is the most straightforward metric. However, Precision, Recall, and F1 Score can also be used for imbalanced datasets.\n",
    "\n",
    "Remember to refer to these guidelines when setting up your configuration for different types of problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration dictionary\n",
    "config = {\n",
    "    'Experiment': {\n",
    "        'NAME': \"Multi-Label_Thresholds-30-60-1-2\",  # Experiment name\n",
    "        'RANDOM_SEED': 42,  # Seed for reproducibility\n",
    "        'PROBLEM_TYPE': 'Multi-Label',  # Problem type: Binary, Multi-Class, Multi-Label\n",
    "    },\n",
    "    'Model': {\n",
    "        'IMG_SIZE': 224,  # Image input size\n",
    "        'BATCH_SIZE': 32,  # Batch size for training\n",
    "        'EPOCHS': 100,  # Number of epochs\n",
    "        'LEARNING_RATE': 1e-3,  # Learning rate\n",
    "        'EARLY_STOPPING_PATIENCE': 5,  # Early stopping patience parameter\n",
    "        'REDUCE_LR_PATIENCE': 3,  # Learning rate reduction patience parameter\n",
    "        'MIN_LR': 1e-6,  # Minimum learning rate\n",
    "        'LOSS': \"binary_crossentropy\",  # Loss function: \"categorical_crossentropy\" for multi-class\n",
    "        'TRAIN_SIZE': 0.8,  # Fraction of data to use for training\n",
    "        'VAL_SIZE': 0.5,  # Fraction of data to use for validation\n",
    "    },\n",
    "    'Labels': {\n",
    "        'MAPPINGS': {  # Class label mappings\n",
    "            'Focus_Label': {'SharpFocus': 0, 'SlightlyBlurred': 1, 'HighlyBlurred': 2},\n",
    "            'StigX_Label': {'OptimalStig_X': 0, 'ModerateStig_X': 1, 'SevereStig_X': 2},\n",
    "            'StigY_Label': {'OptimalStig_Y': 0, 'ModerateStig_Y': 1, 'SevereStig_Y': 2},\n",
    "        }\n",
    "    },\n",
    "    'Augmentation': {  # Data augmentation parameters\n",
    "        'rotation_factor': 0.002,\n",
    "        'height_factor': (-0.18, 0.18),\n",
    "        'width_factor': (-0.18, 0.18),\n",
    "        'contrast_factor': 0.5,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(config['Experiment']['RANDOM_SEED'])\n",
    "tf.random.set_seed(config['Experiment']['RANDOM_SEED'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_activation_and_units(num_classes: int) -> tuple:\n",
    "    \"\"\"Determine the activation function and units based on number of classes and problem type from config.\"\"\"\n",
    "    problem_type = config.get('Experiment').get('PROBLEM_TYPE')\n",
    "    if problem_type == 'Multi-Label':\n",
    "        return \"sigmoid\", num_classes # Sigmoid converts each score of the final node between 0 to 1 independent of what the other scores are\n",
    "    elif problem_type == 'Binary' or num_classes == 2:\n",
    "        return \"sigmoid\", 1 # Sigmoid converts each score of the final node between 0 to 1 independent of what the other scores are\n",
    "    elif problem_type == 'Multi-Class':\n",
    "        return \"softmax\", num_classes # Softmax converts each score of the final node between 0 to 1, but also makes sure all the scores add up to 1\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid problem_type: {problem_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning models\n",
    "def create_transfer_model(base_model, input_shape: tuple, num_classes: int, hidden_units: list, dropout_rate: float, regularizer_rate: float) -> keras.Model:\n",
    "    \"\"\"Creates a transfer learning model based on a given base model.\"\"\"\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D()\n",
    "    ])\n",
    "\n",
    "    for units in hidden_units:\n",
    "        model.add(layers.Dense(units, kernel_regularizer=keras.regularizers.l2(regularizer_rate), bias_regularizer=keras.regularizers.l2(regularizer_rate)))\n",
    "        model.add(layers.LeakyReLU())\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "        \n",
    "    activation, units = determine_activation_and_units(num_classes)\n",
    "    model.add(layers.Dense(units, activation=activation))\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_mobilenetv2_transfer_model(input_shape: tuple, num_classes: int) -> keras.Model:\n",
    "    \"\"\"Creates a MobileNetV2 based transfer learning model.\"\"\"\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    return create_transfer_model(base_model, input_shape, num_classes, [128, 64], 0.5, 0.001)\n",
    "\n",
    "def create_inceptionv3_transfer_model(input_shape: tuple, num_classes: int) -> keras.Model:\n",
    "    \"\"\"Creates an InceptionV3 based transfer learning model.\"\"\"\n",
    "    base_model = tf.keras.applications.InceptionV3(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    return create_transfer_model(base_model, input_shape, num_classes, [128, 64], 0.5, 0.001)\n",
    "\n",
    "def create_resnet50_transfer_model(input_shape: tuple, num_classes: int) -> keras.Model:\n",
    "    \"\"\"Creates a ResNet50 based transfer learning model.\"\"\"\n",
    "    base_model = tf.keras.applications.ResNet50(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    return create_transfer_model(base_model, input_shape, num_classes, [256, 128], 0.5, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create a small version of the Xception network\n",
    "def create_small_xception_model(input_shape, num_classes):\n",
    "    # Input layer\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Entry block: Initial Convolution and BatchNormalization\n",
    "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    previous_block_activation = x  # Set aside residual for later use\n",
    "\n",
    "    # Middle flow: Stacking Separable Convolution blocks\n",
    "    for size in [256, 512, 728]:\n",
    "        # ReLU activation\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        # Separable Convolution\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        # ReLU activation\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        # Separable Convolution\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        # Max Pooling\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual from previous block and add it to the current block\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(previous_block_activation)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Exit flow: Final Separable Convolution, BatchNormalization, and Global Average Pooling\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    activation, units = determine_activation_and_units(num_classes)\n",
    "\n",
    "    # Dropout and Dense output layer\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create a basic CNN model\n",
    "def create_basic_cnn_model(input_shape, num_classes):\n",
    "    conv2d_filter_size = (3, 3)\n",
    "    conv2d_activation = 'relu'\n",
    "    dense_activation = 'relu'\n",
    "    num_conv_blocks = 3\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Explicitly define the input shape\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "\n",
    "    for _ in range(num_conv_blocks):\n",
    "        model.add(tf.keras.layers.Conv2D(32 * (2**_), conv2d_filter_size, activation=conv2d_activation, padding='same'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "    model.add(tf.keras.layers.Dense(128, activation=dense_activation))\n",
    "\n",
    "    activation, units = determine_activation_and_units(num_classes)\n",
    "    model.add(layers.Dense(units, activation=activation))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection function to select which model to use\n",
    "def select_model(model_name: str, input_shape: tuple, num_classes: int) -> keras.Model:\n",
    "    \"\"\"Selects a model to use based on the given model name.\"\"\"\n",
    "    model_map = {\n",
    "        \"mobilenetv2\": create_mobilenetv2_transfer_model,\n",
    "        \"inceptionv3\": create_inceptionv3_transfer_model,\n",
    "        \"resnet50\": create_resnet50_transfer_model,\n",
    "        \"small_xception\": create_small_xception_model,\n",
    "        \"basic_cnn\": create_basic_cnn_model\n",
    "    }\n",
    "    if model_name not in model_map:\n",
    "        raise ValueError(\"Invalid model name\")\n",
    "\n",
    "    return model_map[model_name](input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessing_layers(img_width: int, img_height: int, rescale_factor: float) -> keras.Sequential:\n",
    "    \"\"\"Create preprocessing layers for resizing and rescaling images.\"\"\"\n",
    "    return keras.Sequential([\n",
    "        layers.Resizing(img_width, img_height),\n",
    "        layers.Rescaling(rescale_factor)\n",
    "    ])\n",
    "\n",
    "\n",
    "def create_augmentation_layers(augmentation_config: dict) -> keras.Sequential:\n",
    "    \"\"\"Create data augmentation layers.\"\"\"\n",
    "    try:\n",
    "        augmentation_layers = tf.keras.Sequential([\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomFlip(\"vertical\"),\n",
    "            layers.RandomRotation(augmentation_config['rotation_factor']),\n",
    "            layers.RandomTranslation(\n",
    "                height_factor=augmentation_config['height_factor'],\n",
    "                width_factor=augmentation_config['width_factor'],\n",
    "                fill_mode=\"reflect\"\n",
    "            ),\n",
    "            layers.RandomContrast(augmentation_config['contrast_factor']),\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating augmentation layers: {e}\")\n",
    "    return augmentation_layers\n",
    "\n",
    "\n",
    "def read_and_convert_image(file_path: str) -> tf.Tensor:\n",
    "    \"\"\"Read an image from a file and convert it to a 3-channel tensor.\"\"\"\n",
    "    print(\"Reading image from:\", file_path)\n",
    "    image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        print(\"Failed to read the image.\")\n",
    "        return None\n",
    "    image = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "    image = tf.expand_dims(image, axis=-1)\n",
    "    return tf.image.grayscale_to_rgb(image)\n",
    "\n",
    "\n",
    "def preprocess_image(file_path, label, augment, config: dict) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"Preprocess an image by applying resizing, rescaling, and optional data augmentation.\"\"\"\n",
    "    file_path = file_path.numpy().decode(\"utf-8\")\n",
    "    preprocess_seq = create_preprocessing_layers(\n",
    "        img_width=config['Model']['IMG_SIZE'], \n",
    "        img_height=config['Model']['IMG_SIZE'], \n",
    "        rescale_factor=1./255\n",
    "    )\n",
    "    augment_seq = create_augmentation_layers(config['Augmentation'])\n",
    "    image = read_and_convert_image(file_path)\n",
    "    if image is None:\n",
    "        print(\"Image reading failed.\")\n",
    "        return None, label\n",
    "    image = preprocess_seq(image)\n",
    "    if augment:\n",
    "        image = augment_seq(image)\n",
    "        image = tf.clip_by_value(image, 0, 1)  # Clip values after augmentation\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "import tensorflow as tf\n",
    "\n",
    "def determine_label_shape(config: Dict) -> int:\n",
    "    \"\"\"Determine the shape of the label based on the problem type and label mappings.\"\"\"\n",
    "    problem_type = config.get('Experiment', {}).get('PROBLEM_TYPE', None)\n",
    "    mappings = config.get('Labels', {}).get('MAPPINGS', None)\n",
    "    \n",
    "    if problem_type == 'Multi-Label':\n",
    "        return sum(len(v) for v in mappings.values())\n",
    "    elif problem_type in ['Multi-Class', 'Binary']:\n",
    "        return len(mappings.get(next(iter(mappings))))  # Assuming all label types have the same length\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid PROBLEM_TYPE: {problem_type}\")\n",
    "\n",
    "def preprocess_wrapper(file_path, label, augment: bool, config: Dict) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"Wrapper function for TensorFlow's map function for various types of classification problems.\"\"\"\n",
    "    image, label = tf.py_function(\n",
    "        func=lambda file_path, label, augment: preprocess_image(file_path, label, augment, config),\n",
    "        inp=[file_path, label, augment], \n",
    "        Tout=[tf.float32, tf.int32]  # Using tf.int32 for compatibility across problem types\n",
    "    )\n",
    "    \n",
    "    # Set shapes\n",
    "    image.set_shape([config['Model']['IMG_SIZE'], config['Model']['IMG_SIZE'], 3])\n",
    "    label_shape = determine_label_shape(config)\n",
    "    label.set_shape([label_shape])\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "def preprocess_images(train_ds, valid_ds, test_ds, config: Dict):\n",
    "    \"\"\"Apply preprocessing to training, validation, and test datasets.\"\"\"\n",
    "    train_ds = train_ds.map(lambda file_path, label: preprocess_wrapper(file_path, label, True, config))\n",
    "    valid_ds = valid_ds.map(lambda file_path, label: preprocess_wrapper(file_path, label, False, config))\n",
    "    test_ds = test_ds.map(lambda file_path, label: preprocess_wrapper(file_path, label, False, config))\n",
    "    \n",
    "    return train_ds, valid_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Preparation of CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved way: Making columns to read configurable\n",
    "csv_config = {\n",
    "    'CSV': {\n",
    "        'COLUMNS_TO_READ': ['ImageFile', 'Focus_Offset (V)', 'Stig_Offset_X (V)', 'Stig_Offset_Y (V)']\n",
    "    },\n",
    "    'Thresholds': {\n",
    "        'FOCUS_LOW': 30,  # Lower focus threshold\n",
    "        'FOCUS_HIGH': 60,  # Upper focus threshold\n",
    "        'STIG_LOW': 1,  # Lower astigmatism threshold\n",
    "        'STIG_HIGH': 2,  # Upper astigmatism threshold\n",
    "    },\n",
    "    'Paths': {  # Data and model paths\n",
    "        'DATA_FILE': \"combined_output.csv\",\n",
    "        'OLD_BASE_PATH': \"D:\\\\DOE\\\\\",\n",
    "        'NEW_BASE_PATH': \"Y:\\\\User\\\\Aaron-HX38\\\\DOE\\\\\",\n",
    "    },\n",
    "}\n",
    "config.update(csv_config)\n",
    "\n",
    "\n",
    "def read_csv(config: Dict):\n",
    "    # Functionality to read the data\n",
    "    data_file_path = os.path.join(config['Paths']['NEW_BASE_PATH'], config['Paths']['DATA_FILE'])\n",
    "    print(f\"---> Reading data from: {data_file_path}\")\n",
    "    if not os.path.exists(data_file_path):\n",
    "        raise FileNotFoundError(f\"Error: File does not exist - {data_file_path}\")\n",
    "    try:\n",
    "        data = pd.read_csv(data_file_path)\n",
    "        print(\"---> Data read successfully.\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error: Could not read data - {e}\") from e\n",
    "    return data\n",
    "\n",
    "def update_image_paths(df):\n",
    "    old_base_path = config['Paths']['OLD_BASE_PATH']\n",
    "    new_base_path = config['Paths']['NEW_BASE_PATH']\n",
    "    df['ImageFile'] = df['ImageFile'].str.replace(old_base_path, new_base_path, regex=False)\n",
    "    print(\"---> Image paths updated.\")\n",
    "    return df\n",
    "\n",
    "def generate_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate labels based on the configuration.\"\"\"\n",
    "    print(\"---> Generating labels for Focus, StigX, and StigY...\")\n",
    "    labels_config = config.get('Labels', {}).get('MAPPINGS', {})\n",
    "    thresholds_config = config.get('Thresholds', {})\n",
    "    \n",
    "    for label_key, choices_dict in labels_config.items():\n",
    "        # Infer the offset column name by replacing \"_Label\" with \"_Offset (V)\"\n",
    "        offset_column = label_key.replace(\"_Label\", \"_Offset (V)\")\n",
    "        \n",
    "        # Check if the inferred column exists in the DataFrame\n",
    "        if offset_column not in df.columns:\n",
    "            print(f\"Warning: Column '{offset_column}' not found in DataFrame. Skipping label generation for '{label_key}'.\")\n",
    "            continue\n",
    "        \n",
    "        low_threshold = thresholds_config.get(f'{label_key}_LOW', 0)\n",
    "        high_threshold = thresholds_config.get(f'{label_key}_HIGH', 0)\n",
    "        \n",
    "        # Create conditions and choices\n",
    "        conditions = [\n",
    "            (df[offset_column].abs() <= low_threshold),\n",
    "            (df[offset_column].abs() <= high_threshold),\n",
    "            (df[offset_column].abs() > high_threshold)\n",
    "        ]\n",
    "        choices = list(choices_dict.keys())\n",
    "        \n",
    "        # Generate label\n",
    "        df[label_key] = np.select(conditions, choices)\n",
    "        print(\"---> Labels generated.\")\n",
    "    \n",
    "    # Generate multi-labels if needed\n",
    "    if config.get('Experiment', {}).get('PROBLEM_TYPE') == 'Multi-Label':\n",
    "        label_keys = list(labels_config.keys())\n",
    "        df['Multi_Labels'] = df.apply(lambda row: [row[key] for key in label_keys], axis=1)\n",
    "        print(\"---> Multi-labels generated.\")\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        df['Multi_Labels_Binarized'] = list(mlb.fit_transform(df['Multi_Labels']))\n",
    "    return df\n",
    "\n",
    "def shuffle_and_reset_index(data):\n",
    "    print(\"---> Shuffling and resetting index...\")\n",
    "    shuffled_df = data.sample(frac=1, random_state=config['Experiment']['RANDOM_SEED']).reset_index(drop=True)\n",
    "    print(\"---> Data shuffled and index reset.\")\n",
    "    return shuffled_df\n",
    "\n",
    "def prepare_datasets(df: pd.DataFramet):\n",
    "    \"\"\"Prepare training, validation, and test datasets.\"\"\"\n",
    "    # Split Data\n",
    "    try:\n",
    "        train_df, temp_df = train_test_split(df, test_size=1 - config['Model']['TRAIN_SIZE'], random_state=config['Experiment']['RANDOM_SEED'])\n",
    "        val_df, test_df = train_test_split(temp_df, test_size=1 - config['Model']['VAL_SIZE'], random_state=config['Experiment']['RANDOM_SEED'])\n",
    "    except ValueError:\n",
    "        print(\"Not enough data to split into training, validation, and test sets.\")\n",
    "        return {'train': None, 'valid': None, 'test': None}\n",
    "    return {'train': train_df, 'valid': val_df, 'test': test_df}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_tf_datasets(datasets: Dict[str, pd.DataFrame], config: Dict, preprocess_function: Tuple) -> Dict:\n",
    "    # \"\"\"Prepare TensorFlow datasets and related information.\"\"\"\n",
    "    tf_datasets = {}\n",
    "    \n",
    "    # Decide on the type of problem\n",
    "    problem_type = config.get('Experiment', {}).get('PROBLEM_TYPE')\n",
    "    \n",
    "    # Create an empty dictionary to store datasets and info\n",
    "    tf_datasets = {'train': None, 'valid': None, 'test': None, 'info': {'Training': {}, 'Validation': {}, 'Test': {}}}\n",
    "    \n",
    "    # Define labels based on the problem type\n",
    "    if problem_type == 'Multi-Label':\n",
    "        labels = ['Multi_Labels']\n",
    "    else:\n",
    "        labels = list(config.get('Labels', {}).get('MAPPINGS', {}).keys())\n",
    "    \n",
    "    for label in labels:\n",
    "        print(f\"---> Preparing datasets for label: {label}\")\n",
    "        \n",
    "        # Split DataFrames\n",
    "        train_df = datasets['train']\n",
    "        val_df = datasets['valid']\n",
    "        test_df = datasets['test']\n",
    "        \n",
    "        if train_df is None or val_df is None or test_df is None:\n",
    "            print(\"DataFrames are None. Skipping TensorFlow dataset preparation.\")\n",
    "            return {'train': None, 'valid': None, 'test': None, 'info': {}}\n",
    "        \n",
    "        # Create TensorFlow Datasets\n",
    "        if problem_type == 'Multi-Label':\n",
    "            train_ds = Dataset.from_tensor_slices((train_df['ImageFile'].values, list(train_df['Multi_Labels'])))\n",
    "            val_ds = Dataset.from_tensor_slices((val_df['ImageFile'].values, list(val_df['Multi_Labels'])))\n",
    "            test_ds = Dataset.from_tensor_slices((test_df['ImageFile'].values, list(test_df['Multi_Labels'])))\n",
    "        else:\n",
    "            train_ds = Dataset.from_tensor_slices((train_df['ImageFile'].values, train_df[label].map(config['Labels']['MAPPINGS'][label]).values))\n",
    "            val_ds = Dataset.from_tensor_slices((val_df['ImageFile'].values, val_df[label].map(config['Labels']['MAPPINGS'][label]).values))\n",
    "            test_ds = Dataset.from_tensor_slices((test_df['ImageFile'].values, test_df[label].map(config['Labels']['MAPPINGS'][label]).values))\n",
    "        \n",
    "        # Apply Preprocessing\n",
    "        train_ds, val_ds, test_ds = preprocess_function(train_ds, val_ds, test_ds, config)\n",
    "        \n",
    "        # Configure for Performance\n",
    "        AUTOTUNE = tf.data.AUTOTUNE\n",
    "        train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "        val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "        test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "        # Store Datasets\n",
    "        tf_datasets['train'] = train_ds\n",
    "        tf_datasets['valid'] = val_ds\n",
    "        tf_datasets['test'] = test_ds\n",
    "\n",
    "        # Compute and Store Class Weights and Info for each split\n",
    "        for split, df in zip(['Training', 'Validation', 'Test'], [train_df, val_df, test_df]):\n",
    "            if problem_type == 'Multi-Label':\n",
    "                # Handle multi-label case\n",
    "                pass  # TODO: Implement multi-label class weight computation\n",
    "            else:\n",
    "                # Handle binary and multi-class cases\n",
    "                unique_labels = df[label].unique()\n",
    "                class_weights = compute_class_weight('balanced', classes=unique_labels, y=df[label])\n",
    "                class_weights_dict = dict(zip(unique_labels, class_weights))\n",
    "\n",
    "                tf_datasets['info'][split] = {\n",
    "                    'Total': len(df),\n",
    "                    'ClassInfo': {cls: {'Count': cnt, 'Weight': class_weights_dict[cls]} for cls, cnt in Counter(df[label]).items()}\n",
    "                }\n",
    "        \n",
    "        print(f\"---> Datasets prepared for label: {label}\")\n",
    "\n",
    "    print(\"===== Preprocessing and Dataset Preparation Complete =====\")\n",
    "    \n",
    "    return tf_datasets"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
