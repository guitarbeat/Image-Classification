{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Focus and Astigmatism Classifier\n",
    "**Author:** [Aaron Woods](https://aaronwoods.info)  \n",
    "**Date Created:** September 12, 2023  \n",
    "**Description:** This script provides an end-to-end machine learning pipeline to classify images as either \"In Focus\" or \"Out of Focus\", and additionally identifies astigmatism-related issues.  \n",
    "**Repository:** [Image Classification on VSCode](https://insiders.vscode.dev/tunnel/midnightsim/c:/Users/User/Desktop/Image-Classification)\n",
    "\n",
    "### Overview\n",
    "The script features a comprehensive pipeline that ingests data from Excel spreadsheets and feeds it into various machine learning models. The design is modular, allowing for easy adaptability to address different image classification problems, including focus quality and astigmatism detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: Windows-10-10.0.22621-SP0\n",
      "Python Version: 3.10.9\n",
      "TensorFlow Version: 2.13.0\n",
      "Num GPUs Available: 0\n",
      "Instructions: No GPUs found. To use a GPU, follow these steps:\n",
      "  1. Install NVIDIA drivers for your GPU.\n",
      "  2. Install a compatible CUDA toolkit.\n",
      "  3. Install the cuDNN library.\n",
      "  4. Make sure to install the GPU version of TensorFlow.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# TensorFlow Installation with GPU Support\n",
    "# ------------------------------\n",
    "# Note: TensorFlow versions above 2.10 are not supported on GPUs on native Windows installations.\n",
    "# For more details, visit: https://www.tensorflow.org/install/pip#windows-wsl2_1\n",
    "# Uncomment the following line to install TensorFlow if needed.\n",
    "# %pip install \"tensorflow<2.11\"\n",
    "\n",
    "# ------------------------------\n",
    "# System and TensorFlow Info Check\n",
    "# ------------------------------\n",
    "# Import necessary libraries and initialize an empty dictionary to store system information.\n",
    "import platform\n",
    "system_info = {\"Platform\": platform.platform(), \"Python Version\": platform.python_version()}\n",
    "\n",
    "# Try importing TensorFlow and collecting relevant system information.\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    system_info.update({\n",
    "        \"TensorFlow Version\": tf.__version__,\n",
    "        \"Num GPUs Available\": len(tf.config.list_physical_devices('GPU'))\n",
    "    })\n",
    "    system_info['Instructions'] = (\n",
    "        \"You're all set to run your model on a GPU.\" \n",
    "        if system_info['Num GPUs Available'] \n",
    "        else (\n",
    "            \"No GPUs found. To use a GPU, follow these steps:\\n\"\n",
    "            \"  1. Install NVIDIA drivers for your GPU.\\n\"\n",
    "            \"  2. Install a compatible CUDA toolkit.\\n\"\n",
    "            \"  3. Install the cuDNN library.\\n\"\n",
    "            \"  4. Make sure to install the GPU version of TensorFlow.\"\n",
    "        )\n",
    "    )\n",
    "except ModuleNotFoundError:\n",
    "    system_info['Instructions'] = (\n",
    "        \"TensorFlow is not installed. \"\n",
    "        \"Install it using pip by running: !pip install tensorflow\"\n",
    "    )\n",
    "\n",
    "# Format and display the gathered system information.\n",
    "formatted_info = \"\\n\".join(f\"{key}: {value}\" for key, value in system_info.items())\n",
    "print(formatted_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Package Installation (Optional)\n",
    "# ------------------------------\n",
    "# Uncomment the following lines to install required packages if running on a new machine.\n",
    "# To suppress the output, we use '> /dev/null 2>&1'.\n",
    "# %pip install numpy pandas matplotlib protobuf seaborn scikit-learn tensorflow > /dev/null 2>&1\n",
    "\n",
    "# ------------------------------\n",
    "# Import Libraries\n",
    "# ------------------------------\n",
    "\n",
    "# Standard Libraries\n",
    "import os, sys, random, math, glob, logging\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Third-Party Libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from IPython.display import clear_output\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import Counter\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.callbacks import TensorBoard, Callback\n",
    "from tensorflow.keras.applications import InceptionV3, ResNet50\n",
    "from keras.models import load_model\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Type Annotations\n",
    "from typing import List, Dict, Tuple, Union, Any, Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration dictionary\n",
    "config = {\n",
    "    'Experiment': {\n",
    "        'NAME': \"Multi-Label_Thresholds-30-60-1-2\",  # Experiment name\n",
    "        'RANDOM_SEED': 42,  # Seed for reproducibility\n",
    "        'PROBLEM_TYPE': 'Multi-Label',  # Problem type: Binary, Multi-Class, Multi-Label\n",
    "    },\n",
    "    'Model': {\n",
    "        'IMG_SIZE': 224,  # Image input size\n",
    "        'BATCH_SIZE': 32,  # Batch size for training\n",
    "        'EPOCHS': 100,  # Number of epochs\n",
    "        'LEARNING_RATE': 1e-3,  # Learning rate\n",
    "        'EARLY_STOPPING_PATIENCE': 5,  # Early stopping patience parameter\n",
    "        'REDUCE_LR_PATIENCE': 3,  # Learning rate reduction patience parameter\n",
    "        'MIN_LR': 1e-6,  # Minimum learning rate\n",
    "        'LOSS': \"binary_crossentropy\",  # Loss function: \"categorical_crossentropy\" for multi-class\n",
    "        'TRAIN_SIZE': 0.8,  # Fraction of data to use for training\n",
    "        'VAL_SIZE': 0.5,  # Fraction of data to use for validation\n",
    "    },\n",
    "    'Labels': {\n",
    "        'MAPPINGS': {  # Class label mappings\n",
    "            'Focus_Label': {'SharpFocus': 0, 'SlightlyBlurred': 1, 'HighlyBlurred': 2},\n",
    "            'StigX_Label': {'OptimalStig_X': 0, 'ModerateStig_X': 1, 'SevereStig_X': 2},\n",
    "            'StigY_Label': {'OptimalStig_Y': 0, 'ModerateStig_Y': 1, 'SevereStig_Y': 2},\n",
    "        }\n",
    "    },\n",
    "    'Augmentation': {  # Data augmentation parameters\n",
    "        'rotation_factor': 0.002,\n",
    "        'height_factor': (-0.18, 0.18),\n",
    "        'width_factor': (-0.18, 0.18),\n",
    "        'contrast_factor': 0.5,\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(config['Experiment']['RANDOM_SEED'])\n",
    "tf.random.set_seed(config['Experiment']['RANDOM_SEED'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations for Loss Functions and Other Settings Per Problem Type\n",
    "\n",
    "#### Multi-Label Problems:\n",
    "- **Loss Function**: Typically, \"binary_crossentropy\" is used because each class label is independent and the task is to predict whether it is present or not.\n",
    "- **Label Encoding**: One-hot encoding is commonly used where each label is considered as a separate class.\n",
    "- **Activation Function**: The sigmoid activation function is generally used in the output layer to allow for multiple independent classes.\n",
    "- **Evaluation Metrics**: Precision, Recall, and F1 Score can be effective for evaluating multi-label problems.\n",
    "\n",
    "#### Binary Classification Problems:\n",
    "- **Loss Function**: \"binary_crossentropy\" is the standard loss function because the task is to categorize instances into one of the two classes.\n",
    "- **Label Encoding**: Labels are often encoded as 0 or 1.\n",
    "- **Activation Function**: The sigmoid activation function is usually used in the output layer, producing a probability score that can be thresholded to yield a class label.\n",
    "- **Evaluation Metrics**: Accuracy, Precision, Recall, and AUC-ROC are commonly used metrics.\n",
    "\n",
    "#### Multi-Class Problems:\n",
    "- **Loss Function**: \"categorical_crossentropy\" or \"sparse_categorical_crossentropy\" is commonly used. The former requires one-hot encoded labels, while the latter requires integer labels.\n",
    "- **Label Encoding**: One-hot encoding is often used to convert the categorical labels into a format that can be provided to the neural network.\n",
    "- **Activation Function**: The softmax activation function is used in the output layer to produce a probability distribution over the multiple classes.\n",
    "- **Evaluation Metrics**: Accuracy is the most straightforward metric. However, Precision, Recall, and F1 Score can also be used for imbalanced datasets.\n",
    "\n",
    "Remember to refer to these guidelines when setting up your configuration for different types of problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_activation_and_units(num_classes: int) -> tuple:\n",
    "    \"\"\"Determine the activation function and units based on number of classes and problem type from config.\"\"\"\n",
    "    problem_type = config.get('Experiment').get('PROBLEM_TYPE')\n",
    "    if problem_type == 'Multi-Label':\n",
    "        return \"sigmoid\", num_classes # Sigmoid converts each score of the final node between 0 to 1 independent of what the other scores are\n",
    "    elif problem_type == 'Binary' or num_classes == 2:\n",
    "        return \"sigmoid\", 1 # Sigmoid converts each score of the final node between 0 to 1 independent of what the other scores are\n",
    "    elif problem_type == 'Multi-Class':\n",
    "        return \"softmax\", num_classes # Softmax converts each score of the final node between 0 to 1, but also makes sure all the scores add up to 1\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid problem_type: {problem_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning models\n",
    "def create_transfer_model(base_model, input_shape: tuple, num_classes: int, hidden_units: list, dropout_rate: float, regularizer_rate: float) -> keras.Model:\n",
    "    \"\"\"Creates a transfer learning model based on a given base model.\"\"\"\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D()\n",
    "    ])\n",
    "\n",
    "    for units in hidden_units:\n",
    "        model.add(layers.Dense(units, kernel_regularizer=keras.regularizers.l2(regularizer_rate), bias_regularizer=keras.regularizers.l2(regularizer_rate)))\n",
    "        model.add(layers.LeakyReLU())\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "        \n",
    "    activation, units = determine_activation_and_units(num_classes)\n",
    "    model.add(layers.Dense(units, activation=activation))\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_mobilenetv2_transfer_model(input_shape: tuple, num_classes: int) -> keras.Model:\n",
    "    \"\"\"Creates a MobileNetV2 based transfer learning model.\"\"\"\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    return create_transfer_model(base_model, input_shape, num_classes, [128, 64], 0.5, 0.001)\n",
    "\n",
    "def create_inceptionv3_transfer_model(input_shape: tuple, num_classes: int) -> keras.Model:\n",
    "    \"\"\"Creates an InceptionV3 based transfer learning model.\"\"\"\n",
    "    base_model = tf.keras.applications.InceptionV3(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    return create_transfer_model(base_model, input_shape, num_classes, [128, 64], 0.5, 0.001)\n",
    "\n",
    "def create_resnet50_transfer_model(input_shape: tuple, num_classes: int) -> keras.Model:\n",
    "    \"\"\"Creates a ResNet50 based transfer learning model.\"\"\"\n",
    "    base_model = tf.keras.applications.ResNet50(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    return create_transfer_model(base_model, input_shape, num_classes, [256, 128], 0.5, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create a small version of the Xception network\n",
    "def create_small_xception_model(input_shape, num_classes):\n",
    "    # Input layer\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Entry block: Initial Convolution and BatchNormalization\n",
    "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    previous_block_activation = x  # Set aside residual for later use\n",
    "\n",
    "    # Middle flow: Stacking Separable Convolution blocks\n",
    "    for size in [256, 512, 728]:\n",
    "        # ReLU activation\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        # Separable Convolution\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        # ReLU activation\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        # Separable Convolution\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        # Max Pooling\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual from previous block and add it to the current block\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(previous_block_activation)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Exit flow: Final Separable Convolution, BatchNormalization, and Global Average Pooling\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    activation, units = determine_activation_and_units(num_classes)\n",
    "\n",
    "    # Dropout and Dense output layer\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create a basic CNN model\n",
    "def create_basic_cnn_model(input_shape, num_classes):\n",
    "    conv2d_filter_size = (3, 3)\n",
    "    conv2d_activation = 'relu'\n",
    "    dense_activation = 'relu'\n",
    "    num_conv_blocks = 3\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Explicitly define the input shape\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "\n",
    "    for _ in range(num_conv_blocks):\n",
    "        model.add(tf.keras.layers.Conv2D(32 * (2**_), conv2d_filter_size, activation=conv2d_activation, padding='same'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "    model.add(tf.keras.layers.Dense(128, activation=dense_activation))\n",
    "\n",
    "    activation, units = determine_activation_and_units(num_classes)\n",
    "    model.add(layers.Dense(units, activation=activation))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection function to select which model to use\n",
    "def select_model(model_name: str, input_shape: tuple, num_classes: int) -> keras.Model:\n",
    "    \"\"\"Selects a model to use based on the given model name.\"\"\"\n",
    "    model_map = {\n",
    "        \"mobilenetv2\": create_mobilenetv2_transfer_model,\n",
    "        \"inceptionv3\": create_inceptionv3_transfer_model,\n",
    "        \"resnet50\": create_resnet50_transfer_model,\n",
    "        \"small_xception\": create_small_xception_model,\n",
    "        \"basic_cnn\": create_basic_cnn_model\n",
    "    }\n",
    "    if model_name not in model_map:\n",
    "        raise ValueError(\"Invalid model name\")\n",
    "\n",
    "    return model_map[model_name](input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessing_layers(img_width: int, img_height: int, rescale_factor: float) -> keras.Sequential:\n",
    "    \"\"\"Create preprocessing layers for resizing and rescaling images.\"\"\"\n",
    "    return keras.Sequential([\n",
    "        layers.Resizing(img_width, img_height),\n",
    "        layers.Rescaling(rescale_factor)\n",
    "    ])\n",
    "\n",
    "\n",
    "def create_augmentation_layers(augmentation_config: dict) -> keras.Sequential:\n",
    "    \"\"\"Create data augmentation layers.\"\"\"\n",
    "    try:\n",
    "        augmentation_layers = tf.keras.Sequential([\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomFlip(\"vertical\"),\n",
    "            layers.RandomRotation(augmentation_config['rotation_factor']),\n",
    "            layers.RandomTranslation(\n",
    "                height_factor=augmentation_config['height_factor'],\n",
    "                width_factor=augmentation_config['width_factor'],\n",
    "                fill_mode=\"reflect\"\n",
    "            ),\n",
    "            layers.RandomContrast(augmentation_config['contrast_factor']),\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating augmentation layers: {e}\")\n",
    "    return augmentation_layers\n",
    "\n",
    "\n",
    "def read_and_convert_image(file_path: str) -> tf.Tensor:\n",
    "    \"\"\"Read an image from a file and convert it to a 3-channel tensor.\"\"\"\n",
    "    print(\"Reading image from:\", file_path)\n",
    "    image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        print(\"Failed to read the image.\")\n",
    "        return None\n",
    "    image = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "    image = tf.expand_dims(image, axis=-1)\n",
    "    return tf.image.grayscale_to_rgb(image)\n",
    "\n",
    "\n",
    "def preprocess_image(file_path, label, augment) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"Preprocess an image by applying resizing, rescaling, and optional data augmentation.\"\"\"\n",
    "    file_path = file_path.numpy().decode(\"utf-8\")\n",
    "    preprocess_seq = create_preprocessing_layers(\n",
    "        img_width=config['Model']['IMG_SIZE'], \n",
    "        img_height=config['Model']['IMG_SIZE'], \n",
    "        rescale_factor=1./255\n",
    "    )\n",
    "    augment_seq = create_augmentation_layers(config['Augmentation'])\n",
    "    image = read_and_convert_image(file_path)\n",
    "    if image is None:\n",
    "        print(\"Image reading failed.\")\n",
    "        return None, label\n",
    "    image = preprocess_seq(image)\n",
    "    if augment:\n",
    "        image = augment_seq(image)\n",
    "        image = tf.clip_by_value(image, 0, 1)  # Clip values after augmentation\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "import tensorflow as tf\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def determine_label_shape(config: Dict) -> int:\n",
    "    \"\"\"Determine the shape of the label based on the problem type and label mappings.\"\"\"\n",
    "    problem_type = config.get('Experiment', {}).get('PROBLEM_TYPE', None)\n",
    "    mappings = config.get('Labels', {}).get('MAPPINGS', None)\n",
    "    \n",
    "    if problem_type == 'Multi-Label':\n",
    "        return sum(len(v) for v in mappings.values())\n",
    "    elif problem_type in ['Multi-Class', 'Binary']:\n",
    "        return len(mappings.get(next(iter(mappings))))  # Assuming all label types have the same length\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid PROBLEM_TYPE: {problem_type}\")\n",
    "\n",
    "def preprocess_wrapper(file_path, label, augment: bool) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"Wrapper function for TensorFlow's map function for various types of classification problems.\"\"\"\n",
    "    image, label = tf.py_function(\n",
    "        func=lambda file_path, label, augment: preprocess_image(file_path, label, augment),\n",
    "        inp=[file_path, label, augment], \n",
    "        Tout=[tf.float32, tf.int32]  # Using tf.int32 for compatibility across problem types\n",
    "    )\n",
    "    \n",
    "    # Set shapes\n",
    "    image.set_shape([config['Model']['IMG_SIZE'], config['Model']['IMG_SIZE'], 3])\n",
    "    label_shape = determine_label_shape(config)\n",
    "    label.set_shape([label_shape])\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "def preprocess_single_dataset(ds, is_training: bool = False):\n",
    "    \"\"\"Apply preprocessing to a single dataset.\"\"\"\n",
    "    ds = ds.map(lambda file_path, label: preprocess_wrapper(file_path, label, is_training))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Preparation of CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Improved way: Making columns to read configurable\n",
    "csv_config = {\n",
    "    'CSV': {\n",
    "        'COLUMNS_TO_READ': ['ImageFile', 'Focus_Offset (V)', 'Stig_Offset_X (V)', 'Stig_Offset_Y (V)']\n",
    "    },\n",
    "    'Thresholds': {\n",
    "        'FOCUS_LOW': 30,  # Lower focus threshold\n",
    "        'FOCUS_HIGH': 60,  # Upper focus threshold\n",
    "        'STIG_LOW': 1,  # Lower astigmatism threshold\n",
    "        'STIG_HIGH': 2,  # Upper astigmatism threshold\n",
    "    },\n",
    "    'Paths': {  # Data and model paths\n",
    "        'DATA_FILE': \"combined_output.csv\",\n",
    "        'OLD_BASE_PATH': \"D:\\\\DOE\\\\\",\n",
    "        # 'NEW_BASE_PATH': \"Y:\\\\User\\\\Aaron-HX38\\\\DOE\\\\\",\n",
    "        # 'NEW_BASE_PATH': \"C:\\\\Users\\\\aaron.woods\\\\OneDrive - Thermo Fisher Scientific\\\\Documents\\\\GitHub\\\\Image-Classification\\\\\",\n",
    "        'NEW_BASE_PATH': \"C:\\\\Users\\\\aaron.woods\\OneDrive - Thermo Fisher Scientific\\\\Desktop\\\\Dec 24\",\n",
    "    },\n",
    "    'SAMPLE_FRAC': 1.0,  # Fraction of the data to use for quicker prototyping. 1.0 means use all data.\n",
    "}\n",
    "config.update(csv_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Preprocessing CSV Data =====\n",
      "---> Reading data from: C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher Scientific\\Desktop\\Dec 24\\combined_output.csv\n",
      "---> Data read successfully.\n",
      "---> Image paths updated.\n",
      "---> Generating labels for Focus, StigX, and StigY...\n",
      "---> Labels generated for Focus_Label\n",
      "---> Labels generated for StigX_Label\n",
      "---> Labels generated for StigY_Label\n",
      "---> Multi-labels generated.\n",
      "---> Shuffling and resetting index...\n",
      "---> Data shuffled and index reset.\n",
      "===== Preparing TensorFlow Datasets =====\n",
      "===== Computing and Storing Class Weights =====\n",
      "===== Creating TensorFlow Datasets =====\n",
      "Before calling create_tf_datasets\n",
      "Incoming datasets:\n",
      "{   'test':                                                ImageFile  Focus_Offset (V)  \\\n",
      "5065   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...       -101.983577   \n",
      "10600  C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...       -131.081995   \n",
      "1178   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...          3.401062   \n",
      "11961  C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...        -28.583599   \n",
      "8025   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...          0.000000   \n",
      "...                                                  ...               ...   \n",
      "4743   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...         37.824347   \n",
      "7174   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...         50.000000   \n",
      "1372   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...          0.000000   \n",
      "4395   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...        -38.683372   \n",
      "11772  C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...          0.000000   \n",
      "\n",
      "       Stig_Offset_X (V)  Stig_Offset_Y (V)    Focus_Label    StigX_Label  \\\n",
      "5065           -2.179674          -1.997459  HighlyBlurred   SevereStig_X   \n",
      "10600          -1.667816          -2.887206  HighlyBlurred   SevereStig_X   \n",
      "1178            0.000000           0.000000  HighlyBlurred  OptimalStig_X   \n",
      "11961           0.000000           0.000000  HighlyBlurred  OptimalStig_X   \n",
      "8025            0.000000           0.000000     SharpFocus  OptimalStig_X   \n",
      "...                  ...                ...            ...            ...   \n",
      "4743           -0.169686           0.167486  HighlyBlurred   SevereStig_X   \n",
      "7174           -0.200000           0.200000  HighlyBlurred   SevereStig_X   \n",
      "1372            0.000000           0.000000     SharpFocus  OptimalStig_X   \n",
      "4395            0.188232          -0.098950  HighlyBlurred   SevereStig_X   \n",
      "11772           0.000000           0.000000     SharpFocus  OptimalStig_X   \n",
      "\n",
      "         StigY_Label                                   Multi_Labels  \\\n",
      "5065    SevereStig_Y    [HighlyBlurred, SevereStig_X, SevereStig_Y]   \n",
      "10600   SevereStig_Y    [HighlyBlurred, SevereStig_X, SevereStig_Y]   \n",
      "1178   OptimalStig_Y  [HighlyBlurred, OptimalStig_X, OptimalStig_Y]   \n",
      "11961  OptimalStig_Y  [HighlyBlurred, OptimalStig_X, OptimalStig_Y]   \n",
      "8025   OptimalStig_Y     [SharpFocus, OptimalStig_X, OptimalStig_Y]   \n",
      "...              ...                                            ...   \n",
      "4743    SevereStig_Y    [HighlyBlurred, SevereStig_X, SevereStig_Y]   \n",
      "7174    SevereStig_Y    [HighlyBlurred, SevereStig_X, SevereStig_Y]   \n",
      "1372   OptimalStig_Y     [SharpFocus, OptimalStig_X, OptimalStig_Y]   \n",
      "4395    SevereStig_Y    [HighlyBlurred, SevereStig_X, SevereStig_Y]   \n",
      "11772  OptimalStig_Y     [SharpFocus, OptimalStig_X, OptimalStig_Y]   \n",
      "\n",
      "      Multi_Labels_Binarized  \n",
      "5065      [1, 0, 0, 1, 1, 0]  \n",
      "10600     [1, 0, 0, 1, 1, 0]  \n",
      "1178      [1, 1, 1, 0, 0, 0]  \n",
      "11961     [1, 1, 1, 0, 0, 0]  \n",
      "8025      [0, 1, 1, 0, 0, 1]  \n",
      "...                      ...  \n",
      "4743      [1, 0, 0, 1, 1, 0]  \n",
      "7174      [1, 0, 0, 1, 1, 0]  \n",
      "1372      [0, 1, 1, 0, 0, 1]  \n",
      "4395      [1, 0, 0, 1, 1, 0]  \n",
      "11772     [0, 1, 1, 0, 0, 1]  \n",
      "\n",
      "[1258 rows x 9 columns],\n",
      "    'train':                                                ImageFile  Focus_Offset (V)  \\\n",
      "5950   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...         16.726614   \n",
      "10768  C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...          4.548369   \n",
      "10477  C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...          0.000000   \n",
      "10004  C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...          0.000000   \n",
      "12062  C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...        -37.397433   \n",
      "...                                                  ...               ...   \n",
      "11964  C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...          0.000000   \n",
      "5191   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...          0.000000   \n",
      "5390   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...         14.003768   \n",
      "860    C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...        318.090042   \n",
      "7270   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...          0.000000   \n",
      "\n",
      "       Stig_Offset_X (V)  Stig_Offset_Y (V)    Focus_Label    StigX_Label  \\\n",
      "5950            0.000000           0.000000  HighlyBlurred  OptimalStig_X   \n",
      "10768          -0.029200           0.040171  HighlyBlurred   SevereStig_X   \n",
      "10477          -0.772031           0.000000     SharpFocus   SevereStig_X   \n",
      "10004           0.000000           1.100515     SharpFocus  OptimalStig_X   \n",
      "12062           0.000000           0.000000  HighlyBlurred  OptimalStig_X   \n",
      "...                  ...                ...            ...            ...   \n",
      "11964           0.000000           0.000000     SharpFocus  OptimalStig_X   \n",
      "5191            0.654630           0.000000     SharpFocus   SevereStig_X   \n",
      "5390            0.000000           0.000000  HighlyBlurred  OptimalStig_X   \n",
      "860            -0.026761          -0.191425  HighlyBlurred   SevereStig_X   \n",
      "7270            1.184083           0.000000     SharpFocus   SevereStig_X   \n",
      "\n",
      "         StigY_Label                                   Multi_Labels  \\\n",
      "5950   OptimalStig_Y  [HighlyBlurred, OptimalStig_X, OptimalStig_Y]   \n",
      "10768   SevereStig_Y    [HighlyBlurred, SevereStig_X, SevereStig_Y]   \n",
      "10477  OptimalStig_Y      [SharpFocus, SevereStig_X, OptimalStig_Y]   \n",
      "10004   SevereStig_Y      [SharpFocus, OptimalStig_X, SevereStig_Y]   \n",
      "12062  OptimalStig_Y  [HighlyBlurred, OptimalStig_X, OptimalStig_Y]   \n",
      "...              ...                                            ...   \n",
      "11964  OptimalStig_Y     [SharpFocus, OptimalStig_X, OptimalStig_Y]   \n",
      "5191   OptimalStig_Y      [SharpFocus, SevereStig_X, OptimalStig_Y]   \n",
      "5390   OptimalStig_Y  [HighlyBlurred, OptimalStig_X, OptimalStig_Y]   \n",
      "860     SevereStig_Y    [HighlyBlurred, SevereStig_X, SevereStig_Y]   \n",
      "7270   OptimalStig_Y      [SharpFocus, SevereStig_X, OptimalStig_Y]   \n",
      "\n",
      "      Multi_Labels_Binarized  \n",
      "5950      [1, 1, 1, 0, 0, 0]  \n",
      "10768     [1, 0, 0, 1, 1, 0]  \n",
      "10477     [0, 0, 1, 1, 0, 1]  \n",
      "10004     [0, 1, 0, 0, 1, 1]  \n",
      "12062     [1, 1, 1, 0, 0, 0]  \n",
      "...                      ...  \n",
      "11964     [0, 1, 1, 0, 0, 1]  \n",
      "5191      [0, 0, 1, 1, 0, 1]  \n",
      "5390      [1, 1, 1, 0, 0, 0]  \n",
      "860       [1, 0, 0, 1, 1, 0]  \n",
      "7270      [0, 0, 1, 1, 0, 1]  \n",
      "\n",
      "[10057 rows x 9 columns],\n",
      "    'valid':                                                ImageFile  Focus_Offset (V)  \\\n",
      "3781   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...          0.000000   \n",
      "12301  C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...         -9.228334   \n",
      "11806  C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...          0.000000   \n",
      "9706   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...         14.351364   \n",
      "4381   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...       -106.189899   \n",
      "...                                                  ...               ...   \n",
      "7277   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...        -77.646493   \n",
      "3447   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...          0.000000   \n",
      "8236   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...         37.570507   \n",
      "5478   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...          0.000000   \n",
      "1006   C:\\Users\\aaron.woods\\OneDrive - Thermo Fisher ...        150.587031   \n",
      "\n",
      "       Stig_Offset_X (V)  Stig_Offset_Y (V)    Focus_Label    StigX_Label  \\\n",
      "3781           -1.497785           0.000000     SharpFocus   SevereStig_X   \n",
      "12301           0.000000           0.000000  HighlyBlurred  OptimalStig_X   \n",
      "11806           0.000000          -0.928992     SharpFocus  OptimalStig_X   \n",
      "9706            0.089141          -0.156860  HighlyBlurred   SevereStig_X   \n",
      "4381           -1.299876          -2.209880  HighlyBlurred   SevereStig_X   \n",
      "...                  ...                ...            ...            ...   \n",
      "7277            0.000000           0.000000  HighlyBlurred  OptimalStig_X   \n",
      "3447            0.509567           0.000000     SharpFocus   SevereStig_X   \n",
      "8236            0.018694           0.136014  HighlyBlurred   SevereStig_X   \n",
      "5478            1.470837           0.000000     SharpFocus   SevereStig_X   \n",
      "1006            1.803193           1.749136  HighlyBlurred   SevereStig_X   \n",
      "\n",
      "         StigY_Label                                   Multi_Labels  \\\n",
      "3781   OptimalStig_Y      [SharpFocus, SevereStig_X, OptimalStig_Y]   \n",
      "12301  OptimalStig_Y  [HighlyBlurred, OptimalStig_X, OptimalStig_Y]   \n",
      "11806   SevereStig_Y      [SharpFocus, OptimalStig_X, SevereStig_Y]   \n",
      "9706    SevereStig_Y    [HighlyBlurred, SevereStig_X, SevereStig_Y]   \n",
      "4381    SevereStig_Y    [HighlyBlurred, SevereStig_X, SevereStig_Y]   \n",
      "...              ...                                            ...   \n",
      "7277   OptimalStig_Y  [HighlyBlurred, OptimalStig_X, OptimalStig_Y]   \n",
      "3447   OptimalStig_Y      [SharpFocus, SevereStig_X, OptimalStig_Y]   \n",
      "8236    SevereStig_Y    [HighlyBlurred, SevereStig_X, SevereStig_Y]   \n",
      "5478   OptimalStig_Y      [SharpFocus, SevereStig_X, OptimalStig_Y]   \n",
      "1006    SevereStig_Y    [HighlyBlurred, SevereStig_X, SevereStig_Y]   \n",
      "\n",
      "      Multi_Labels_Binarized  \n",
      "3781      [0, 0, 1, 1, 0, 1]  \n",
      "12301     [1, 1, 1, 0, 0, 0]  \n",
      "11806     [0, 1, 0, 0, 1, 1]  \n",
      "9706      [1, 0, 0, 1, 1, 0]  \n",
      "4381      [1, 0, 0, 1, 1, 0]  \n",
      "...                      ...  \n",
      "7277      [1, 1, 1, 0, 0, 0]  \n",
      "3447      [0, 0, 1, 1, 0, 1]  \n",
      "8236      [1, 0, 0, 1, 1, 0]  \n",
      "5478      [0, 0, 1, 1, 0, 1]  \n",
      "1006      [1, 0, 0, 1, 1, 0]  \n",
      "\n",
      "[1257 rows x 9 columns]}\n",
      "Creating TensorFlow Datasets...\n",
      "Working on train split...\n",
      "Debugging Information:\n",
      "<class 'numpy.ndarray'> object\n",
      "<class 'list'> <class 'numpy.ndarray'>\n",
      "Checking for NaN and Inf:\n",
      "Exception occurred: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "\n",
    "def read_csv(config: Dict):\n",
    "    # Functionality to read the data\n",
    "    data_file_path = os.path.join(config['Paths']['NEW_BASE_PATH'], config['Paths']['DATA_FILE'])\n",
    "    print(f\"---> Reading data from: {data_file_path}\")\n",
    "    if not os.path.exists(data_file_path):\n",
    "        raise FileNotFoundError(f\"Error: File does not exist - {data_file_path}\")\n",
    "    try:\n",
    "        data = pd.read_csv(data_file_path, usecols=config['CSV']['COLUMNS_TO_READ'])\n",
    "        print(\"---> Data read successfully.\")\n",
    "        sample_frac = config.get('SAMPLE_FRAC', 1.0)\n",
    "        if 0 < sample_frac < 1.0:\n",
    "            data = data.sample(frac=sample_frac).reset_index(drop=True)\n",
    "            print(f\"---> Data sampled: Using {sample_frac * 100}% of the available data.\")\n",
    "\n",
    "            data = data.dropna(subset=['ImageFile']) # Drop rows with missing image paths (remove later)\n",
    "            \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error: Could not read data - {e}\") from e\n",
    "    return data\n",
    "\n",
    "def update_image_paths(df):\n",
    "    old_base_path = config['Paths']['OLD_BASE_PATH']\n",
    "    new_base_path = config['Paths']['NEW_BASE_PATH']\n",
    "    df['ImageFile'] = df['ImageFile'].str.replace(old_base_path, new_base_path, regex=False)\n",
    "    print(\"---> Image paths updated.\")\n",
    "    return df\n",
    "\n",
    "def generate_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate labels based on the configuration.\"\"\"\n",
    "    print(\"---> Generating labels for Focus, StigX, and StigY...\")\n",
    "    labels_config = config.get('Labels', {}).get('MAPPINGS', {})\n",
    "    thresholds_config = config.get('Thresholds', {})\n",
    "    \n",
    "    # Define a mapping from label_keys to offset column names\n",
    "    offset_column_mapping = {\n",
    "        'Focus_Label': 'Focus_Offset (V)',\n",
    "        'StigX_Label': 'Stig_Offset_X (V)',\n",
    "        'StigY_Label': 'Stig_Offset_Y (V)'\n",
    "    }\n",
    "    \n",
    "    for label_key, choices_dict in labels_config.items():\n",
    "        offset_column = offset_column_mapping.get(label_key)\n",
    "        \n",
    "        if not offset_column:\n",
    "            print(f\"Warning: No offset column mapping found for '{label_key}'. Skipping label generation.\")\n",
    "            continue\n",
    "\n",
    "        if offset_column not in df.columns:\n",
    "            print(f\"Warning: Column '{offset_column}' not found in DataFrame. Skipping label generation for '{label_key}'.\")\n",
    "            continue\n",
    "        \n",
    "        low_threshold = thresholds_config.get(f'{label_key}_LOW', 0)\n",
    "        high_threshold = thresholds_config.get(f'{label_key}_HIGH', 0)\n",
    "        \n",
    "        # Create conditions and choices\n",
    "        conditions = [\n",
    "            (df[offset_column].abs() <= low_threshold),\n",
    "            (df[offset_column].abs() <= high_threshold),\n",
    "            (df[offset_column].abs() > high_threshold)\n",
    "        ]\n",
    "        choices = list(choices_dict.keys())\n",
    "        \n",
    "        # Generate label\n",
    "        df[label_key] = np.select(conditions, choices)\n",
    "        print(\"---> Labels generated for\", label_key)\n",
    "    \n",
    "    # Generate multi-labels if needed\n",
    "    if config.get('Experiment', {}).get('PROBLEM_TYPE') == 'Multi-Label':\n",
    "        label_keys = list(labels_config.keys())\n",
    "        df['Multi_Labels'] = df.apply(lambda row: [row[key] for key in label_keys], axis=1)\n",
    "        print(\"---> Multi-labels generated.\")\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        df['Multi_Labels_Binarized'] = list(mlb.fit_transform(df['Multi_Labels']))\n",
    "    return df\n",
    "\n",
    "def shuffle_and_reset_index(data):\n",
    "    print(\"---> Shuffling and resetting index...\")\n",
    "    shuffled_df = data.sample(frac=1, random_state=config['Experiment']['RANDOM_SEED']).reset_index(drop=True)\n",
    "    print(\"---> Data shuffled and index reset.\")\n",
    "    return shuffled_df\n",
    "\n",
    "def prepare_datasets(df: pd.DataFrame):\n",
    "    \"\"\"Prepare training, validation, and test datasets.\"\"\"\n",
    "    # Check if DataFrame is empty\n",
    "    if df is None or df.empty:\n",
    "        print(\"Warning: DataFrame is empty. Cannot proceed with data preparation.\")\n",
    "        return {'train': None, 'valid': None, 'test': None}\n",
    "    \n",
    "    # Split Data\n",
    "    try:\n",
    "        train_df, temp_df = train_test_split(df, test_size=1 - config['Model']['TRAIN_SIZE'], random_state=config['Experiment']['RANDOM_SEED'])\n",
    "        val_df, test_df = train_test_split(temp_df, test_size=1 - config['Model']['VAL_SIZE'], random_state=config['Experiment']['RANDOM_SEED'])\n",
    "    except ValueError:\n",
    "        print(\"Not enough data to split into training, validation, and test sets.\")\n",
    "        return {'train': None, 'valid': None, 'test': None}\n",
    "    \n",
    "    return {'train': train_df, 'valid': val_df, 'test': test_df}\n",
    "\n",
    "def compute_and_store_class_weights(datasets: Dict[str, pd.DataFrame]) -> Dict[str, Union[pd.DataFrame, Dict]]:\n",
    "    \"\"\"Compute and store class weights for dataset splits (train, valid, test).\"\"\"\n",
    "    \n",
    "    problem_type = config.get('Experiment', {}).get('PROBLEM_TYPE', 'Binary')\n",
    "    \n",
    "    # Initialize info dictionary\n",
    "    info = {'train': {}, 'valid': {}, 'test': {}}\n",
    "    \n",
    "    if problem_type == 'Multi-Label':\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        for split, df in datasets.items():\n",
    "            if df is None:\n",
    "                continue\n",
    "            label_column = np.array(df['Multi_Labels'].tolist())\n",
    "            binarized_labels = mlb.fit_transform(label_column)\n",
    "            for label_idx, label_name in enumerate(mlb.classes_):\n",
    "                label_data = binarized_labels[:, label_idx]\n",
    "                unique_labels = np.unique(label_data)\n",
    "                class_weights = compute_class_weight('balanced', classes=unique_labels, y=label_data)\n",
    "                class_weights_dict = dict(zip(unique_labels, class_weights))\n",
    "                \n",
    "                info[split][label_name] = {\n",
    "                    'Total': len(df),\n",
    "                    'ClassInfo': {cls: {'Count': cnt, 'Weight': class_weights_dict.get(cls, 0)} for cls, cnt in Counter(label_data).items()}\n",
    "                }\n",
    "    \n",
    "    else:\n",
    "        for split, df in datasets.items():\n",
    "            if df is None:\n",
    "                continue\n",
    "            for label, mappings in config['Labels']['MAPPINGS'].items():\n",
    "                unique_labels = df[label].unique()\n",
    "                class_weights = compute_class_weight('balanced', classes=unique_labels, y=df[label])\n",
    "                class_weights_dict = dict(zip(unique_labels, class_weights))\n",
    "                \n",
    "                # Translate numerical labels back to original label names\n",
    "                class_info = {}\n",
    "                for num_label, cnt in Counter(df[label]).items():\n",
    "                    orig_label = [k for k, v in mappings.items() if v == num_label][0]\n",
    "                    class_info[orig_label] = {'Count': cnt, 'Weight': class_weights_dict[num_label]}\n",
    "                \n",
    "                info[split][label] = {\n",
    "                    'Total': len(df),\n",
    "                    'ClassInfo': class_info\n",
    "                }\n",
    "                \n",
    "    return {'info': info}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_tf_dataset_from_df(df: pd.DataFrame, label_column: str) -> tf.data.Dataset:\n",
    "    image_files = df['ImageFile'].values\n",
    "    labels = df[label_column].values.astype(np.float32)\n",
    "    return tf.data.Dataset.from_tensor_slices((image_files, labels))\n",
    "\n",
    "def check_for_invalid_values(array: np.ndarray):\n",
    "    if np.isnan(array).any():\n",
    "        print(\"Warning: NaN values found.\")\n",
    "    if np.isinf(array).any():\n",
    "        print(\"Warning: Inf values found.\")\n",
    "\n",
    "def create_tf_datasets(datasets: Dict[str, pd.DataFrame], config: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    print(\"Creating TensorFlow Datasets...\")\n",
    "    problem_type = config['Experiment']['PROBLEM_TYPE']\n",
    "    \n",
    "    if problem_type == 'Multi-Label':\n",
    "        for split, df in datasets.items():\n",
    "            image_files = df['ImageFile'].values\n",
    "            labels = np.array(df['Multi_Labels_Binarized'].tolist(), dtype=np.float32)\n",
    "            \n",
    "            check_for_invalid_values(labels)\n",
    "            \n",
    "            datasets[split] = tf.data.Dataset.from_tensor_slices((image_files, labels))\n",
    "            \n",
    "    else:\n",
    "        for label in config['Labels']['MAPPINGS'].keys():\n",
    "            for split, df in datasets.items():\n",
    "                datasets[label][split] = create_tf_dataset_from_df(df, label)\n",
    "                \n",
    "    return datasets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def apply_preprocessing(datasets: Dict) -> Dict:\n",
    "    \"\"\"Apply preprocessing to training, validation, and test datasets.\"\"\"\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        is_training = split == 'train' # Only augment for training data\n",
    "        # datasets[split] = preprocess_single_dataset(datasets[split], is_training)\n",
    "\n",
    "        # Configure for Performance\n",
    "        AUTOTUNE = tf.data.AUTOTUNE\n",
    "        datasets[split] = datasets[split].cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    return datasets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main function to integrate all steps\n",
    "def main_pipeline(config: Dict):\n",
    "    print(\"===== Preprocessing CSV Data =====\")\n",
    "    data = read_csv(config)\n",
    "    data = update_image_paths(data)\n",
    "    data = generate_labels(data)\n",
    "    data = shuffle_and_reset_index(data)\n",
    "    print(\"===== Preparing TensorFlow Datasets =====\")\n",
    "    datasets = prepare_datasets(data)\n",
    "    print(\"===== Computing and Storing Class Weights =====\")\n",
    "    info = compute_and_store_class_weights(datasets)\n",
    "    print(\"===== Creating TensorFlow Datasets =====\")\n",
    "    try:\n",
    "        print(\"Before calling create_tf_datasets\")\n",
    "        datasets = create_tf_datasets(datasets)\n",
    "        print(\"After calling create_tf_datasets\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "    # datasets = apply_preprocessing(datasets)\n",
    "    \n",
    "    return datasets, info\n",
    "\n",
    "datasets, info = main_pipeline(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'info': {   'test': {   'HighlyBlurred': {   'ClassInfo': {   0: {   'Count': 636,\n",
      "                                                                         'Weight': 0.9889937106918238},\n",
      "                                                                  1: {   'Count': 622,\n",
      "                                                                         'Weight': 1.0112540192926045}},\n",
      "                                                 'Total': 1258},\n",
      "                            'OptimalStig_X': {   'ClassInfo': {   0: {   'Count': 624,\n",
      "                                                                         'Weight': 1.0080128205128205},\n",
      "                                                                  1: {   'Count': 634,\n",
      "                                                                         'Weight': 0.9921135646687698}},\n",
      "                                                 'Total': 1258},\n",
      "                            'OptimalStig_Y': {   'ClassInfo': {   0: {   'Count': 599,\n",
      "                                                                         'Weight': 1.0500834724540902},\n",
      "                                                                  1: {   'Count': 659,\n",
      "                                                                         'Weight': 0.9544764795144158}},\n",
      "                                                 'Total': 1258},\n",
      "                            'SevereStig_X': {   'ClassInfo': {   0: {   'Count': 634,\n",
      "                                                                        'Weight': 0.9921135646687698},\n",
      "                                                                 1: {   'Count': 624,\n",
      "                                                                        'Weight': 1.0080128205128205}},\n",
      "                                                'Total': 1258},\n",
      "                            'SevereStig_Y': {   'ClassInfo': {   0: {   'Count': 659,\n",
      "                                                                        'Weight': 0.9544764795144158},\n",
      "                                                                 1: {   'Count': 599,\n",
      "                                                                        'Weight': 1.0500834724540902}},\n",
      "                                                'Total': 1258},\n",
      "                            'SharpFocus': {   'ClassInfo': {   0: {   'Count': 622,\n",
      "                                                                      'Weight': 1.0112540192926045},\n",
      "                                                               1: {   'Count': 636,\n",
      "                                                                      'Weight': 0.9889937106918238}},\n",
      "                                              'Total': 1258}},\n",
      "                'train': {   'HighlyBlurred': {   'ClassInfo': {   0: {   'Count': 5011,\n",
      "                                                                          'Weight': 1.0034923169028138},\n",
      "                                                                   1: {   'Count': 5046,\n",
      "                                                                          'Weight': 0.9965319064605628}},\n",
      "                                                  'Total': 10057},\n",
      "                             'OptimalStig_X': {   'ClassInfo': {   0: {   'Count': 5040,\n",
      "                                                                          'Weight': 0.997718253968254},\n",
      "                                                                   1: {   'Count': 5017,\n",
      "                                                                          'Weight': 1.0022922064979072}},\n",
      "                                                  'Total': 10057},\n",
      "                             'OptimalStig_Y': {   'ClassInfo': {   0: {   'Count': 5008,\n",
      "                                                                          'Weight': 1.0040934504792332},\n",
      "                                                                   1: {   'Count': 5049,\n",
      "                                                                          'Weight': 0.9959397900574372}},\n",
      "                                                  'Total': 10057},\n",
      "                             'SevereStig_X': {   'ClassInfo': {   0: {   'Count': 5017,\n",
      "                                                                         'Weight': 1.0022922064979072},\n",
      "                                                                  1: {   'Count': 5040,\n",
      "                                                                         'Weight': 0.997718253968254}},\n",
      "                                                 'Total': 10057},\n",
      "                             'SevereStig_Y': {   'ClassInfo': {   0: {   'Count': 5049,\n",
      "                                                                         'Weight': 0.9959397900574372},\n",
      "                                                                  1: {   'Count': 5008,\n",
      "                                                                         'Weight': 1.0040934504792332}},\n",
      "                                                 'Total': 10057},\n",
      "                             'SharpFocus': {   'ClassInfo': {   0: {   'Count': 5046,\n",
      "                                                                       'Weight': 0.9965319064605628},\n",
      "                                                                1: {   'Count': 5011,\n",
      "                                                                       'Weight': 1.0034923169028138}},\n",
      "                                               'Total': 10057}},\n",
      "                'valid': {   'HighlyBlurred': {   'ClassInfo': {   0: {   'Count': 623,\n",
      "                                                                          'Weight': 1.0088282504012842},\n",
      "                                                                   1: {   'Count': 634,\n",
      "                                                                          'Weight': 0.9913249211356467}},\n",
      "                                                  'Total': 1257},\n",
      "                             'OptimalStig_X': {   'ClassInfo': {   0: {   'Count': 616,\n",
      "                                                                          'Weight': 1.0202922077922079},\n",
      "                                                                   1: {   'Count': 641,\n",
      "                                                                          'Weight': 0.9804992199687987}},\n",
      "                                                  'Total': 1257},\n",
      "                             'OptimalStig_Y': {   'ClassInfo': {   0: {   'Count': 663,\n",
      "                                                                          'Weight': 0.9479638009049773},\n",
      "                                                                   1: {   'Count': 594,\n",
      "                                                                          'Weight': 1.0580808080808082}},\n",
      "                                                  'Total': 1257},\n",
      "                             'SevereStig_X': {   'ClassInfo': {   0: {   'Count': 641,\n",
      "                                                                         'Weight': 0.9804992199687987},\n",
      "                                                                  1: {   'Count': 616,\n",
      "                                                                         'Weight': 1.0202922077922079}},\n",
      "                                                 'Total': 1257},\n",
      "                             'SevereStig_Y': {   'ClassInfo': {   0: {   'Count': 594,\n",
      "                                                                         'Weight': 1.0580808080808082},\n",
      "                                                                  1: {   'Count': 663,\n",
      "                                                                         'Weight': 0.9479638009049773}},\n",
      "                                                 'Total': 1257},\n",
      "                             'SharpFocus': {   'ClassInfo': {   0: {   'Count': 634,\n",
      "                                                                       'Weight': 0.9913249211356467},\n",
      "                                                                1: {   'Count': 623,\n",
      "                                                                       'Weight': 1.0088282504012842}},\n",
      "                                               'Total': 1257}}}}\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
